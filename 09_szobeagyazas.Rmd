# Szóbeágyazások

```{r include=FALSE}

source("_common.R")

```

Az eddigi fejezetekben elsősorban a szózsák (bag of words) alapú módszerek voltak előtérben. Ez a reprezentálása a szövegnek szigorúan véve nem felel meg a valóságnak a kotextuális tartalom elvesztése miatt, de ezt az esetek többségében figyelmen kívül hagyhatjuk. A szóbeágyazáson (word embedding) alapuló modellek viszont kimondottan a kotextuális információt ragadják meg. A szóbeágyazás a topikmodellekhez hasonlóan szintén a felügyelet nélküli tanulás módszerére épül, azonban itt a dokumentum domináns kifejezéseinek és témáinak feltárása helyett a szavak közötti szemantikai kapcsolat megértése a cél. Vagyis a modellnek képesnek kell lennie az egyes szavak esetén szinonimáik, és ellentétpárjaik megtalálására.

A hagyományos topikmodellezés esetén a modell a szavak dokumentumokon belüli együttes megjelenési statisztikái alapján becsül dokumentum-topik, illetve topik-szó eloszlásokat, azzal a céllal, hogy koherens téma-csoportokat képezzen a modell, ezzel szemben a szóbeágyazás legújabb iskolája már neurális halókon alapul. A neurális háló a tanítási folyamata során az egyes szavak vektorreprezentációját állítja elő. A vektorok jellemzően 100-300 dimenzióból állnak, a távolságuk alapján pedig megállapítható, hogy az egyes kifejezések milyen szemantikai kapcsolatban állnak egymással.

A szóbeágyazás célja tehát a szemantikai relációk feltárása. A szavak vektorizálásának köszönhetően bármely (a korpuszunkban szereplő) tetszőleges számú szóról eldönthetjük, hogy azok milyen szemantikai kapcsolatban állnak egymással -- szinonimaként, vagy ellentétes fogalompárként szerepelnek. A szóvektorokon dimenziócsökkentő eljárást alkalmazva, s a multidimenzionális (100-300 dimenziós) teret 2 dimenziósra szűkítve könnyen vizualizálhatjuk is a korpuszunk kifejezései között fennálló szemantikai távolságot, és ahogy a lenti ábrákon is, láthatjuk, hogy az egyes kifejezések milyen relációban állnak egymással -- a szemantikailag hasanló tartalmú kifejezések egymáshoz közel, míg a távolabbi jelentéstartalmú kifejezések egymástól távolabb foglalnak helyet. A klasszkus példa, amivel jól lehet szemléltetni a szóvektorok közötti összefüggést: `king - man + woman = queen`

## Word2Vec, GloVe és fastText

A szóbeágyazásra társadalomtudományokban a két legnépszerűbb algoritmus -- Word2Vec és a GloVe -- a *kontextuális szövegeloszláson* (distributional similarity based representations) alapszik, vagyis abból a feltevésből indul ki, hogy a hasonló kifejezések hasonló kontextusban fordulnak elő, valamint mindkettő sekély neurális hálón (2 rejtett réteg) alapuló modell.[^szobeagyazas-1] A Word2Vec-nek két verziója van: Continuous Bag-of-words (CBOW) és SkipGram (SG) -- előbbi a kontextuális szavakból jelzi előre (predicting) a kontextushoz legszorosabban kapcsolódó kifejezést, míg utóbbi adott kifejezésből jelzi előre a kontextust @mikolov2013efficient. A GloVe (Global Vectors for Word Representation) a Word2Vec-hez hasonlóan neurális hálón alapuló, szóvektorok előállítását célzó modell, a Word2Vec-kel szemben azonban nem a meghatározott kontextus-ablakban (context window) megjelenő kifejezések közti kapcsolatokat tárja fel, hanem a szöveg globális jellemzőit igyekszik megragadni az egész szöveget jellemző együttes előfordulási gyakoriságok (co-occurrance) meghatározásával @pennington2014glove. Míg a Word2Vec modell prediktív jellegű, addig a GloVe egy statisztikai alapú (count-based) modell, melyek gyakorlati hasznosításukat tekintve nagyon hasonlóak.

[^szobeagyazas-1]: Egy kíváló tanulmányban @spirlingword összehasonlítják a Word2Vec és GloVe módszereket, különbözö paraméterekkel, adatbázisokkal. Amennyiben valakit komolyabban érdekelnek a szóbeágyazás gyakorlati alkalmazásának a részletei annak mindenképp ajánljuk elolvasásra.

A szóvektor modellek között érdemes megemlíteni a fastText-et is, mely 157 nyelvre kínál (köztük magyarra is) a szóbeágyazás módszeren alapuló, előre tanított szóvektorokat, melyet tovább lehet tanítani speciális szövegkorpuszokra, ezzel jelentősen lerövidítve a modell tanításához szükséges idő-, és kapacitásszükségletet @mikolov2018advances. Habár a GloVe és Word2Vec skip-gram módszerek hasonlóságát a szakirodalom adottnak veszi, a tényleges kép ennél árnyaltabb. A GloVe esetében a ritkán előforduló szavak kisebb súlyt kapnak a szóvektorok számításánal, míg a Word2Vec alulsúlyozz a nagy frekvenciájú szavakat. Ennek a következménye, hogy a Word2Vec esetében gyakori hogy a szemantikailag legközelebbi szó az egy elütés, nem pedig valid találat. Ennek ellenére a két módszer (amennyiben a Word2Vec algoritmusnál a kisfrekvenciájú tokeneket kiszűrjük) az emberi validálás során nagyon hasonló eredményeket hozott [@spirlingword].

A fejezetben a gyakorlati példa során a GloVe algoritmust használjuk majd, mivel véleményünk szerint az implementációt tartalmazó R csomagnak jobb a dokumentációja mint a többi alternatívának.

### GloVe használata magyar média korpuszon

Az elemzéshez a `text2vec` csomagot fogjuk használni, ami a GloVe implementációt tartalmazza. A lenti kód a csomag dokumentáción alapul és a Társadalomtudományi Kutatóközpont által a *Hungarian Comparative Agendas Project (CAP)* adatbázisában tárolt Magyar Nemzet korpuszt használja.[^cap-link]

[^cap-link]: A Magyar CAP Projekt által kezelt adatbázisok itt megtalálhatóak: [https://cap.tk.hu/adatbazisok](https://cap.tk.hu/adatbazisok)


```{r}
library(text2vec)
library(quanteda)
library(readtext)
library(readr)
library(dplyr)
library(tibble)
library(stringr)
```

A lenti kód blokk azt mutatja be, hogy hogyan kell a betöltött korpuszt tokenizálni és mátrix formátumba alakítani. A korpusz az a Magyar Nemzet 2004 és 2014 közötti címlapos cikkeit tartalmazza. Az eddigi előkészítő lépéseket most is megtesszük: kitöröljük a központozást, számokat, magyar töltelékszavakat, illetve kisbetűsítünk és eltávolítjuk a felesleges szóközöket és tördeléseket.

```{r}
mn <- read_csv("data/mn_large.csv")

mn_clean <- mn %>%
  mutate(
    text = str_remove_all(string = text, pattern = "[:cntrl:]"),
    text = str_remove_all(string = text, pattern = "[:punct:]"),
    text = str_remove_all(string = text, pattern = "[:digit:]"),
    text = str_to_lower(text),
    text = str_trim(text),
    text = str_squish(text)
  )
```


Fontos különbség hogy az eddigi munkafolyamatokkal ellentétben a GloVe algoritmus nem egy dokumentum-kifejezés mátrixon dolgozik, hanem egy kifejezések együttes előfordulását tartalmazó mátrixot (feature co-occurence matrix) kell készíteni inputként. Ezt a `quanteda` `fcm()` függvényével tudjuk előállítani, ami a tokenekből készíti el a mátrixot. A tokenek sorrendiségét úgy tudjuk megőrízni, hogy egy `dfm` objektumból csak a kifejezéseket tartjuk meg a `featnames()` függvény segítségével, majd a teljes token halmazból a `tokens_select()` függvénnyel kiválasztjuk őket.

```{r}
mn_corpus <- corpus(mn_clean)

mn_tokens <- tokens(mn_corpus) %>%
  tokens_remove(stopwords(language = "hungarian"))

features <- dfm(mn_tokens) %>%
  dfm_trim(min_termfreq = 5) %>%
  featnames()

mn_tokens <- tokens_select(mn_tokens, features, padding = TRUE)
```


Az `fcm` megalkotása során a célkifejezéstől való távolság függvényében súlyozzuk a tokeneket.

```{r tidy = TRUE, tidy.opts=list(width.cutoff=80)}
mn_fcm <- fcm(mn_tokens, context = "window", count = "weighted", weights = 1 / (1:5), tri = TRUE)
```

A tényleges szóbeágyazás a `text2vec` csomaggal történik. A `GlobalVector` egy új "környezetet" (environment) hoz létre. Itt adhatjuk meg az alapvető paramétereket. A `rank` a vektor dimenziót adja meg (az irodalomban a 300-500 dimenzió a megszokott). A többi paraméterrel is lehet kísérletezni, hogy mennyire változtatja meg a kapott szóbeágyazásokat. A `fit_transform` pedig a tényleges becslést végzi. Itt az iterációk számát (a gépi tanulásos irodalomban *epoch*-nak is hívják a tanulási köröket) és a korai leállás (*early stopping*) kritériumát a `convergence_tol` megadásával. Minél több dimenziót szeretnénk és minél több iterációt, annál tovább fog tartani a szóbeágyazás futtatása. 

Az egyszerűség és gyorsaság miatt a lenti kód 10 körös tanulást ad meg, ami a relatíve kicsi Magyar Nemzet korpuszon ~3 perc alatt fut le.[^config] Természetesen minél nagyobb korpuszon, minél több iterációt futtatunk, annál pontosabb eredményt fogunk kapni. A `text2vec` csomag képes a számítások párhuzamosítására, így alapbeállításként a rendelkezésre álló összes CPU magot teljesen kihasználja a számításhoz. Ennek ellenére egy százezres, milliós korpusz esetén több óra is lehet a tanítás.

[^config]: A futtatásra használt PC nem különösebben erős: 4 magos Intel Core i5-4460 (3.2GHz) CPU és 16GB RAM 

```{r 09_glove_fit, eval=FALSE}
glove <- GlobalVectors$new(rank = 300, x_max = 10, learning_rate = 0.1)

mn_main <- glove$fit_transform(mn_fcm, n_iter = 10, convergence_tol = 0.01)
```

A végleges szóvektorokat a becslés során elkészült két mátrix összegeként kapjuk.

```{r, eval=FALSE}
mn_context <- glove$components

mn_word_vectors <- mn_main + t(mn_context)
```

```{r}
# saveRDS(mn_word_vectors, "data/temp/mn_word_vector.RDS")

mn_word_vectors <- readRDS("data/temp/mn_word_vector.RDS")
```


Az egyes szavakhoz legközelebb álló szavakat a koszinusz hasonlóság alapján kapjuk, a `sim2()` függvénnyel. A lenti példában "l2" normalizálást alkalmazunk, majd a kapott hasonlósági vektort csökkenő sorrendbe rendezzük. Példaként a "polgármester" szónak a környezetét nézzük meg. Mivel a korpuszunk egy politikai napilap, ezért nem meglepő, hogy a legközelebbi szavak a politikához kapcsolódnak. 

```{r}
teszt <- mn_word_vectors["polgármester", , drop = F]

cos_sim_rom <- sim2(x = mn_word_vectors, y = teszt, method = "cosine", norm = "l2")

head(sort(cos_sim_rom[, 1], decreasing = TRUE), 5)
```

A lenti `show_vector()` függvényt definiálva a kapott eredmény egy data frame lesz, és az `n` változtatásával a kapcsolódó szavak számát is könnyen változtathatjuk.

```{r}
show_vector <- function(vectors, pattern, n = 5) {
  term <- mn_word_vectors[pattern, , drop = F]
  cos_sim <- sim2(x = vectors, y = term, method = "cosine", norm = "l2")
  cos_sim_head <- head(sort(cos_sim[, 1], decreasing = TRUE), n)
  output <- enframe(cos_sim_head, name = "term", value = "dist")
  return(output)
}
```


Példaként a "barack" nem gyümölcsöket fog adni, hanem az Egyesült Államok elnökét és hozzá kapcsolódó szavakat.

```{r}
show_vector(mn_word_vectors, "barack", 10)
```


Ugyanez működik magyar vezetőkkel is.

```{r}
show_vector(mn_word_vectors, "orbán", 10)
```
