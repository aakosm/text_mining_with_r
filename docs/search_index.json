[["index.html", "Szövegbányászat és mesterséges intelligencia R-ben 1 Bevezetés", " Szövegbányászat és mesterséges intelligencia R-ben Sebk Miklós, Ring Orsolya 2021-03-08 11:35:09 1 Bevezetés Jelen kötet a Kvantitatív szövegelemzés és szövegbányászat a politikatudományban (LHarmattan, 2016) cím könyv folytatásaként és egyben kiegészítéseként a szövegbányászat és a mesterséges intelligencia társadalomtudományi alkalmazásának gyakorlatába nyújt bevezetést. A szövegek kvantitatív elemzése (quantitative text analysis  QTA) a nemzetközi társadalomtudomány egyik leggyorsabban fejld irányzata. A szövegek és más minségi adatok (filmek, képek) elemzése annyiban különbözik a mennyiségi (kvantitatív) adatokétól, hogy nyers formájukban még nem alkalmasak arra, hogy statisztikai, illetve ökonometriai elemzés alá vessük ket, s így további módszertani problémákat vetnek fel, melyek speciális tárgyalása szükséges. A tervezett kötetben bemutatott példák többsége a politikatudományhoz kapcsolódik, de más alkalmazási területekre is kitér. Míg az elz kötet az egyes kódolási eljárásokat, illetve ezek kutatás-módszertani elnyeit és hátrányait ismertette, itt a társadalomtudományi elemzésének során használható kvantitatív szövegelemzés legfontosabb gyakorlati feladatait vesszük sorra. A kézirat a magyar tankönyvpiacon az elsnek számít a tekintetben, hogy a társadalomtudományban használatos kvantitatív szövegelemzési eljárásokat részletesen, lépésrl-lépésre ismerteti, kezdve a megfelel korpusz kialakításához szükséges ismeretektl, a különböz szövegbányászati módszerek (szózsák, dokumentum-kifejezés mátrix, a névelem-felismerés, az osztályozás, illetve a csoportosítás feladataira), illetve az egyszerbb szövegösszehasonlítási-feladatok áttekintésén át, egészen a felügyelt és felügyelet nélküli gépi tanulásig, a politikatudományi vizsgálatok során leggyakrabban használatos R programnyelven készült programok segítségével. Az olvasó a két kötet együttes használatával olyan ismeretek birtokába kerül, melyek révén képes lesz alkalmazni a kvantitatív szövegelemzés és szövegbányászat legalapvetbb eljárásait saját kutatására. Deduktív vagy induktív felfedez logikája fényében dönthet az adatelemzés módjáról, és a felkínált menübl kiválaszthatja a kutatási tervéhez legjobban illeszked megoldásokat. A kötetet végigkísér konkrét példák segítségével pedig akár reprodukálhatja is ezen eljárásokat saját kutatásában. Mindezt a kötet függelékében helyet kapó R-scriptek részletes leírása is segíti majd. A kötet két f célcsoportjaként így a társadalomtudományi kutatói és felsoktatási közösséget határozzuk meg, valamint rögzítjük, hogy a kvantitatív szövegelemzés területén belül elsdlegesen a dokumentum- és tartalomelemzési módszertanhoz kapcsolódunk. A könyvben ugyancsak helyet kap a fontosabb fogalmak magyar és angol nyelv szószedete, valamint a további olvasásra ajánlott szakirodalom szerepeltetése. Az oktatásban való közvetlen alkalmazást segíthetik továbbá a fejezetek végén megadott vizsgakérdések, illetve a kötet honlapján (qta.tk.mta.hu) szerepl további információk: gyakorlófeladatok (megoldásokkal), az egyes feladatokra alkalmazható scriptek és kereskedelmi programok bemutatása, a témával kapcsolatos prezentációk és további ajánlott irodalmak. "],["a-kvantitatív-szövegelemzés-és-szövegbányászat-alapfogalmai.html", "2 A kvantitatív szövegelemzés és szövegbányászat alapfogalmai", " 2 A kvantitatív szövegelemzés és szövegbányászat alapfogalmai elso fejezet "],["az-adatkezelés-r-ben.html", "3 Az adatkezelés R-ben 3.1 Adatok importálása és exportálása 3.2 Adatok exportálása 3.3 A pipe operátor 3.4 Muveletek a date framekkel 3.5 Munka karakter vektorokkal4", " 3 Az adatkezelés R-ben 3.1 Adatok importálása és exportálása library(readr) Az adatok importálására az R alapfüggvénye mellett több package is megoldást kínál. Ezek közül a könyv írásakor a legnépszerbbek a readr és a rio csomagok. A karakter kódolással a legjobban a tapasztalataink szerint a readr csomag read_csv() megoldása bíkózik meg, ezért ezt fogjuk használni a .csv állományok beolvasására. Amennyiben kihasználjuk az RStudio projekt opcióját (lásd a Függelékben) akkor elegend csak az elérni kívánt adatok relativ elérési útját megadni (relative path). Ideális esetben az adataink egy csv fileban vannak (comma separated values), ahol az egyes értékeket vesszk (vagy egyéb speciális karakter) választják el. Ez esetben a read_delim() függvényt használjuk. A beolvasásnál egybl el is tároljuk az adatokat egy objektumban. A sep = opcióval tudjuk a szeparátor karaktert beállítani, mert elfordulhat hogy vessz helyett pontosvessz tagolja az adatainkat. df &lt;- read_csv(&quot;data/adatfile.csv&quot;) Az R képes linkrl letölteni fileokat, elég megadnunk egy mköd elérési útvonalat. placeholder link, cserelni majd mukodore df_online &lt;- read.csv(&quot;https://www.qta.tk.mta.hu/adatok/adatfile.csv&quot;) Az R package ökoszisztémája kellen változatos ahhoz, hogy gyakorlatilag bármilyen inputtal meg tudjon bírkózni. Az Excel fileokat a readxl csomagot használva tudjuk betölteni (a csomagok installálásával kapcsolatban lásd a Függeléket), a read_excel()-t használva. A leggyakoribb statisztikai programok formátumait pedig a haven csomag tudja kezelni (például Stata, Spss, SAS). A szintaxis itt is hasonló: read_stata, read_spss, read_sas. 3.1.1 Szöveges dokumentumok importálása A nagy mennyiség szöveges dokumentum (a legyakrabban elforduló kiterjesztések: .txt, .doc, .pdf, .json, .csv, .xml, .rtf, .odt) betöltésére a legalkalmasabb a readtext package. Az alábbi példa azt mutatja be, hogy hogyan tudunk beolvasni egy adott mappából az összes .txt kiterjesztés file-t, anélkül hogy bármilyen loop-ot kellene írnunk, vagy egyenként megadni a file-ok neveit. A * karakter az azt jelenti ebben a környezetben, hogy bármilyen fájl, ami .txt-re végzdik. Amennyiben a fájlok nevei tartalmaznak valamilyen meta adatot tartalmaznak, akkor ezt be tudjuk allítani a betöltés során. Ilyen meta adat lehet például egy parlamenti felszólalásnál a felszólaló neve és a beszéd ideje és párttagsága (például: kovacsjanos_1994_fkgp.txt). df_text &lt;- readtext( &quot;data/*.txt&quot;, docvarsfrom = &quot;filenames&quot;, dvsep = &quot;_&quot;, docvarnames = c(&quot;nev&quot;, &quot;ev&quot;, &quot;part&quot;) ) 3.2 Adatok exportálása Az adatainkat R-bl a write.csv()-vel exportálhatjuk a kívánt helyre, .csv formátumban. Az R rendelkezik saját, .Rds és .Rda kiterjesztés, tömörített fájlformátummal. Mivel ezeket csak az R-ben nyithatjuk meg, érdemes a köztes, hosszadalmas számítást igényl lépések elmentésére használni, a saveRDS() és a save() parancsokkal. Az openxlsx csomaggal .xls és .xlsx Excel formátumokba is tudunk exportálni, hogyha szükséges. 3.3 A pipe operátor Az úgynevezett pipe operátor alapjaiban határozta meg a modern R fejldését és a népszer package ökoszisztéma, a tidyverse, egyik alapköve. Úgy gondoljuk, hogy a tidyverse és a pipe egyszerbbé teszi elsajátítani az R használatát, ezért mi is erre helyezzük a hangsúlyt.1 Vizuálisan a pipe operátor így néz ki: %&gt;% és arra szolgál hogy a kódban több egymáshoz kapcsolódó mveletet egybefzznk.2 Technikailag a pipe a bal oldali elemet adja meg a jobb oldali függvény els argumentumának. A lenti példa ugyanazt a folyamatot írja le, az alap R (base R) illetve a pipe használatával.3 Miközben a kódot olvassuk érdemes a pipe-ot és aztán-nak fordítani. reggeli(oltozkodes(felkeles(ebredes(en, idopont = &quot;8:00&quot;), oldal = &quot;jobb&quot;), nadrag = TRUE, ing = TRUE)) en %&gt;% ebredes(idopont = &quot;8:00&quot;) %&gt;% felkeles(oldal = &quot;jobb&quot;) %&gt;% oltozkodes(nadrag = TRUE, ing = TRUE) %&gt;% reggeli() A fenti példa is jól mutatja, hogy a pipe a bal oldali elemet fogja a jobb oldali függvény els elemének berakni. A fejezet további részeiben még bven fogunk gyakorlati példát találni a használatára. A fejezetben bemutatott példák az alkalmazásoknak csak egy relatíve szk körét mutatják be, ezért érdemes átolvasni a csomagokhoz tartozó dokumentációt, illetve ha van, akkor a mködést demonstráló bemutató oldalakat is. 3.4 Muveletek a date framekkel A data frame az egyik leghasznosabb és leggyakrabban használt adat tárolási mód az R-ben (a részletesebb leírás a Függelékben található) és ebben az alfejezetben azt mutatjuk be a dplyr és gapminder csomagok segíségével, hogy hogyan lehet hatékonyan dolgozni velük. A dplyr az egyik legnépszerbb R csomag, a tidyverse része. A gapminder csomag pedig a példa adatbázisunkat tartalmazza, amiben a világ országainak különböz gazdasági és társadalmi mutatói vannak. library(dplyr) library(gapminder) 3.4.1 Megfigyelések szrése: filter() A sorok (megfigyelések) szréséhez a dplyr csomag filter() parancsát használva lehetségünk van arra hogy egy vagy több kritérium alapján szkítsük az adatbázisunkat. A lenti példában azokat megfigyeléseket tartjuk meg, ahol az év 1962 és a várható élettartam nagyobb mint 72 év. gapminder %&gt;% filter(year == 1962, lifeExp &gt; 72) #&gt; # A tibble: 5 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Denmark Europe 1962 72.4 4646899 13583. #&gt; 2 Iceland Europe 1962 73.7 182053 10350. #&gt; 3 Netherlands Europe 1962 73.2 11805689 12791. #&gt; 4 Norway Europe 1962 73.5 3638919 13450. #&gt; 5 Sweden Europe 1962 73.4 7561588 12329. De ugyanígy leválogathatjuk a data frame-bl az adatokat akkor is hogyha egy karakter változó alapján szeretnénk szrni. gapminder %&gt;% filter(country == &quot;Sweden&quot;, year &gt; 1990) #&gt; # A tibble: 4 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Sweden Europe 1992 78.2 8718867 23880. #&gt; 2 Sweden Europe 1997 79.4 8897619 25267. #&gt; 3 Sweden Europe 2002 80.0 8954175 29342. #&gt; 4 Sweden Europe 2007 80.9 9031088 33860. Itt tehát a data frame azon sorait szeretnénk látni, ahol az ország megegyezik a Sweden\" karakterlánccal az év pedig nagyobb, mint 1990. 3.4.2 Változók kiválogatása: select() A select() függvény segítségével válogathatunk oszlopokat a data frame-bl. A változók kiválasztására több megoldás is van. A dplyr csomag tartalmaz apróbb kisegít függvényeket, amik megkönnyítik a nagy adatbázisok esetén a változók kiválogatását a nevük alapján. Ezek a függvények a contains(), starts_with(), ends_with(), matches() és beszédesen arra szolgálnak hogy bizonyos nev változókat ne kelljen egyenként felsorolni. A select()-en belüli változó sorrend egyben az eredmény data frame változó sorrendjet is megadja. A negatív kiválasztás is lehetséges, ebben az esetben egy - kell tennünk a nemkívánt változó(k) elé (pl.: select(df, year, country, -continent). gapminder %&gt;% select(contains(&quot;ea&quot;), starts_with(&quot;co&quot;), pop) #&gt; # A tibble: 1,704 x 4 #&gt; year country continent pop #&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 1952 Afghanistan Asia 8425333 #&gt; 2 1957 Afghanistan Asia 9240934 #&gt; 3 1962 Afghanistan Asia 10267083 #&gt; 4 1967 Afghanistan Asia 11537966 #&gt; 5 1972 Afghanistan Asia 13079460 #&gt; 6 1977 Afghanistan Asia 14880372 #&gt; 7 1982 Afghanistan Asia 12881816 #&gt; 8 1987 Afghanistan Asia 13867957 #&gt; 9 1992 Afghanistan Asia 16317921 #&gt; 10 1997 Afghanistan Asia 22227415 #&gt; # ... with 1,694 more rows 3.4.3 Új változók létrehozása: mutate() Az elemzési munkafolyamat elkerülhetetlen része hogy új változókat hozzunk létre, vagy a meglévket módosítsuk. Ezt a mutate()-el tehetjuk meg, ahol a szintaxis a következ: mutate(data frame, uj valtozo = ertekek). Példaként kiszámoljuk a Svéd GDP-t (milliárd dollárban) 1992-tl kezdve. A mutate() alkalmazásával részletesebben is foglalkozunk a szövegek elkészítésével foglalkozó fejezetben. gapminder %&gt;% filter(country == &quot;Sweden&quot;, year &gt;= 1992) %&gt;% mutate(gdp = (gdpPercap * pop) / 10^9) #&gt; # A tibble: 4 x 7 #&gt; country continent year lifeExp pop gdpPercap gdp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Sweden Europe 1992 78.2 8718867 23880. 208. #&gt; 2 Sweden Europe 1997 79.4 8897619 25267. 225. #&gt; 3 Sweden Europe 2002 80.0 8954175 29342. 263. #&gt; 4 Sweden Europe 2007 80.9 9031088 33860. 306. 3.4.4 Csoportonkénti statisztikák: group_by() és summarize() Az adataink részletesebb és alaposabb megismerésében segítenek a különböz szint leíró statisztikai adatok. A szintek megadására a group_by() használható, a csoportokon belüli számításokhoz pedig a summarize(). A lenti példa azt illusztrálja, hogyha kontinensenként csoportosítjuk a gapminder data framet, akkor a summarise() használatával megkaphatjuk a megfigyelések számát, illetve az átlagos per capita GDP-t. A summarise() a mutate() közeli rokona, hasonló szintaxissal és logikával használható. Ezt a függvény párost fogjuk majd használni a szöveges adataink leíró statisztikáinál is a 4. fejezetben. gapminder %&gt;% group_by(continent) %&gt;% summarise(megfigyelesek = n(), atlag_gdp = mean(gdpPercap)) #&gt; # A tibble: 5 x 3 #&gt; continent megfigyelesek atlag_gdp #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Africa 624 2194. #&gt; 2 Americas 300 7136. #&gt; 3 Asia 396 7902. #&gt; 4 Europe 360 14469. #&gt; 5 Oceania 24 18622. 3.5 Munka karakter vektorokkal4 A szöveges adatokkal (karakter stringekkel) való munka elkerülhetetlen velejárója hogy a felesleges szövegelemeket, karaktereket el kell távolítanunk ahhoz hogy az elemzésünk hatásfoka javuljon (errl részletesebben a 3. fejezetben lesz szó). Erre a célra a stringr csomagot fogjuk használni, kombinálva a korábban bemutatott mutate()-el. A stringr függvények az str_ eltaggal kezddnek és eléggé beszédes nevekkel rendelkeznek. Egy gyakran elforduló probléma, hogy extra szóközök maradnak a szövegben, vagy bizonyos szavakról, karakterkombinációkról tudjuk hogy nem kellenek az elemzésünkhoz. Ebben az esetben egy vagy több regular expression (regex) használatával tudjuk pontosan kijelölni hogy a karakter sornak melyik részét akarjuk módosítani. A legegyszerbb formája a regexeknek, hogyha pontosan tudjuk milyen szöveget akarunk megtalálni. A kísérletezésre az str_view()-t használjuk, ami megjeleníti hogy a megadott regex mintánk pontosan mit jelöl. library(stringr) szoveg &lt;- c(&quot;gitar&quot;, &quot;ukulele&quot;, &quot;nagybogo&quot;) str_view(szoveg, pattern = &quot;ar&quot;) Az anchor-okkal azt lehet megadni, hogy a karakter string elején vagy végén szeretnénk egyezést találni. A string eleji anchor a ^, a string végi pedig a $. str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;^Dr.&quot;) str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;Dr.$&quot;) Egy másik jellemz probléma, hogy olyan speciális karaktert akarunk leírni a regex kifejezésünkkel, ami amúgy a regex szintaxisban használt. Ilyen eset például a ., ami mint írásjel sokszor csak zaj, ám a regex kotextusban a bármilyen karakter megfelelje. str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;.k.&quot;) Ahhoz hogy magát az írásjelet jelöljük, a \\\\ -t kell elé rakni. str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;\\\\.&quot;) Néhány hasznos regex kifejezés: [:digit:] - számok (123) [:alpha:] - betk (abc ABC) [:lower:] - kisbetk (abc) [:upper:] - nagybetk (ABC) [:alnum:] - betk és számok (123 abc ABC) [:punct:] - központozás (.!?\\(){}) [:graph:] - betk, számok és központozás (123 abc ABC .!?\\(){}) [:space:] - szóköz ( ) [:blank:] - szóköz és tabulálás [:cntrl:] - kontrol karakterek (\\n, \\r, stb.) * - bármi A tidyverse megközelítés miatt a kötetben szerepl R kód követi a The tidyverse style guide dokumentációt (https://style.tidyverse.org/) Az RStudio-ban a pipe operátor billenty kombinációja a Ctrl + Shift + M Köszönjük Andrew Heissnek a kitn példát. A könyv terjedelme miatt ezt a témát itt csak bemutatni tudjuk, de minden részletre kiterjeden nem tudunk elmélyülni benne. Kíváló online anyagok találhatóak az RStudio GitHub tárhelyén (https://github.com/rstudio/cheatsheets/raw/master/strings.pdf), illetve Wickham and Grolemund (2016) 14. fejezetében. "],["korpuszépítés-és-szövegel-készítés.html", "4 Korpuszépítés és szövegelkészítés 4.1 Szövegbeszerzés 4.2 Szövegelkészítés", " 4 Korpuszépítés és szövegelkészítés 4.1 Szövegbeszerzés A szövebányászati elemzések egyik els lépése az elemzés alapjául szolgáló korpusz megépítése. A korpuszt alkotó szövegek beszerzésének egyik módja a webscarping, melynek során weboldalakról történik az információ kinyerése. A scrapelést végezhetjük R-ben az rvest csomomag segítségével. Fejezetünkben a scrapelésnek csupán néhány alaplépését mutatjuk meg, a folyamatról bvebb információ található például az alábbi oldalakon:https://cran.r-project.org/web/packages/rvest/rvest.pdf, https://rvest.tidyverse.org/. Telepítsük, majd olvassuk be az rvest csomagot. install.packages(&quot;rvest&quot;) library(rvest) library(readr) Majd a read_html() függvény segítségével az adott weboldal adatait kérjük le a szerverrl. A read_html() függvény argumentuma az adott weblap URL-je. Ha például a poltextLAB projekt honlapjáról szeretnénk adatokat gyjteni, azt az alábbi módon tehetjük meg: r &lt;- read_html(&quot;https://poltextlab.tk.hu/hu&quot;) r #&gt; {html_document} #&gt; &lt;html lang=&quot;hu&quot; class=&quot;no-js&quot;&gt; #&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... #&gt; [2] &lt;body class=&quot;index&quot;&gt;\\n\\n\\t&lt;script&gt;\\n\\t (function(i,s,o,g,r,a,m){i[&#39;Googl ... Ezután a html_nodes() függvény argumentumaként meg kell adnunk azt a HTML címkét vagy CSS azonosítót, ami a legyjteni kívánt elemeket azonosítja a weboldalon. Ezeket az azonosítókat az adott weboldal forráskódjának megtekintésével tudhatjuk meg, amire a különböz böngészk különböz lehetségeket kínálnak. Majd a html_text() függvény segítségével megkapjuk azokat a szövegeket, amely az adott weblapon az adott azonosítóval rendelkeznek. Példánkban a https://poltextlab.tk.hu/hu weboldalról azokat az információkat szeretnénk kigyjteni, amelyek az &lt;title&gt; címke alatt szerepenek: title &lt;- read_html(&quot;https://poltextlab.tk.hu/hu&quot;) %&gt;% html_nodes(&quot;title&quot;) %&gt;% html_text() title #&gt; [1] &quot;MTA TK Political and Legal Text Mining and Artificial Intelligence Laboratory (poltextLAB)&quot; A kigyjtött információkat pedig ezután kiíratjuk egy csv fájlba. write_csv(title, &quot;title.csv&quot;) A web scraping során az egyik nehézség, ha a weboldal letiltja az automatikus letöltést, ezt kivédhetjük például különböz böngészbvítmények segítségével, illetve a fejléc (header) vagy a user agent megváltoztatásával, de segíthet véletlenszer proxy vagy VPN szolgáltatás használata is, valamint ha az egyes kérések között idt hagynunk. A weboldalakon legtöbbször a legyjtött szövegekhez tartozó különböz metaadatok is szerepelnek (például egy parlamenti beszéd dátuma, az azt elmondó képvisel neve), melyeket érdemes a scarpelés során szintén összegyjteni. A scrapelés során fontos figyelnünk arra, hogy késbb jól használható formában mentsük el az adatokat, például .csv,.json vagy .txt kiterjesztésekkel. A karakterkódolási problémák elkerülése érdekében érdemes UTF-8 vagy UTF-16-os kódolást alkalmazni, mivel ezek tartalmazzák a magyar nyelv ékezetes karaktereit is. A karakterkódolással kapcsolatosan hasznos további információk találhatóak az alábbi oldalon: http://www.cs.bme.hu/~egmont/utf8/ Arra is van lehetség, hogy az elemezni kívánt korpuszt papíron keletkezett, majd szkennelt és szükség szerint optikai karakterfelismerés (OCR, Optical Character Recognition) segítségével feldolgozott szövegekbl építsük fel. Azonban mivel ezeket a feladatokat nem R-ben végezzük, ezekrl itt nem szólunk bvebben. Az így beszerzett és .txt, vagy .csv fájlá alakított szövegekbl való korpuszépítés a következ lépésekben megegyezik a weboldalakról gyjtött szövegekével. 4.2 Szövegelkészítés Az elemzéshez vezet következ lépés a szövegelkészítés, amit a szöveg tisztításával kell megkezdenünk. A szövegtisztításnél mindig járjunk el körültekinten és az egyes lépéseket a kutatási kérdésünknek megfelelen tervezzük meg, a folyamat során pedig idnként végezzünk ellenrzést, ezzel elkerülhetjük a kutatásunkhoz szükséges információk elvesztését. A korpusz elkészítéséhez az install.packages() paranccsal telepítsük, majd a library() paranccsal olvassuk be az alábbi csomagokat. install.packages(&quot;dplyr&quot;) install.packages(&quot;lubridate&quot;) install.packages(&quot;stringr&quot;) install.packages(&quot;quanteda&quot;) install.packages(&quot;quanteda.textmodels&quot;) library(dplyr) library(lubridate) library(stringr) library(quanteda) library(quanteda.textmodels) Miután az elemezni kívánt szövegeinket beszereztük, majd a Szöveges dokumentumok importálása[BE KELL MAJD ÍRNI A VÉLGLEGES FEJEZETSZÁMOT] cím aljezetben leírtak szerint importáltuk, következhetnek az alapvet elfeldolgozási lépések, ezek közé tartozik például a scrapelés során a kopuszba került html címkék, számok és egyéb zajok (például a speciális karakterek, írásjelek) eltávolítása a korpuszból, valamint a kisbetsítés, a tokenizálás, a szótövezés és a stopszavazás. 4.2.1 String mveletek A stringr csomag segítségével elször eltávolíthatjuk a felesleges html címkéket a kopruszból. Ehhez elször létrehozzuk a text1 nev objektumot ami egy karaktervektoból áll. text1 &lt;- c(&quot;MTA TK&quot;, &quot;&lt;font size=&#39;6&#39;&gt; Political and Legal Text Mining and Artificial Intelligence Laboratory (poltextLAB)&quot;) text1 #&gt; [1] &quot;MTA TK&quot; #&gt; [2] &quot;&lt;font size=&#39;6&#39;&gt; Political and Legal Text Mining and Artificial Intelligence Laboratory (poltextLAB)&quot; Majd a str_replace_all()függvény segítségével eltávolítjuk két html címke közötti szövegrészt. Ehhez a függvény argumentumában létrehozunk egy regex kifejezést, aminek segítségével a függvény minden &lt;&gt; közötti szövegrészt üres karakterekre cserél. Ezután a str_to_lower()mindent kisbetvé konvertál, majd a str_trim()eltávolítja a szóközöket a karekterláncok elejérl és végérl. text1 %&gt;% str_replace_all(pattern = &quot;&lt;.*?&gt;&quot;, replacement = &quot;&quot;) %&gt;% str_to_lower() %&gt;% str_trim() #&gt; [1] &quot;mta tk&quot; #&gt; [2] &quot;political and legal text mining and artificial intelligence laboratory (poltextlab)&quot; 4.2.2 Tokenizálás, szótövezés, kisbetsítés és a stopszavak eltávolítása Az elkészítés következ lépésében tokenizáljuk, azaz egységeire bontjuk az elemezni kívánt szöveget, a tokenek így pedig az egyes szavakat vagy kifejezéseket fogják jelölni. Ennek eredményeként kapjuk meg az n-gramokat, amik a vizsgált egységek (számok, betk, szavak, kifejezések) n-elem sorozatát alkotják. A következkben a Példa az elkészítésre mondatot bontjuk elször tokenekre a tokens() függvénnyel, majd a tokeneket a tokens_tolower() segítségével kisbetsítjük, a tokens_wordstem() függvénnyel pedig szótövezzük. Végezetül a quanteda csomagban található magyar nyelv stopszótár segítségével, elvégezzük a stopszavak eltávolítását.Ehhez elször létrehozzuk az sw elenevezés karaktervektort a magyar stopszvakból. A head() függvény segítségével belenézhetünk a szótárba, és a console-ra kiírathatjuk a szótár els hat szavát. Végül a tokens_remove()segítségével eltávolítjuk a stopszavakat. text &lt;- &quot;Példa az elokészítésre&quot; toks &lt;- tokens(text) toks &lt;- tokens_tolower(toks) toks &lt;- tokens_wordstem(toks) toks #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;példa&quot; &quot;az&quot; &quot;elokészítésr&quot; sw &lt;- stopwords(&quot;hungarian&quot;) head(sw) #&gt; [1] &quot;a&quot; &quot;ahogy&quot; &quot;ahol&quot; &quot;aki&quot; &quot;akik&quot; &quot;akkor&quot; tokens_remove(toks, sw) #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;példa&quot; &quot;elokészítésr&quot; 4.2.2.1 Stemmelés vagy lemmatizálás Ezt követi a szótövezés lépése, melynek során az alkalmazott stemmel algoritmus egyszeren levágja a szavak összes toldalékát, a képzket, jelzket és ragokat. A stemmelés helyett alkalmazhatunk lemmatizálást, melynek során a szavakat a szótári alakjukra formáljuk. A stemming és lemmatizálás közötti különbség abban rejlik, hogy a szótövezés során csupán eltávolítjuk a szavak toldalékként azonosított végzdéseit, hogy ugyanannak a szónak különböz megjelenési formáit közös törzsre redukáljuk, míg a lemmatizálás esetében rögtön az értelmes, szótári formát kapjuk vissza. A két módszer közötti választás a kutatási kérdés alapján meghozott kutatói döntésen alapul.(grimmer2013a?) 4.2.2.1.1 Lemmatizálás Az alábbi példában egyetlen szó különböz alakjainak szótári alakra hozásával szemléltetjük a lemmatizáslás mködését. Ehhez elször a text1 nev objektumban tároljuk a lemmatizálni kívánt szöveget, majd tokenizáljuk és eltávolítjuk a központozást. Ezután definiáljuk azt a megfelel szótövet és azt, hogy mely szavak alakjait szeretnénk erre a szótre egységesíteni majd a rep() függvény segítségével elvégezzük a lemmatizálást, amely a korábban definiált szólakokat az általunk megadott szótári alakkal helyettesíti. Hosszabb szövegek lemmatizálásához elre létrehozott szótárakat használhatunk, ilyen például a Wordnet, ami magyar nyelven is elérhet: https://github.com/mmihaltz/huwn A magyar nyelv szövegek lemmatizálását elvégezhetjük a szövegek R-be való beolvasása eltt is a magyarláncnyelvi elemz segítségével, melyrl a kötet függelékében, a Magyar nyelv NLP és nyelvtechnológiai eszközök között szólunk részletesebben. text1 &lt;- &quot;Példa az elokészítésre. Az elokészítést a szövetisztítással kell megkezdenünk. Az elokészített korpuszon elemzést végzünk&quot; toks1 &lt;- tokens(text1, remove_punct = TRUE) elokészítés &lt;- c(&quot;elokészítésre&quot;, &quot;elokészítést&quot;, &quot;elokészített&quot;) lemma &lt;- rep(&quot;elokészítés&quot;, length(elokészítés)) toks1 &lt;- tokens_replace(toks1, elokészítés, lemma, valuetype = &quot;fixed&quot;) toks1 #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;Példa&quot; &quot;az&quot; &quot;elokészítés&quot; #&gt; [4] &quot;Az&quot; &quot;elokészítés&quot; &quot;a&quot; #&gt; [7] &quot;szövetisztítással&quot; &quot;kell&quot; &quot;megkezdenünk&quot; #&gt; [10] &quot;Az&quot; &quot;elokészítés&quot; &quot;korpuszon&quot; #&gt; [ ... and 2 more ] 4.2.2.1.2 Stemmelés A fenti text1 objektumban tárolt szöveg stemmelését az alábbiak szerint tudjuk elvégezni. Megvizsgálva az elkészítés különböz alakjainak lemmatizált és stemmelt változatát jól láthatjuk a két módszer közötti különbséget. text1 &lt;- &quot;Példa az elokészítésre. Az elokészítést a szövetisztítással kell megkezdenünk. Az elokészített korpuszon elemzést végzünk&quot; toks2 &lt;- tokens(text1, remove_punct = TRUE) toks2 &lt;- tokens_wordstem(toks2) toks2 #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;Példa&quot; &quot;az&quot; &quot;elokészítésr&quot; &quot;Az&quot; #&gt; [5] &quot;elokészítést&quot; &quot;a&quot; &quot;szövetisztításs&quot; &quot;kell&quot; #&gt; [9] &quot;megkezdenünk&quot; &quot;Az&quot; &quot;elokészített&quot; &quot;korpuszon&quot; #&gt; [ ... and 2 more ] 4.2.3 Dokumentum kifejezés mátrix (DTM) A szövegbányászati elemzések nagy részéhez szükségünk van arra, hogy a szövegeinkbl dokumentum kifejezés matrix-ot (DTM), vagy dokumentum feature matrxi-ot (DFM) hozzunk létre. Ezzel a lépéssel alkaítjuk a szövegeinket számokká, ami lehetvé teszi, hogy utána különböz staisztikai mveleteket végezzünk velük. A dokumentumk kifejezés mátrix minden sora egy dokumentum, minden oszlopa egy kifejezés, az oszlopokban szerepl változók pedig az egyes kifejezések számát mutatják meg az egyes dokumnetumokban. A legtöbb DTM ritka mátrix, mivel a legtöbb dokumentum és kifejezés párosítása nem történik meg, mivel a kifejezések nagy része csak néhány dokumentumban szerepel, ezek értéke nulla lesz. Az alábbi példában három egy-egy mondatos dokumentumon szemléltetjük a fentieket. A korábban megismert módon elkészítjük, azaz kisbetsítjük, stemmeljük és stopszavazzuk a dokumentumokat, majd létrehozzuk bellük a sokumentum kifejezés mátrixot. text &lt;- c( d1 = &quot;Ez egy példa az elofeldolgozásra&quot;, d2 = &quot;Egy másik lehetséges példa&quot;, d3 = &quot;Ez pedig egy harmadik példa&quot; ) dtm &lt;- dfm( text, tolower = TRUE, stem = TRUE, remove = stopwords(&quot;hungarian&quot;) ) dtm #&gt; Document-feature matrix of: 3 documents, 4 features (50.0% sparse). #&gt; features #&gt; docs példa elofeldolgozásra lehetség harmadik #&gt; d1 1 1 0 0 #&gt; d2 1 0 1 0 #&gt; d3 1 0 0 1 Egy másik szövegbányáaszati megközelítés a mátrixot nem DTM-nek, hanem DFM-nek nevezi, például a quanteda csomag használata során nem DTM-et, hanem DFM-et kell létrehoznunk. text &lt;- c( d1 = &quot;Ez egy példa az elofeldolgozásra&quot;, d2 = &quot;Egy másik lehetséges példa&quot;, d3 = &quot;Ez pedig egy harmadik példa&quot; ) dfm &lt;- dfm( text, tolower = TRUE, stem = TRUE, remove = stopwords(&quot;hungarian&quot;) ) dfm #&gt; Document-feature matrix of: 3 documents, 4 features (50.0% sparse). #&gt; features #&gt; docs példa elofeldolgozásra lehetség harmadik #&gt; d1 1 1 0 0 #&gt; d2 1 0 1 0 #&gt; d3 1 0 0 1 4.2.4 Súlyozás A dokumentum kifejezés mátrix lehet egy egyszer bináris mátrix, ami csak azt az információt tartalmazza, hogy egy adott szó elfordul-e egy adott dokumentumban. Míg az egyszer bináris mátrixban ugyanakkora súlya van egy szónak ha egyszer és ha tízszer szerepel, készíthetünk olyan mátrixot is, ahol egy szónak annál nagyobb a súlya egy dokumentumban, minél többször fordul el. A szógyakoriság (term frequency, TM) szerint súlyozott TD mátrixnál azt is figyelembe vesszük, hogy az adott szó hány dokumentumban szerepel. Minél több dokumentumban szerepel egy szó, annál kisebb a jelentsége. Ilyen szavak például a névelk, amelyek sok dokumentumban elfordulnak ugyan, de nem sok tartalmi jelentséggel bírnak. Két szó közül általában az a fontosabb, amelyik koncentráltan, kevés dokumentumban, de azokon belül nagy gyakorisággal fordul el. A dokumentum gyakorisági érték (document frequency, df) egy szó ritkaságát jellemzi egy korpuszon belül, azaz megadja, hogy mekkora megkülönböztet ereje van egy szónak a dokumentum tartalmára vonatkozóan. A súlyozási sémákban általában a dokumentum gyakorisági érték inverzével számolnak (inverse document frequency, idf) ez a leggyakrabban használt td-idf súlyozás (term frequency &amp; inverse document frequency. Az így súlyozott TD mátrix egy-egy cellájában található érték azt mutatja, hogy egy adott szónak mekkora a jelentsége egy adott dokumentumban. A tf -idf súlyozás értéke tehát magas azon szavak esetén, amelyek az adott dokumentumban gyakran fordulnak el, míg a teljes korpuszban ritkán, alacsonyabb azon szavak esetén, amelyek az adott dokumentumban ritkábban, vagy a korpuszban gyakrabban fordulnak el és kicsi azon szavaknál, amelyek a korpusz lényegében összes dokumentumában elfordulnak (Tikk2007:33-37?) Az alábbiakban az 1999-es törvényszöveken szemlétetjük hogy egy 125 dokumentumból létrehozott mátrix segítségével milyen alapvet statisztikai mveleteket végezhetünk.5 Ehhez elször importáljuk a törvények egy .csv kiterjesztés fájlból. A read_csv() használatának az az elnye, hogy alapbeállításként UTF-8 formátumban importálja be a szöveges oszlopokat. lawtext_df &lt;- read_csv(&quot;data/lawtext_1999.csv&quot;) Majd az importált fájlokból létrehozzuk a korpusz lawtext_corpus néven. Ezt követi a dokumnetum kifejezés mátrix kialakítása (mivel a quanteda csomaggal dolgozunk, dfm mátrixot hozunk létre), és ezzel egy lépésben, elvégezzük az alapvet szövegtisztitó lépéseket is. lawtext_corpus &lt;- corpus(lawtext_df) lawtext_dfm &lt;- dfm( lawtext_corpus, tolower = TRUE, remove = stopwords(&quot;hungarian&quot;), stem = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE ) A topfeatures függvény segítségével megnézhetjük a mátrix leggyakoribb szavait a függvény argumentumában a dokumnetum kifejezés mátrix nevét és a kívánt kifejezésszámot megadva. topfeatures(lawtext_dfm, 15) #&gt; the of szerzodo to b ha and kiadások #&gt; 7902 5665 3619 3290 2831 2794 2712 2447 #&gt; törvéni in következo muködési or évi is #&gt; 2385 2253 2178 2038 2034 1908 1864 Mivel látható, hogy a szövegekben sok angol kifejezés is volt egy következ lépcsben, az angol stopszavakat is eltávolitjuk. lawtext_dfm_2 &lt;- dfm(lawtext_dfm, remove = stopwords(&quot;english&quot;)) Majd ismét megnézzük a leggyakoribb 15 kifejezést. topfeatures(lawtext_dfm_2, 15) #&gt; szerzodo b ha kiadások törvéni #&gt; 3619 2831 2794 2447 2385 #&gt; következo muködési évi állam c #&gt; 2178 2038 1908 1718 1713 #&gt; meghatározott költségveté államban lép fél #&gt; 1654 1637 1622 1616 1533 Ezután tf-idf súlyozású statisztikát készítünk, a dokumentum kifejezés mátrix alapján. Ehhez elször létrehozzuk a lawtext_tfidf nev objektumot, majd a textstat_frequency függvény segítségével, és kilistázzuk annak els 10 elemét. lawtext_tfidf &lt;- dfm_tfidf(lawtext_dfm_2) textstat_frequency(lawtext_tfidf, force = TRUE, n = 10) #&gt; feature frequency rank docfreq group #&gt; 1 kiadások 2120.2303 1 17 all #&gt; 2 felhalmozási 1448.3465 2 7 all #&gt; 3 szerzodo 1378.5012 3 52 all #&gt; 4 költségveté 1302.8556 4 20 all #&gt; 5 shall 1291.1619 5 14 all #&gt; 6 államban 1223.7785 6 22 all #&gt; 7 részes 1155.9688 7 13 all #&gt; 8 muködési 1101.7581 8 36 all #&gt; 9 articl 967.8961 9 14 all #&gt; 10 parti 845.2246 10 20 all Az itt használt kódok az alábbiakon alapulnak: http://www.akosmate.com/QTA_SZISZ_2019/week03_descriptives_i/session3_r_script.html, https://rdrr.io/cran/quanteda/man/dfm_weight.html, https://rdrr.io/cran/quanteda/man/dfm_tfidf.html "],["leíró-statisztika-szózsák-és-szóeloszlások.html", "5 Leíró statisztika: szózsák és szóeloszlások", " 5 Leíró statisztika: szózsák és szóeloszlások negyedik fejezet "],["a-szövegek-reprezentálása-a-vektortérben.html", "6 A szövegek reprezentálása a vektortérben", " 6 A szövegek reprezentálása a vektortérben otodik fejezet "],["a-korpuszépítés-problémái-és-a-szövegel-készítés.html", "7 A korpuszépítés problémái és a szövegelkészítés", " 7 A korpuszépítés problémái és a szövegelkészítés hatodik fejezet "],["szótáralapú-elemzések-érzelem-elemzés.html", "8 Szótáralapú elemzések, érzelem-elemzés 8.1 Szótárak az R-ben 8.2 Magyar Nemzet cikkek 8.3 MNB sajtóközlemények", " 8 Szótáralapú elemzések, érzelem-elemzés A szótár alapú szentiment elemzés egy egyszer ötleten alapul. Hogyha tudjuk hogy egyes szavak milyen érzelmeket, érzéseket, információt hordoznak, akkor minél gyakoribb egy-egy érzelem kategóriához tartozó szó, akkor a szentiment annél inkább jellemz lesz a dokumentumra amit vizsgálunk. Természetesen itt is jó pár dolognak kell teljesülnie ahhoz hogy az elemzésünk eredménye megbízható legyen. Mivel a szótár alapú elemzés az adott szentiment kategórián belüli kulcsszavak gyakoriságán alapul, ezért van aki nem tekinti statisztikai elemzésnek (lásd például Young and Soroka (2012)). A tágabb kvantitatív szövegelemzési kontextusban az osztályozáson (classification) belül a felügyelt módszerekhez hasonlóan itt is ismert kategóriákkal dolgozunk (pl.: egy kulcsszó az öröm kategóriába tartozik), csak egyszerbb módszertannal (Grimmer and Stewart 2013a). A kulcsszavakra építés miatt a módszer a kvalitatív és kvantitatív kutatási vonalak találkozásának is tekinthet, hiszen egy-egy szónak az érzelmi töltete nem mindig ítélhet meg objektíven. Mint minden módszer esetében, amirl ebben a tankönyvben szó van, itt is kiemelten fontos hogy ellenrízzük hogy a használt szótár kategóriák és kulcsszavak fedik-e a valóságot. Más szavakkal: validate, validate, validate. A módszer elnyei: Tökéletesen megbízható: nincsen probabilisztikus eleme a számításoknak, mint például a Support Vector alapú osztályozásnál, illetve az emberi szövegkódolásnál elforduló problémákat is elkerüljük így. Képesek vagyunk vele mérni a szöveg látens dimenzióit. Széles körben alkalmazható, egyszeren számolható. A politikatudományon és számítogépes nyelvtudományokon belül nagyon sok kész szótár elérhet, amik különböz módszerekkel készültek és különböz területet fednek le (pl.: populizmus, pártprogramok policy tartalma, érzelmek, gazdasági tartalom.)6 Relatíve könnyen adaptálható egyik nyelvi környezetbl másikba. A módszer lehetéges hátrányai: A szótár hatékonysága és validitása azon múlik hogy mennyire egyezik a szótár és a viszgálni kívánt dokumentum területe. Például jellemz hiba, hogy gazdasági bizonytalanságot szeretnék tzsdei jelentések alapján vizsgálni a kutatók egy általános szentimet szótár használatával. A terület-specifikus szótár építése egy kvalitatív folyamat (lsd. a labjegyzetben), éppen ezért gyakran id és emberi erforrás igényes. A szózsák alapú elemzéseknél a kontextus elvész (ez gyakran igaz a bigram és trigramok használatánál is) a kulcsszavak esetében. Erre egy triviális példa a tagadás a mondatban: nem vagyok boldog esetén egy általános szentiment szótár a tagadás miatt félreosztályozná a mondat érzelmi töltését. Az elemzés sikere több faktortól is függ. Fontos hogy a korpuszban lév dokumentumokat körültekinten tisztítsuk meg az elemzés elején (lásd a 4. fejezetet a szövegelkészítésrl). A következ lépésben meg kell bizonyosodnunk arról, hogy a kiválasztott szentiment szótár alkalmazható a korpuszunkra. Amennyiben nem találunk alkalmas szótárat, akkor a saját szótár validálására kell figyelni. A negyedik fejezetben leírtak itt is érvényesek, érdemes a dokumentum-kifejezés mátrixot súlyozni valamilyen módon. 8.1 Szótárak az R-ben A szótár alapú elemzéshez a quanteda csomagot fogjuk használni, illetve a 3. fejezetben már megismert readr, stringr, dplyr tidyverse csomagokat.7 library(readr) library(stringr) library(dplyr) library(quanteda) Mieltt a két esettanulmányt bemutatnánk, vizsgáljuk meg hogy hogyan néz ki egy szentiment szótár az R-ben. A szótárt kézzel úgy tudjuk létrehozni, hogy egy listán belül létrehozzuk karaktervektorként a kategóriákat és a kulcsszavakat és ezt a listát a quanteda dictionary függvényével eltároljuk. szentiment_szotar &lt;- dictionary(list( pozitiv = c(&quot;jó&quot;, &quot;boldog&quot;, &quot;öröm&quot;), negativ = c(&quot;rossz&quot;, &quot;szomorú&quot;, &quot;lehangoló&quot;) )) szentiment_szotar #&gt; Dictionary object with 2 key entries. #&gt; - [pozitiv]: #&gt; - jó, boldog, öröm #&gt; - [negativ]: #&gt; - rossz, szomorú, lehangoló A quanteda, quanteda.corpora és tidytext R csomagok több széles körben használt szentiment szótárat tartalmaznak, így nem kell kézzel replikálni minden egyes szótárat amit használni szeretnénk. A szentiment elemzési munkafolyamat amit a részfejezetben bemutatunk a következ lépésekbl áll: dokumentumok betöltése szöveg elkészítése a korpusz létrehozása dokumentum-kifejezés mátrix szótár betöltése a dokumentum-kifejezés mátrix szrése a szótárban lév kulcsszavakkal az eredmény vizualizálása, további felhasználása A fejezetben két különböz korpuszt fogunk elemezni: a 2006-os Magyar Nemzet címlapjainak egy 252 cikkbl álló mintájának szentimentjét vizsgáljuk egy magyar szentiment szótárral. A második korpusz a Magyar Nemzeti Bank angol nyelv sajtóközleményeibl áll, amin bemutatjuk egy széles körben használt gazdasági szótár használatát. 8.2 Magyar Nemzet cikkek mn_minta &lt;- read_csv(&quot;data/magyar_nemzet_small.csv&quot;) summary(mn_minta) #&gt; doc_id text doc_date #&gt; Min. : 1.0 Length:2834 Min. :2006-01-02 #&gt; 1st Qu.: 709.2 Class :character 1st Qu.:2006-03-29 #&gt; Median :1417.5 Mode :character Median :2006-06-28 #&gt; Mean :1417.5 Mean :2006-06-28 #&gt; 3rd Qu.:2125.8 3rd Qu.:2006-09-26 #&gt; Max. :2834.0 Max. :2006-12-29 A read_csv() segítségével beolvassuk a Magyar Nemzet adatbázis egy kisebb részét, ami az esetünkben a 2006-os cimlapokon szerepl hírek . A summary(), ahogy a neve is mutatja, egy gyors áttekinténtést nyújt a betöltött adatbázisról. Látjuk, hogy 2834 sorbol (megfigyelés) és 3 oszlopból (változó) áll. Az els ránézésre látszik hogy a text változónk tartalmazza a szövegeket, és hogy tisztításra szorulnak. Az els szöveget megnézve látjuk, hogy a standard elkészítési lépések mellett a sortörést (\\n) is ki kell törölnünk. mn_minta$text[1] #&gt; [1] &quot;Hat fovárosi képviselo öt percnél is kevesebbet beszélt egy év alatt a közgyulésben.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n&quot; Habár a quanteda is lehetséget ad néhány elékészít lépésre, érdemes ezt olyan céleszközzel tenni ami nagyobb rugalmasságot ad a kezünkbe. Mi erre a célra a stringr csomagot használjuk. Els lépésben kitöröljük a sortöréseket (\\n), a központozást, számokat, kisbetsítünk minden szót. Elfordulhat hogy (számunkra nehezen látható) extra szóközök maradnak a szövegben. Ezeket az str_squish()-el tüntetjük el. A szöveg eleji és végi extra szóközöket (ún. leading vagy trailing white space) az str_trim() vágja le. mn_tiszta &lt;- mn_minta %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;\\n&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) A szöveg sokkal jobban néz ki, habár észrevehetjük hogy maradhattak benne problémás részek, fleg a sortörés miatt, ami sajnos hol egyes szavak közepén van (a jobbik eset), vagy pedig pont szóhatáron, ez esetben a két szó sajnos összevonódik. Az egyszerség kedvéért feltételezzük hogy ez kellen ritkán fordul el ahhoz hogy ne befolyásolja az elemzésünk eredményét. mn_tiszta$text[1] #&gt; [1] &quot;hat fovárosi képviselo öt percnél is kevesebbet beszélt egy év alatt a közgyulésben&quot; Miután kész a tiszta(bb) szövegünk, kreálunk egy korpuszt a quanteda corpus() fuggvenyevel. A létrehozott corpus objektum a szöveg mellett egyéb dokumentum meta adatokat is tud tárolni (dátum, író, hely, stb.) Ezeket mi is hozzáadhatjuk (erre majd látunk példát nemsokára) illetve amikor létrehozzuk a korpuszt a data frame-ünkbl, akkor autómatikusan meta adatokként tárolódnak az változóink. Jelen esetben az egyetlen dokument változónk az a dátum lesz a szöveg mellett. A korpusz dokumentum változóihoz a docvars() segíségével tudunk hozzáférni. mn_corpus &lt;- corpus(mn_tiszta) head(docvars(mn_corpus), 5) #&gt; doc_date #&gt; 1 2006-01-02 #&gt; 2 2006-01-02 #&gt; 3 2006-01-02 #&gt; 4 2006-01-02 #&gt; 5 2006-01-02 A következ lépés a dokument-kifejezés mátrix létrehozása a dfm() függvénnyel (ami a document-feature matrix rövidítése). Elszr tokenekre bontjuk a szövegeket a tokens()-el, és aztán ezt a tokenizált szózsákot kapja meg a dfm inputnak. A sornak a végén a létrehozott mátrixunkat TF-IDF módszerrel súlyozzuk a dfm_tfidf() használatával. mn_dfm &lt;- mn_corpus %&gt;% tokens(what = &quot;word&quot;) %&gt;% dfm() %&gt;% dfm_tfidf() A cikkek szentimentjét egy magyar szótárral fogjuk becsülni, amit a Társadalomtudmányi Kutatóközpont CSS-RECENS és a POLTEXTLab kutatói készítették.8 Két dimenziót tarlamaz (pozitív és negatív), 2299 pozitív és 2588 negatív kulcsszóval. Ez nem számít kirívóan nagynak a szótárak között, mivel az adott kategóriák minél teljesebb lefedése a cél. Azt is látjuk, hogy a kulcsszavak egyszavas tokenek (szóval nem érdemes bigramokat és trigramokat készítenünk a tokenizálás során), illetve nem szótövek (így szótöveznünk sem kell). poltext_szotar #&gt; Dictionary object with 2 key entries. #&gt; - [positive]: #&gt; - abszolút, ad, adaptív, adekvát, adócsökkentés, adókedvezmény, adomány, adományoz, adóreform, adottság, adottságú, áfacsökkentés, agilis, agytröszt, áhított, ajándék, ajándékoz, ajánl, ajánlott, akadálytalan [ ... and 2,279 more ] #&gt; - [negative]: #&gt; - aberrált, abnormális, abnormalitás, abszurd, abszurditás, ádáz, adócsalás, adócsaló, adós, adósság, áfacsalás, áfacsaló, affér, aggasztó, aggodalom, aggódik, aggódás, agresszió, agresszíven, agresszivitás [ ... and 2,568 more ] Az egyes dokumentumok szentimentjét a dfm_lookup() becsüli, ahol az elz lépésben létrehozott súlyozott dfm az input és a magyar szentimentszótár a dictionary. Egy gyors pillantás az eredményre és látjuk hogy minden dokumentumhoz készült egy pozitív és egy negatív értéket. A TF-IDF súlyozás miatt nem látunk egész számokat (a súlyozás nélkül a sima szófrekvenciát kapnánk). mn_szentiment &lt;- dfm_lookup(mn_dfm, dictionary = poltext_szotar) head(mn_szentiment, 5) #&gt; Document-feature matrix of: 5 documents, 2 features (40.0% sparse) and 1 docvar. #&gt; features #&gt; docs positive negative #&gt; 1 0 0 #&gt; 2 0.8375026 12.497973 #&gt; 3 0 0 #&gt; 4 21.1044299 6.449036 #&gt; 5 11.0358129 8.131890 Ahhoz hogy fel tudjuk használni a kapott eredményt, érdemes dokumentumváltozóként eltárolni a korpuszban. Ezt a fent már használt docvars() segítségével tudjuk megtenni, ahol a második argumentumkét az új változó nevét adjuk meg stringként. docvars(mn_corpus, &quot;pos&quot;) &lt;- as.numeric(mn_szentiment[, 1]) docvars(mn_corpus, &quot;neg&quot;) &lt;- as.numeric(mn_szentiment[, 2]) head(docvars(mn_corpus), 5) #&gt; doc_date pos neg #&gt; 1 2006-01-02 0.0000000 0.000000 #&gt; 2 2006-01-02 0.8375026 12.497973 #&gt; 3 2006-01-02 0.0000000 0.000000 #&gt; 4 2006-01-02 21.1044299 6.449036 #&gt; 5 2006-01-02 11.0358129 8.131890 Végül a kapott korpuszt a kiszámolt szentiment értékekkel a quanteda-ban lév convert() fügvénnyel data frame-é alakítjuk. A convert függvény dokumentációját érdemes elolvasni, mert ennek segítségével tudjuk a quanteda-ban elkeszült objektumainkat átalakítani úgy, hogy azt más csomagok is tudják használni. mn_df &lt;- convert(mn_corpus, to = &quot;data.frame&quot;) summary(mn_df) #&gt; doc_id text doc_date pos #&gt; Length:2834 Length:2834 Min. :2006-01-02 Min. : 0.000 #&gt; Class :character Class :character 1st Qu.:2006-03-29 1st Qu.: 0.000 #&gt; Mode :character Mode :character Median :2006-06-28 Median : 2.373 #&gt; Mean :2006-06-28 Mean : 4.074 #&gt; 3rd Qu.:2006-09-26 3rd Qu.: 6.280 #&gt; Max. :2006-12-29 Max. :35.648 #&gt; neg #&gt; Min. : 0.000 #&gt; 1st Qu.: 0.000 #&gt; Median : 2.037 #&gt; Mean : 3.528 #&gt; 3rd Qu.: 5.348 #&gt; Max. :39.096 Mieltt vizualizálnánk az eredményt érdemes a napi szintre aggregálni a szentimentet és egy nettó értéket kalkulálni.9 mn_df &lt;- mn_df %&gt;% group_by(doc_date) %&gt;% summarise( daily_pos = sum(pos), daily_neg = sum(neg), net_daily = daily_pos - daily_neg ) A plot alapján egyértelm trendet nem lehet megállapítani és még a 2006-os év végi turbulens belpolitikai események sem feltétlenül jelennek meg markánsan. Ennek az oka abban is kereshet, hogy egy napilap címlapját ritkán dominálja teljes egészében a belpolitika és így a negatív és pozitív szentimentek kioltják egymást. Természetesen messzemen következtetéseket egy ábra alapján nem érdemes levonni, de az elemzésünk azt mutatha hogy a nyári hónapok alatt kevesebb volt az igazán negatív címlap, ellenben az év eleje és vége tartalmazta a minta alsó széls értékeit. library(ggplot2) ggplot(mn_df, aes(doc_date, net_daily)) + geom_line() + labs( title = &quot;Magyar Nemzet címlap szentimentje&quot;, subtitle = &quot;A szentiment érték a pozitív és negatív szentiment pontszámok különbsége a teljes mintára.&quot;, y = &quot;Szentiment&quot;, x = NULL, caption = &quot;Adatforrás: https://cap.tk.hu/&quot; ) 8.3 MNB sajtóközlemények A második esettanulmányban a kotextuális szótár elemzést mutatjuk be egy angol nyelv korpusz és specializált szótár segítségével. A korpusz az MNB kamatdöntéseit kísér nemzetközi sajtóközleményei és a szótár pedig a Loughran and McDonald (2011) pénzügyi szentimentszótár.10 A szótár a quanteda.dictionaries csomag részeként elérhet, illetve a tankönyv honlapján is megtalálható. penzugy_szentiment #&gt; Dictionary object with 9 key entries. #&gt; - [NEGATIVE]: #&gt; - abandon, abandoned, abandoning, abandonment, abandonments, abandons, abdicated, abdicates, abdicating, abdication, abdications, aberrant, aberration, aberrational, aberrations, abetting, abnormal, abnormalities, abnormality, abnormally [ ... and 2,335 more ] #&gt; - [POSITIVE]: #&gt; - able, abundance, abundant, acclaimed, accomplish, accomplished, accomplishes, accomplishing, accomplishment, accomplishments, achieve, achieved, achievement, achievements, achieves, achieving, adequately, advancement, advancements, advances [ ... and 334 more ] #&gt; - [UNCERTAINTY]: #&gt; - abeyance, abeyances, almost, alteration, alterations, ambiguities, ambiguity, ambiguous, anomalies, anomalous, anomalously, anomaly, anticipate, anticipated, anticipates, anticipating, anticipation, anticipations, apparent, apparently [ ... and 277 more ] #&gt; - [LITIGIOUS]: #&gt; - abovementioned, abrogate, abrogated, abrogates, abrogating, abrogation, abrogations, absolve, absolved, absolves, absolving, accession, accessions, acquirees, acquirors, acquit, acquits, acquittal, acquittals, acquittance [ ... and 883 more ] #&gt; - [CONSTRAINING]: #&gt; - abide, abiding, bound, bounded, commit, commitment, commitments, commits, committed, committing, compel, compelled, compelling, compels, comply, compulsion, compulsory, confine, confined, confinement [ ... and 164 more ] #&gt; - [SUPERFLUOUS]: #&gt; - aegis, amorphous, anticipatory, appertaining, assimilate, assimilating, assimilation, bifurcated, bifurcation, cessions, cognizable, concomitant, correlative, deconsolidation, delineation, demonstrable, demonstrably, derecognized, derecognizes, derivatively [ ... and 36 more ] #&gt; [ reached max_nkey ... 3 more keys ] A szentiment szótár 9 kategóriából áll. A legtöbb kulcsszó a negatív dimenzióhoz van (2355). A munkamenet hasonló a Magyar Nemzetes példához: adat betöltés szövegtisztítás korpusz tokenek kulcs kontextuális tokenek szrése dfm elállítás és szentiment számítás az eredmény vizualizálása, további felhasználása mnb_pr &lt;- read_csv(&quot;data/mnb_pr_corpus.csv&quot;) summary(mnb_pr) #&gt; date text id year #&gt; Min. :2005-01-24 Length:180 Min. : 1.00 Min. :2005 #&gt; 1st Qu.:2008-10-14 Class :character 1st Qu.: 45.75 1st Qu.:2008 #&gt; Median :2012-07-10 Mode :character Median : 90.50 Median :2012 #&gt; Mean :2012-07-08 Mean : 90.50 Mean :2012 #&gt; 3rd Qu.:2016-03-30 3rd Qu.:135.25 3rd Qu.:2016 #&gt; Max. :2019-12-17 Max. :180.00 Max. :2019 Az adatbázisunk 180 megfigyelésbl és 4 változóbol áll. Az egyetlen lényeges dokumentum meta adat itt is a szövegek megjelenési ideje. A szövegeket ugyanazokkal a standard eszközökkel kezeljük mint a Magyar Nemzet esetében. Érdemes minden esetben ellenrízni, hogy az R kód amit használunk az tényleg azt csinálja-e mint amit szeretnénk hogy csináljon. Ez hatványozottan igaz abban az esetben, amikor szövegekkel és regular expressionökkel dolgozunk. mnb_tiszta &lt;- mnb_pr %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) Miután rendelkezésre állnak a tiszta dokumentumaink, egy karaktervektorba gyüjtjuk azokat a kulcsszavakat amelyek környékén szeretnénk megfigyelni a szentiment alakulását. A példa kedvéért mi az unemp*, growth, gdp, inflation* szótöveket és szavakat választottuk. A tokens_keep() megtartja a kulcsszavainkat és egy általunk megadott +/- n tokenes környezetüket (jelen esetben 10). A szentiment elemzést pedig már ezen a jóval kisebb mátrixon fogjuk lefuttatni. A phrase() segítségével több szóból álló kifejezéséket is vizsgálhatunk. Ilyen szókapcsolat például az Európai Unió is, ahol lényeges hogy egyben kezeljük a két szót. mnb_corpus &lt;- corpus(mnb_tiszta) gazdasag &lt;- c(&quot;unemp*&quot;, &quot;growth&quot;, &quot;gdp&quot;, &quot;inflation*&quot;, &quot;inflation expectation*&quot;) mnb_token &lt;- tokens(mnb_corpus) %&gt;% tokens_keep(pattern = phrase(gazdasag), window = 10) A szentimentet most is egy súlyozott dfm-bl számoljuk. A kész eredményt hozzáadjuk a korpuszhoz majd data framet hozunk létre belle. A 9 kategóriából 5-öt adunk választunk csak ki, amelyeknek jegybanki környezetben értelmezhet tartalma van. mnb_szentiment &lt;- tokens_lookup(mnb_token, dictionary = penzugy_szentiment) %&gt;% dfm() %&gt;% dfm_tfidf() docvars(mnb_corpus, &quot;negative&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;negative&quot;]) docvars(mnb_corpus, &quot;positive&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;positive&quot;]) docvars(mnb_corpus, &quot;uncertainty&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;uncertainty&quot;]) docvars(mnb_corpus, &quot;constraining&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;constraining&quot;]) docvars(mnb_corpus, &quot;superfluous&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;superfluous&quot;]) mnb_df &lt;- convert(mnb_corpus, to = &quot;data.frame&quot;) A célunk hogy szentiment kategóriánkénti bontásban mutassuk be az elemzésünk eredményét, de eltte egy kicsit alakítani kell a data frame-n, hogy a második fejezetben is tárgyalt tidy formára hozzuk. A különböz szentiment értékeket tartalmazó oszlopokat fogjuk átrendezni úgy hogy kreálunk egy sent_type változót ahol a kategória nevet fogjuk eltárolni és egy sent_score változót, ahol a szentiment értéket. Ehhez a tidyr-ben található pivot_longer() -t használjuk. mnb_df &lt;- mnb_df %&gt;% pivot_longer( cols = negative:superfluous, names_to = &quot;sent_type&quot;, values_to = &quot;sent_score&quot; ) Az átalakítás után már könnyedén tudjuk kategóriákra bontva megjeleníteni az MNB közlemények különböz látens dimenzióit. Fontos emlékezni arra, hogy ez az eredmény a kulcsszavaink +/- 10 tokenes környezetében lév szavak szentimentjét mérik. Ami érdekes eredmény, hogy a felesleges töltelék szövegek (superflous kategória) szinte soha nem fordulnak el a kulcsszavaink körül. A többi érték is nagyjából megfelel a várakozásainknak, habár a 2008-as gazdasági válság nem tnik kiugró pontnak. Azonban a 2010 utáni európai válság már láthatóan megjelnik az idsorainkban. A szótár amit használtunk az alapveten az Egyesült Államokban a tzsdén kereskedett cégek publikus beszámolóiból készült így elképzelhet, hogy egyes jegybanki környezetben sokat használt kifejezés nincs benne. A validálása a kapott eredményeknek ezért is nagyon fontos, illetve érdemes azzal is tisztában lenni hogy a szótáras módszer nem tökéletes (ahogy az emberi vagy más gépi kódolás sem). ggplot(mnb_df, aes(date, sent_score)) + geom_line() + labs( title = &quot;Magyar Nemzeti Bank közleményeinek szentimentje&quot;, y = &quot;Szentiment&quot;, x = NULL ) + facet_wrap(~sent_type, ncol = 2) A lehetséges, területspecifikus szótáralkotási módszerekrl részletesebben ezekben a cikkekben lehet olvasni: Laver and Garry (2000); Young and Soroka (2012); Loughran and McDonald (2011); Máté, Sebk, and Barczikay (2021) A szentiment elemzéshez gyakran használt csomag még a tidytext. Az online is szabadon elérhet Silge and Robinson (2017) 2. fejezetében részletesen is bemutatják a szerzk a tidytext munkafolyamatot (https://www.tidytextmining.com/sentiment.html). A szótár elérhet a könyv online verziójának a GitHub repozitorijából. (LINK) A csoportosított adatokkal való munka bvebb bemutatását lsd. a Függelékben A témával részletesebben is foglalkoztunk a Máté, Sebk, and Barczikay (2021) tanulmányban, ahol egy saját monetáris szentiment szótárt mutatunk be. Az implementáció és a hozzá tartozó R forráskód a nyilvános https://doi.org/10.6084/m9.figshare.13526156.v1 linken. "],["felügyelet-nélküli-tanulás-topik-modellezés-magyar-törvényszövegeken.html", "9 Felügyelet nélküli tanulás: Topik modellezés magyar törvényszövegeken 9.1 K közép klaszterezés kvalitatív adatokkal 9.2 Látens Dirichlet Allokáció topik modellek11 9.3 Struktúrális topik modellek", " 9 Felügyelet nélküli tanulás: Topik modellezés magyar törvényszövegeken A klaszterezés egy adathalmaz pontjainak, rekordjainak hasonlóság alapján való csoportosítása, ami szinte minden nagyméret adathalmaz leíró modellezésére alkalmas. A klaszterezés során az adatpontokat diszjunkt halmazokba, azaz klaszterekbe soroljuk, hogy az elemeknek egy olyan partíciója jöjjön létre, amelyben a közös csoportokba kerül elempárok lényegesen hasonlóbbak egymáshoz, mint azok a pontpárok, melyek két különböz csoportba sorolódtak. Klaszterezés során a megfelel csoportok kialakítása nem egyértelm feladat, mivel a különböz adatok eltér jelentése és felhasználása miatt adathalmazonként más szempontokat kell figyelembe vennünk. Egy klaszterezési feladat megoldásához ismernünk kell a különböz algoritmusok alapvet tulajdonságait és mindig szükség van az eredményként kapott klaszterezés kiértékelésére. Mivel egy klaszterezés az adatpontok hasonlóságából indul ki, ezért az eljárás során az els fontos lépés az adatpontok páronkénti hasonlóságát lehet legjobban megragadó hasonlósági függvény kiválasztása Tan, Steinbach, and Kumar (2011). Számos klaszterezési eljárás létezik, melyek között az egyik leggyakoribb különbségtétel, hogy a klaszterek egymásba ágyazottak vagy sem. Ez alapján beszélhetünk hierarchikus és felosztó klaszterezésrl. A hierarchikus klaszterezés egymásba ágyazott klaszterek egy fába rendezett halmaza, azaz ahol a klaszterek alklaszterekkel rendelkeznek. A fa minden csúcsa (klasztere), a levélcsúcsokat kivéve, a gyermekei (alklaszterei) uniója, és a fa gyökere az összes objektumot tartalmazó klaszter. Felosztó (partitional) klaszterezés esetén az adathalmazt olyan, nem átfed alcsoportokra bontjuk, ahol minden adatobjektum pontosan egy részhalmazba kerül Tan, Steinbach, and Kumar (2011), Tikk (2007a). A klaszterezési eljárások között aszerint is különbséget tehetünk, hogy azok egy objektumot csak egy vagy több klaszterbe is beilleszthetnek. Ez alapján beszélhetünk kizáró (exclusive), illetve nem-kizáró (non exclusive), vagy átfed (overlapping) klaszterezésrl. Az elbbi minden objektumot csak egyetlen klaszterhez rendel hozzá, az utóbbi esetén egy pont több klaszterbe is beleillik. Fuzzy klaszterezés esetén minden objektum minden klaszterbe beletartozik egy tagsági súly erejéig, melynek értéke 0 (egyáltalán nem tartozik bele) és 1 (teljesen beletartozik) közé esik. A klasztereknek is különböz típusai vannak, így beszélhetünk prototípus-alapú, gráf-alapú vagy srség-alapú klaszterekrl. A prototípus-alapú klaszter olyan objektumokat tartalmazó halmaz, amelynek mindegyik objektuma jobban hasonlít a klasztert definiáló objektumhoz, mint bármelyik másik klasztert definiáló objektumhoz. A prototípus-alapú klaszter klaszterek közül a K-közép klaszter az egyik leggyakrabban alkalmazott. A K-közép klaszterezési módszer els lépése K darab kezd középpontot kijelölése, ahol K a klaszterek kívánt számával egyenl. Ezután minden adatpontot a hozzá legközelebb es középponthoz rendelünk. Az így képzett csoportok lesznek a kiinduló klaszterek. Ezután újra meghatározzuk mindegyik klaszter középpontját a klaszterhez rendelt pontok alapján. A hozzárendelési és frissítési lépéseket felváltva folytatjuk addig, amíg egyetlen pont sem vált klasztert, vagy ameddig a középpontok ugyanazok nem maradnak Tan, Steinbach, and Kumar (2011). 9.1 K közép klaszterezés kvalitatív adatokkal A K közép klaszterezés tehát a dokumentumokat alkotó szavak alapján keresi meg a felhasználó által megadott számú (K) klasztert, amelyeket a középpontjaik képviselnek, és így rendezi a dokumentumokat csoportokba. A klaszterezés vagy csoportosítás egy induktív kategorizálás, ami akkor hasznos, amikor nem állnak a kutató rendelkezésére elzetesen ismert csoportok, amelyek szerint a vizsgált dokumentumokat rendezni tudná. Hiszen ebben az esetben a korpusz elemeinek rendezéséhez nem határozunk meg elzetesen csoportokat, hanem az eljárás során olyan különálló csoportokat hozunk létre a dokumentumokból, amelynek tagjai valamilyen szempontból hasonlítanak egymásra. A csoportosítás legfbb célja az, hogy az egy csoportba kerül szövegek minél inkább hasonlítsanak egymásra, miközben a különböz csoportba kerülk minél inkább eltérjenek egymástól. Azaz klaszterezésnél nem egy-egy szöveg jellemzire vagyunk kíváncsiak, hanem arra, hogy a szövegek egy-egy csoportja milyen hasonlóságokkal bír Tikk (2007a), Burtejin (2016). A gépi kódolással végzett klaszterezés egy felügyelet nélküli tanulás, mely a szöveg tulajdonságaiból tanul, anélkül, hogy elre meghatározott csoportokat ismerne. Alkalmazása során a dokumentum tulajdonságait és a modell becsléseit felhasználva jönnek létre a különböz kategóriák, melyekhez késbb hozzárendeli a szöveget Grimmer and Stewart (2013b) . Az osztályozással ellentétben a csoportosítás esetén tehát nincs ismert címkékkel\" ellátott kategóriarendszer vagy olyan minta, mint az osztályozás esetében a tanítókörnyezet, amibl tanulva a modellt fel lehet építeni Tikk (2007a). A gépi kódolással végzett csoportosítás (klaszterezés) esetén a kutató feladata a megfelel csoportosító mechanizmus kiválasztása, mely alapján egy program végzi el a szövegek különböz kategóriákba sorolását. Ezt követi a hasonló szövegeket tömörít csoportok elnevezésének lépése. A több dokumentumból álló korpuszok esetében a gépi klaszterelemzés különösen eredményes és költséghatékony lehet, mivel egy nagy korpusz vizsgálata sok erforrást igényel @grimmer2013texta:1. A klaszterezés bemutatásához a rendszerváltás utáni magyar miniszterelnökök egy-egy véletlenszeren kiválasztott beszédét használjuk. library(readr) library(dplyr) library(stringr) library(readtext) library(quanteda) library(ggplot2) library(topicmodels) library(factoextra) A beszédek szövege meglehetsen tiszta, ezért az egyszerség kedvéért a most kihagyjuk a szövegtisztítás lépéseit. Az elemzés els lépéseként a quanteda csomaggal egy korpusz kreálunk, majd abból egy dokumentum-kifejezés mátrixot készítünk a dfm() függvénnyel. beszedek &lt;- read_csv(&quot;data/miniszterelnokok.csv&quot;) beszedek_corpus &lt;- corpus(beszedek) beszedek_dfm &lt;- dfm(beszedek_corpus) A beszédek klaszterekbe rendezését az R egyik alapfüggvénye végzi, a kmeans. Els lépésben 3 klasztert készítünk. A table() függvénnyel megnézhetjük hogy egy-egy csoportba hány dokumentum került. beszedek_klaszter &lt;- kmeans(beszedek_dfm, centers = 2) table(beszedek_klaszter$cluster) #&gt; #&gt; 1 2 #&gt; 5 2 A felügyelet nélküli klasszifikáció nagy kérdése, hogy hány klasztert készítsünk, hogy megközelítsük a valóságot és ne csak mesterségesen kreáljunk csoportokat abban az esetben is amikor ténylegesen nem léteznek. A kvalitatív megközelítések mellett kvantitatív opciók is vannak. A factoextra csomagban több ilyen módszer is van implementálva. A lenti ábra azt mutatja hogy a klasztereken belüli négyzetösszegek hogyan változnak a k paraméter változásának függvényében. A lenti ábra alapján az ideális klaszter szám 2. fviz_nbclust(as.matrix(beszedek_dfm), kmeans, method = &quot;wss&quot;, k.max = 5) Vizuálisan is megjeleníthetjük a kialakított csoportokat. fviz_cluster(beszedek_klaszter, data = beszedek_dfm) 9.2 Látens Dirichlet Allokáció topik modellek11 A topik-modellezés a dokumentumok téma-klasztereinek meghatározására szolgáló valószínség-alapú eljárás, amely szó-gyakoriságot állapít meg minden témához, és minden dokumentumhoz hozzárendeli az adott témák valószínségét. A topik modellezés egy felügyelet nélküli tanulási módszer, amely során az alkalmazott algoritmus a dokumentum tulajdonságait és a modell becsléseit felhasználva hoz létre különböz kategóriákat, melyekhez késbb hozzárendeli a szöveget Tikk (2007b), Grimmer and Stewart (2013b), Burtejin (2016) . Az egyik leggyakrabban alkalmazott topik modellezési eljárás, a Látens Dirichlet Allokáció (LDA) alapja az a feltételezés, hogy minden korpusz topikok/témák keverékébl áll, ezen témák pedig statisztikailag a korpusz szókészlete valószínségi függvényeinek (eloszlásának) tekinthetek Blei, Ng, and Jordan (2003) . Az LDA a korpusz dokumentumainak csoportosítása során az egyes dokumentumokhoz topik szavakat rendel, a topikok megbecsléséhez pedig a szavak együttes megjelenését vizsgálja a dokumentum egészében. Az LDA algoritmusnak elzetesen meg kell adni a keresett klaszterek (azaz a keresett topikok) számát, ezt követen a dokumentumhalmazban szerepl szavak eloszlása alapján az algoritmus azonosítja a kulcsszavakat, amelyek eloszlása kirajzolja a topikokat Blei, Ng, and Jordan (2003), Burtejin (2016), Jacobi, Van Atteveldt, and Welbers (2016) . A következkben a magyar törvények korpuszán szemléltetjük a topik modellezés módszerét, hogy a mesterséges intelligencia segítségével feltárjuk a korpuszon belüli rejtett összefüggéseket. A korábban leírtak szerint tehát nincsenek elre meghatározott kategóriáink, dokumentumainkat a klaszterezés segítségével szeretnénk csoportosítani. Egy-egy dokumentumban keveredhetnek a témák és az azokat reprezentáló szavak. Mivel ugyanaz a szó több topikhoz is kapcsolódhat, így az eljárás komplex elemzési lehetséget nyújt, az egy szövegen belül témák és akár azok dokumentumon belüli súlyának azonosítására. Példánkban csak a korpusz egy részén szemléltetjük a topik modellezést, a teljes korpusz és annak elemzéséhez szükséges kód elérhet az alábbi github linken: https://github.com/poltextlab Az alábbiakban 1998-2002 és a 2002-2006-os parlamenti ciklus 1032 törvényszövegének topik modellezését és a szükséges elkészít, korpusztisztító lépéseket mutatjuk be. Az következkben használt fájlok letölthetek az alábbi github linkrl: https://github.com/poltextlab/text_mining_with_r A fájlokat töltsük be az R által használt munkakönyvtárba.12 Töltsük be az elemezni kívánt csv fájlt, megadva az elérési útvonalát. torvenyek &lt;- read_csv(&quot;data/lawtext_1998_2006.csv&quot;) Az elz fejezetekben láthattuk hogy hogyan lehet használni a stringr csomagot a szövegtisztításra. A lépések a már megismert sztenderd folyamatot követik: számok, központozás, sortörések, extra szóközök eltávolítása, illetve a szöveg kisbetsítése. Az eddigieket további szövegtisztító lépésekkel is kiegészíthetjük. Olyan elemek esetében, amelyek nem feltétlenül különálló szavak és el akarjuk távolítani ket a korpuszból szintén az str_remove_all() a legegyszerbb megoldás. torvenyek_tiszta &lt;- torvenyek %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;«&quot;), text = str_remove_all(string = text, pattern = &quot;»&quot;), text = str_remove_all(string = text, pattern = &quot;§&quot;), text = str_remove_all(string = text, pattern = &quot;°&quot;), text = str_remove_all(string = text, pattern = &quot;&lt;U+25A1&gt;&quot;), text = str_remove_all(string = text, pattern = &quot;&lt;U+25A1&gt;&quot;), text = str_remove_all(string = text, pattern = &quot;@&quot;) ) A dokumentum változókat egy külön fájlból adjuk hozzá, ami a törvények keletkezési évét tartalmazza, illetve hogy melyik kormányzati ciklusban születtek. Mindkét adatbázisban egy közös egyedi azonosító jelöli az egyes törvényeket, így ki tudjuk használni a dplyr left_join() függvényét, ami hatékonyan és gyorsan kapcsol össze adatbázisokat közös egyedi azonosító mentén. Jelen esetben ez az egyedi azonosító a txt_filename oszlopból fog elkészülni, amely a torvenyek neveit tartalmazza. Els lépésben betöltjük a meta adatokat tartalmazó .csv fájlt, majd a .txt rész eltti törvényneveket tartjuk csak meg a létrehozott doc_id- oszlopban. A [^\\\\.]* regular expression itt a string elejétl indulva kijelöl mindent az elso . karakterig. Az str_extract() pedig ezt a kijelölt string szakaszt (ami a törvények neve) menti át az új változónkba. torveny_meta &lt;- read_csv(&quot;data/cap_law_meta.csv&quot;) torveny_meta &lt;- torveny_meta %&gt;% mutate(doc_id = str_extract(txt_filename, &quot;[^\\\\.]*&quot;)) %&gt;% select(-txt_filename) head(torveny_meta, 5) #&gt; # A tibble: 5 x 4 #&gt; year electoral_cycle majortopic doc_id #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1998 1998-2002 13 1998XXXV #&gt; 2 1998 1998-2002 20 1998XXXVI #&gt; 3 1998 1998-2002 3 1998XXXVII #&gt; 4 1998 1998-2002 6 1998XXXVIII #&gt; 5 1998 1998-2002 13 1998XXXIX Végül összefzzük a dokumentumokat és a meta adatokat tartalmazó data frameket. torveny_final &lt;- left_join(torvenyek_tiszta, torveny_meta, by = &quot;doc_id&quot;) Majd hozzuk létre a korpuszt és ellenrizzük azt. #&gt; Text Types Tokens Sentences year electoral_cycle majortopic #&gt; 1 1998L 2879 9628 1 1998 1998-2002 3 #&gt; 2 1998LI 352 680 1 1998 1998-2002 20 #&gt; 3 1998LII 446 992 1 1998 1998-2002 9 #&gt; 4 1998LIII 126 221 1 1998 1998-2002 9 #&gt; 5 1998LIV 835 2013 1 1998 1998-2002 9 Az RStudio environments fülén láthatjuk, hogy egy 1032 elembl álló korpusz jött létre, amelynek tartalmát a summary() paranccsal kiíratva a console ablakban megjelenik a dokumentumok listája és a fbb leíró statisztikai adatok (egyedi szavak - types; szószám - tokens; mondatok - sentences). Az elbbi fejezettl eltéren most a tokenizálás során is végzünk még egy kis tisztítást: a felesleges stop szavakat kitöröljük a tokens_remove() és stopwords() kombinálásával. A quanteda tartalmaz egy beépített magyar stopszó szótárat. A második lépésben szótövesítjük a tokeneket a tokens_words() használatával, ami szintén képes a magyar nyelv szövegeket kezelni. Szükség esetén a beépített magyar nyelv stopszó szótárat saját stopszavakkal is kiegészíthetjük. Ehhez elször csv fájlba el kell mentenünk a stopszavakat, majd a csv fájlt be kell olvasnunk. Az pull() egy karaktervektort fog kreálni a data frame text oszlopából. custom_stopwords &lt;- readtext(&quot;data/custom_legal_stopwords.csv&quot;, encoding = &quot;UTF8&quot;) %&gt;% pull(text) Mivel jogi szövegekrl van szó, ezért még egy kis extra szószedetet is készítnk a felesleges szavakról. custom_stopwords_egyeb &lt;- c(&quot;lábjegyzet&quot;, &quot;országgyulés&quot;, &quot;ülésnap&quot;) Aztán pedig a pipe használatával elkészítjük a token objektumunkat. A szótövesített tokeneket egy külön objektumban tároljuk, mert gyakran elfordul hogy torvenyek_tokens &lt;- tokens(torvenyek_corpus) %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% tokens_remove(custom_stopwords) %&gt;% tokens_remove(custom_stopwords_egyeb) %&gt;% tokens_wordstem(language = &quot;hun&quot;) Végül eltávolítjuk a dokumentum kifejezés mátrixból a túl gyakori kifejezéseket. A dfm_trim() függvénnyel a nagyon ritka és nagyon gyakori szavak megjelenését kontrollálhatjuk. A termfreq_type opció \"prop\" akkor 0 és 1.0 közötti értéket vehetnek fel a max_termfreq/docfreq és min_termfreq/docfreq paraméterek. A lenti példában azokat a tokeneket tartjuk meg, amelyek legalább egyszer elfordulnak ezer dokumentumonként (így kizárva a nagyon ritka kifejezéseket). torvenyek_dfm &lt;- dfm(torvenyek_tokens) %&gt;% dfm_trim(min_termfreq = 0.001, termfreq_type = &quot;prop&quot;) A szövegtisztító lépesek eredményét úgy ellenrizhetjük, hogy az 2. fejezetben bemutatottak szerint szógyakorisági listát készítünk a korpuszban maradt kifejezésekrl. Itt kihasználhatjuk a korpuszunkban lév meta adatokat és megnézhetjük ciklus szerinti bontásban a szófrekvencia ábrát. Az ábránál figyeljünk arra hogy a tidytext reorder_within fuggvenyet használjuk, ami egy nagyon hasznos megoldás a csoportosított sorrendbe rendezésre a ggplot ábránál. library(tidytext) top_tokens &lt;- textstat_frequency(torvenyek_dfm, n = 15, groups = docvars(torvenyek_dfm, field = &quot;electoral_cycle&quot;)) ggplot(top_tokens, aes(reorder_within(feature, frequency, group), frequency)) + geom_point(aes(shape = group), size = 2) + coord_flip() + labs( y = NULL, x = &quot;szófrekvencia&quot;, title = &quot;A 15 leggyakoribb token a korpuszban&quot; ) + facet_wrap(~group, nrow = 2, scales = &quot;free&quot;) + tidytext::scale_x_reordered() A szövegtisztító lépéseket késbb újabbakkal is kiegészíthetjük, ha észrevesszük, hogy az elemzést zavaró tisztítási lépés maradt ki. Ilyen esetben tovább tisztíthatjuk a korpuszt, majd újra lefuttathatjuk az elemzést. Például, ha szükséges, további stopszavak eltávolítását is elvégezhetjük egy újabb stopszólista hozzáadásával. Ilyenkor ugyanúgy járunk el, mint az elz stopszólista esetén, vagyis beolvassuk a munkakönyvtárban elhelyezett a csv fájlt, a beolvasott stopszólistából karakter vektort majd objektumot hozunk létre, végezetül pedig ezeket a szavakat is eltávolítjuk a kopuszból. custom_stopwords2 &lt;- readtext(&quot;data/custom_stopwords2.csv&quot;, encoding = &quot;UTF8&quot;) %&gt;% pull(text) torvenyek_tokens_final &lt;- torvenyek_tokens %&gt;% tokens_remove(custom_stopwords2) Ezután újra ellenrízzük az eredményt. torvenyek_dfm_final &lt;- dfm(torvenyek_tokens_final) %&gt;% dfm_trim(min_termfreq = 0.001, termfreq_type = &quot;prop&quot;) top_tokens_final &lt;- textstat_frequency(torvenyek_dfm_final, n = 15, groups = docvars(torvenyek_dfm, field = &quot;electoral_cycle&quot;)) ggplot(top_tokens_final, aes(reorder_within(feature, frequency, group), frequency)) + geom_point(aes(shape = group), size = 2) + coord_flip() + labs( y = NULL, x = &quot;szófrekvencia&quot;, title = &quot;A 15 leggyakoribb token a korpuszban&quot;, subtitle = &quot;Eredmény a bovített stop szó listával&quot; ) + facet_wrap(~group, nrow = 2, scales = &quot;free&quot;) + tidytext::scale_x_reordered() A szövegtisztító és korpusz elkészít mveletek után következhet az LDA illesztése. Az alábbiakban az LDA illesztés két módszerét a VEM-et és a Gibbs-et mutatjuk be. A modell minkét módszer esetén ugyanaz, a különbség a következtetés módjában van. A VEM módszer variációs következtetés, míg a Gibbs mintavételen alapuló következtetés. (Blei, Ng, and Jordan 2003; Griffiths2002?; Phan2008?) A két modell illesztése nagyon hasonló, meg kell adnunk, az elemezni kívánt dfm nevét, majd a k\" értékét, ami egyenl az általunk létrehozni kívánt topikok számával, ezt követen meg kell jelölnünk, hogy a VEM vagy a Gibbs módszert alkalmazzuk. A set.seed() a funkció az R véletlen szám generátor magjának beállítására szolgál, ami ahhoz kell, hogy az eredmény, ábra, stb. pontosan reprodukálható legyen. A set.seed() bármilyen tetszleges egész szám lehet. Kihasználhatjuk hogy minden dokumentumhoz tartozik egy kormányzati ciklus azonosító, mivel ésszer lehet a feltételezés, hogy különböz parlamentek és kormányok más-más jogalkotási fokusszal rendelkeznek. A dokumentum változók alapján a dfm_subset()-el tudjuk feldarabolni a már elkészült és tisztított mátrixunkat. dfm_98_02 &lt;- dfm_subset(torvenyek_dfm_final, electoral_cycle == &quot;1998-2002&quot;) dfm_02_06 &lt;- dfm_subset(torvenyek_dfm_final, electoral_cycle == &quot;2002-2006&quot;) 9.2.1 A VEM\" módszer alkalmazása a magyar törvények korpuszán Saját korpuszunkon elször a VEM a módszert alkalmazzuk, ahol k = 10 azaz a modell 10 témacsoportot alakít ki. Mint arról korábban már volt szó a k értékét szabadon változtathatjuk, aszerint hogy hány topik kialakítását szeretnénk. Bár a k értékének meghatározása kutatói döntésen alapul, és a modell futtatása során bevett gyakorlat a különböz k\" értékekkel való kísérletezés, miután elkészült az elemzés a perplexity() funkció segítségével  ahol a theta az adott topikhoz való tartozás valószínsége  lehetségünk van az elkészült modell kiértékelésére. A függvény a topikok által reprezentált elméleti szóeloszlásokat hasonlítja össze a szavak tényleges eloszlásával a dokumentumokban. A függvény értéke nem önmagában értelmezend, hanem két modell összehasonlításában, ahol a legalacsonyabb perplexity (zavarodottság) értékkel rendelkez modellt tekintik a legjobbnak.[^klaszeterzes-2] Az illusztráció kedvéért lefuttatunk5 LDA modellt az 1998-2002 kormánzyati ciklushoz tartozó dfm-en. Az iterációhoz a purrr csomag map függvényét használjuk (ez a lapply tidyverse ekvivalense). Fontos emlékezni arra, hogy minél nagyobb a korpuszunk annál több számítási kapacitásra van szükség (és annál tovább tart a számítás). [^klaszeterzes-2] http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/ library(purrr) k_topics &lt;- c(5, 10, 15, 20, 25) lda_98_02 &lt;- k_topics %&gt;% map(LDA, x = dfm_98_02, control = list(seed = 1234)) tibble( k = k_topics, perplexity = map_dbl(lda_98_02, perplexity) ) %&gt;% ggplot(aes(k, perplexity)) + geom_point() + geom_line() + labs( title = &quot;Mekkora k?&quot;, subtitle = &quot;Perplexity változása a k függvényében&quot;, x = &quot;k&quot;, y = &quot;Perplexity&quot; ) A perplexity pontszám alapján a 25 topikos modell szerepel a legjobban, de fontos emlékezni arra hogy a megfelel k kiválasztása a kutató kvalitatív döntésén múlik. Ehhez természetesen kvantitatív szempontokat is figyelembe vehetünk, mint például a perplexity indikátor.13 A reprodukálhatóság és futási sebesség érdekében a fejezet további részeiben a k paraméternek 10-es értéket adunk. Ezzel lefututtatunk egy-egy modellt a két ciklusra. vem_98_02 &lt;- LDA(dfm_98_02, k = 10, method = &quot;VEM&quot;, control = list(seed = 1234)) vem_02_06 &lt;- LDA(dfm_02_06, k = 10, method = &quot;VEM&quot;, control = list(seed = 1234)) Ezt követen a modell által létrehozott topic-okat tidy formátumba tesszük és egyesítjük egy data frameben.14 library(tidytext) topics_98_02 &lt;- tidy(vem_98_02, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;1998-2002&quot;) topics_02_06 &lt;- tidy(vem_02_06, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;2002-2006&quot;) lda_vem &lt;- bind_rows(topics_98_02, topics_02_06) Majd listázzuk az egyes topikokhoz tartozó leggyakoribb kifejezéseket. top_terms &lt;- lda_vem %&gt;% group_by(electoral_cycle, topic) %&gt;% top_n(5, beta) %&gt;% top_n(5, term) %&gt;% ungroup() %&gt;% arrange(topic, -beta) Majd a ggplot2 csomag segítségével ábrán is megjeleníthetjük az egyes topikok 10 legfontosabb kifejezését. top_terms %&gt;% filter(electoral_cycle == &quot;1998-2002&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 2) + coord_flip() + labs( title = &quot;1998-2002 ciklus topikok és kifejezések&quot;, x = NULL, y = expression(beta) ) + tidytext::scale_x_reordered() top_terms %&gt;% filter(electoral_cycle == &quot;2002-2006&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 2) + coord_flip() + labs( title = &quot;2002-2006 ciklus topikok és kifejezések&quot;, x = NULL, y = expression(beta) ) + tidytext::scale_x_reordered() 9.2.2 Az LDA Gibbs\" módszer alkalmazása a magyar törvények korpuszán A következkben ugyanazon a korpuszon az LDA Gibbs módszert alkalmazzuk. A szövegelkészít és tisztító lépések ennél a módszernél is ugyanazok mint a fentebb bemutatott VEM módszer esetében, így itt most csak a modell illesztését mutatjuk be. gibbs_98_02 &lt;- LDA(dfm_98_02, k = 10, method = &quot;Gibbs&quot;, control = list(seed = 1234)) gibbs_02_06 &lt;- LDA(dfm_02_06, k = 10, method = &quot;Gibbs&quot;, control = list(seed = 1234)) Itt is elvégezzük a topikok tidy formátumra alakítását. topics_g98_02 &lt;- tidy(gibbs_98_02, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;1998-2002&quot;) topics_g02_06 &lt;- tidy(gibbs_02_06, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;2002-2006&quot;) lda_gibbs &lt;- bind_rows(topics_g98_02, topics_g02_06) Majd listázzuk az egyes topikokhoz tartozó leggyakoribb kifejezéseket. top_terms_gibbs &lt;- lda_gibbs %&gt;% group_by(electoral_cycle, topic) %&gt;% top_n(5, beta) %&gt;% top_n(5, term) %&gt;% ungroup() %&gt;% arrange(topic, -beta) Majd a ggplot2 csomag segítségével ábrán is megjeleníthetjük. top_terms_gibbs %&gt;% filter(electoral_cycle == &quot;1998-2002&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 2) + coord_flip() + labs( title = &quot;1998-2002 ciklus topikok és kifejezések&quot;, x = NULL, y = expression(beta) ) + tidytext::scale_x_reordered() top_terms_gibbs %&gt;% filter(electoral_cycle == &quot;2002-2006&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 2) + coord_flip() + labs( title = &quot;2002-2006 ciklus topikok és kifejezések&quot;, x = NULL, y = expression(beta) ) + scale_x_reordered() 9.3 Struktúrális topik modellek A kvantitatív szövegelemzés elterjedésével együtt megjelentek a módszertani innovációk is és a probabilisztikus topic modellek esetében ez a politikatudomány területérl érkezett. Roberts et al. (2014) egy kíváló cikkben mutatta be a struktúrális topic modelleket (structural topic models, stm) ahol a f újítás az az hogy a dokumentumok metaadatai kovariánsként tudják befolyásolni hogy egy-egy kifejezés mekkora valószínséggel lesz egy-egy téma része. A kovariánsok egyrészrl megmagyarázhatják hogy egy-egy dokumentum mennyire függ össze egy-egy témával (topical prevalence), illetve hogy egy-egy szó mennyire függ össze egy-egy témán belül (topical content). Az stm modell becslése során mindkét típusú kovariánst használhatjuk, illetve hogyha nem adunk meg dokumentum meta adatot akkor az stm csomag stm függvénye a Korrelált Topic Modell-t fogja becsülni. Az stm modelleket az R-ben az stm csomaggal tudjuk kivitelezni. A csomag fejleszti között van a módszer kidolgozója is, ami nem ritka az R csomagok esetében. library(stm) A lenti lépésekben a csomag dokumentációjában szerepl ajánlásokat követjük, habár a könyv írásakor a stm már képes volt a quanteda-ban létrehozott dfm-ek kezelésére is. A kiinduló adatbázisunk a törvény_final amit a fejezet elején hoztunk létre a dokuemntumokból és a metaadatokból. A javasolt munkafolyamat a textProcessor()-használatával indul, ami szintén tartalmazza az alap szöveg elkészítési lépéseket. Az egyszerség és futási sebesség érdekében itt most ezek többségétl eltekintünk, mivel a fejezet korábbi részeiben részletesen tárgyaltuk ket. Az elkészítés utolsó szakaszában az out objektumban tároljuk el a dokumentumokat, egyedi szavakat, illetve a meta adatokat (kovariánsokat). data_stm &lt;- torveny_final processed_stm &lt;- textProcessor( torveny_final$text, metadata = torveny_final, lowercase = FALSE, removestopwords = FALSE, removenumbers = FALSE, removepunctuation = FALSE, ucp = FALSE, stem = TRUE, language = &quot;hungarian&quot;, verbose = FALSE ) out &lt;- prepDocuments(processed_stm$documents, processed_stm$vocab, processed_stm$meta) #&gt; Removing 96264 of 180243 terms (96264 of 1252793 tokens) due to frequency #&gt; Your corpus now has 1032 documents, 83979 terms and 1156529 tokens. A struktúrális topic modellünket az stm függvénnyel becsüljük és a kovariánsokat a prevalence opciónál tudjuk formulaként megadni. A lenti példában a Comparative Agendas Projekt kategóriáit (pl.: gazdaság, egészségügy, stb.) és a kormányciklusokat használjuk. A futási id kicsit hosszabb mint az LDA modellek esetében. stm_fit &lt;- stm( out$documents, out$vocab, K = 10, prevalence = ~ majortopic + electoral_cycle, data = out$meta, init.type = &quot;Spectral&quot;, seed = 1234, verbose = FALSE ) Amennyiben a kutatási kérdés megkívánja, akkor megvizsálhatjuk hogy a kategórikus változóinknak milyen hatása volt egyes topikok esetében. Ehhez az estimateEffect() függvénnyel lefuttatunk egy lineáris regressziót és a summary() használatával láthatjuk az egyes kovariánsok koefficienseit. Itt az els topikkal illusztráljuk az eredményt, ami azt mutatja hogy (a kategórikus változóink els kategoriájához mérten) statisztikailag szignifikáns mint a téma mind pedig a kormányzati ciklusok abban hogy egyes dokumentumok milyen témákból épülnek fel. out$meta$electoral_cycle &lt;- as.factor(out$meta$electoral_cycle) out$meta$majortopic &lt;- as.factor(out$meta$majortopic) cov_estimate &lt;- estimateEffect(1:10 ~ majortopic + electoral_cycle, stm_fit, meta = out$meta, uncertainty = &quot;Global&quot;) summary(cov_estimate, topics = 1) #&gt; #&gt; Call: #&gt; estimateEffect(formula = 1:10 ~ majortopic + electoral_cycle, #&gt; stmobj = stm_fit, metadata = out$meta, uncertainty = &quot;Global&quot;) #&gt; #&gt; #&gt; Topic 1: #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.30340 0.03100 9.787 &lt; 2e-16 *** #&gt; majortopic2 -0.20400 0.06769 -3.014 0.002646 ** #&gt; majortopic3 -0.20439 0.05955 -3.432 0.000623 *** #&gt; majortopic4 -0.22113 0.05892 -3.753 0.000185 *** #&gt; majortopic5 0.10304 0.04720 2.183 0.029280 * #&gt; majortopic6 -0.22311 0.05868 -3.802 0.000152 *** #&gt; majortopic7 -0.15568 0.06809 -2.286 0.022436 * #&gt; majortopic8 -0.21569 0.07288 -2.959 0.003155 ** #&gt; majortopic9 0.52502 0.08761 5.992 2.87e-09 *** #&gt; majortopic10 -0.10869 0.05491 -1.979 0.048045 * #&gt; majortopic12 -0.17410 0.04066 -4.281 2.03e-05 *** #&gt; majortopic13 -0.13579 0.05597 -2.426 0.015432 * #&gt; majortopic14 -0.21725 0.07553 -2.876 0.004107 ** #&gt; majortopic15 -0.14752 0.04267 -3.457 0.000568 *** #&gt; majortopic16 -0.09594 0.05308 -1.807 0.071004 . #&gt; majortopic17 -0.22433 0.05805 -3.864 0.000118 *** #&gt; majortopic18 0.21036 0.05727 3.673 0.000252 *** #&gt; majortopic19 0.07385 0.05122 1.442 0.149659 #&gt; majortopic20 -0.21048 0.03923 -5.366 1.00e-07 *** #&gt; majortopic21 -0.22473 0.06975 -3.222 0.001314 ** #&gt; majortopic23 -0.16701 0.09392 -1.778 0.075662 . #&gt; electoral_cycle2002-2006 -0.10358 0.02097 -4.939 9.17e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Az LDA modelleknél már bemutatott munkafolyamat az stm modellünk esetében is alkalmazható, hogy vizuálisan is megjelenítsük az eredményeinket. A tidy() data frammé alakítja az stm objektumot, amit aztán a már ismers dplyr csomagban lév fügvényekkel tudunk átalakítani és végül vizualizálni a ggplot2 csomaggal. A lenti ábrán az egyes témákhoz tartozó 5 legvalószbb szót mutatjuk be. tidy_stm &lt;- tidy(stm_fit) tidy_stm %&gt;% group_by(topic) %&gt;% top_n(5, beta) %&gt;% ungroup() %&gt;% mutate( topic = paste0(&quot;Topic &quot;, topic), term = reorder_within(term, beta, topic) ) %&gt;% ggplot(aes(term, beta)) + geom_col() + facet_wrap(~topic, scales = &quot;free_y&quot;, ncol = 3) + coord_flip() + scale_x_reordered() + labs( x = NULL, y = expression(beta), title = &quot;Topikonkénti legmagasabb valószínuségu szavak&quot; ) Egy-egy topichoz tartozó meghatározó szavak annak függvényében változhatnak hogy milyen algoritmust használunk. A labelTopics() a már becsült stm modellünket alapul véve kínál 4 féle alternatív opciót. Az egyes algoritmusok részletes magyarázatáért érdemes elolvasni a csomag részletes leírását.15 labelTopics(stm_fit, c(1:2)) #&gt; Topic 1 Top Words: #&gt; Highest Prob: szerzodo, vagi, egyezméni, fél, államban, nem, másik #&gt; FREX: megadóztatható, haszonhúzója, beruházóinak, segélycsapatok, adóztatást, jövedelemadók, kijelölések #&gt; Lift: árucikkeket, átalányösszegben, átléphetik, átszállítást, beruházóikat, célországban, cikktanulók #&gt; Score: szerzodo, államban, illetoségu, egyezméni, megadóztatható, adóztatható, cikka #&gt; Topic 2 Top Words: #&gt; Highest Prob: muködési, célú, támogatások, költségvetésegyéb, felhalmozási, terhelo, beruházási #&gt; FREX: kiadásokfelújításegyéb, kiadásokintézményi, kiadásokközponti, költségvetésfelhalmozási, kiadásokkormányzati, felújításegyéb, rek #&gt; Lift: a+b+c, a+b+c+d, adago, adódóa, adósságállományából, adósságrendezésr, adótartozásának #&gt; Score: költségvetésegyéb, költségvetésszemélyi, kiadásokfelhalmozási, járulékokdolog, költségvetésintézményi, kiadásokegyéb, juttatásokmunkaadókat A korpuszunkon belüli témák megoszlását a plot.STM()-el tudjuk ábrázolni. Jól látszik hogy a Topic 2-be tartozó szavak vannak jelen a legnagyobb arányban a dokumentumaink között. plot.STM(stm_fit, &quot;summary&quot;) Végezetül, a témák közötti korrelációt a topicCorr függvénnyel becsülhetjük és az igraph csomagot betöltve a plot() paranccsal tudjuk vizualizálni. Az eredmény egy hálózat lesz amit gráfként ábrázolunk. Az élei a gráfoknak a témák közötti összefüggést jelölik. library(igraph) plot(topicCorr(stm_fit)) a kód részben az alábbiakon alapul: tidytextmining.com/topicmodeling.html Az általunk is használt topicmodels csomag interfészt biztosít az LDA modellek és a korrelált témamodellek (CTM) C kódjához, valamint az LDA modellek illesztéséhez szükséges C ++ kódhoz. A teljes törvényeket és a metadatokat tartalmazó adatbázisokat a https://cap.tk.hu/ honlapról lehet letölteni. A ldatuning R csomagban további indikátor implementációja található, ami a perplexityhez hasonlóan minimalizásra alapoz (Arun et al. (2010), Cao et al. (2009)), illetve maximalizálásra (Deveaud, SanJuan, and Bellot (2014), Griffiths and Steyvers (2004)) a tidy formátumról bvebben: https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html Az `stm` csomaghoz tartozó leírás: https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf "],["szóbeágyazások.html", "10 Szóbeágyazások 10.1 Word2Vec, GloVe és fastText", " 10 Szóbeágyazások Az eddigi fejezetekben elssorban a szózsák (bag of words) alapú módszerek voltak eltérben. Ez a reprezentálása a szövegnek szigorúan véve nem felel meg a valóságnak a kotextuális tartalom elvesztése miatt, de ezt az esetek többségében figyelmen kívül hagyhatjuk. A szóbeágyazáson (word embedding) alapuló modellek viszont kimondottan a kotextuális információt ragadják meg. A szóbeágyazás a topikmodellekhez hasonlóan szintén a felügyelet nélküli tanulás módszerére épül, azonban itt a dokumentum domináns kifejezéseinek és témáinak feltárása helyett a szavak közötti szemantikai kapcsolat megértése a cél. Vagyis a modellnek képesnek kell lennie az egyes szavak esetén szinonimáik, és ellentétpárjaik megtalálására. A hagyományos topikmodellezés esetén a modell a szavak dokumentumokon belüli együttes megjelenési statisztikái alapján becsül dokumentum-topik, illetve topik-szó eloszlásokat, azzal a céllal, hogy koherens téma-csoportokat képezzen a modell, ezzel szemben a szóbeágyazás legújabb iskolája már neurális halókon alapul. A neurális háló a tanítási folyamata során az egyes szavak vektorreprezentációját állítja el. A vektorok jellemzen 100-300 dimenzióból állnak, a távolságuk alapján pedig megállapítható, hogy az egyes kifejezések milyen szemantikai kapcsolatban állnak egymással. A szóbeágyazás célja tehát a szemantikai relációk feltárása. A szavak vektorizálásának köszönheten bármely (a korpuszunkban szerepl) tetszleges számú szóról eldönthetjük, hogy azok milyen szemantikai kapcsolatban állnak egymással  szinonimaként, vagy ellentétes fogalompárként szerepelnek. A szóvektorokon dimenziócsökkent eljárást alkalmazva, s a multidimenzionális (100-300 dimenziós) teret 2 dimenziósra szkítve könnyen vizualizálhatjuk is a korpuszunk kifejezései között fennálló szemantikai távolságot, és ahogy a lenti ábrákon is, láthatjuk, hogy az egyes kifejezések milyen relációban állnak egymással  a szemantikailag hasanló tartalmú kifejezések egymáshoz közel, míg a távolabbi jelentéstartalmú kifejezések egymástól távolabb foglalnak helyet. A klasszkus példa, amivel jól lehet szemléltetni a szóvektorok közötti összefüggést: king - man + woman = queen 10.1 Word2Vec, GloVe és fastText A szóbeágyazásra társadalomtudományokban a két legnépszerbb algoritmus  Word2Vec és a GloVe  a kontextuális szövegeloszláson (distributional similarity based representations) alapszik, vagyis abból a feltevésbl indul ki, hogy a hasonló kifejezések hasonló kontextusban fordulnak el, valamint mindkett sekély neurális hálón (2 rejtett réteg) alapuló modell.16 A Word2Vec-nek két verziója van: Continuous Bag-of-words (CBOW) és SkipGram (SG)  elbbi a kontextuális szavakból jelzi elre (predicting) a kontextushoz legszorosabban kapcsolódó kifejezést, míg utóbbi adott kifejezésbl jelzi elre a kontextust Mikolov et al. (2013). A GloVe (Global Vectors for Word Representation) a Word2Vec-hez hasonlóan neurális hálón alapuló, szóvektorok elállítását célzó modell, a Word2Vec-kel szemben azonban nem a meghatározott kontextus-ablakban (context window) megjelen kifejezések közti kapcsolatokat tárja fel, hanem a szöveg globális jellemzit igyekszik megragadni az egész szöveget jellemz együttes elfordulási gyakoriságok (co-occurrance) meghatározásával Pennington, Socher, and Manning (2014). Míg a Word2Vec modell prediktív jelleg, addig a GloVe egy statisztikai alapú (count-based) modell, melyek gyakorlati hasznosításukat tekintve nagyon hasonlóak. A szóvektor modellek között érdemes megemlíteni a fastText-et is, mely 157 nyelvre kínál (köztük magyarra is) a szóbeágyazás módszeren alapuló, elre tanított szóvektorokat, melyet tovább lehet tanítani speciális szövegkorpuszokra, ezzel jelentsen lerövidítve a modell tanításához szükséges id-, és kapacitásszükségletet Mikolov et al. (2018). Habár a GloVe és Word2Vec skip-gram módszerek hasonlóságát a szakirodalom adottnak veszi, a tényleges kép ennél árnyaltabb. A GloVe esetében a ritkán elforduló szavak kisebb súlyt kapnak a szóvektorok számításánal, míg a Word2Vec alulsúlyozz a nagy frekvenciájú szavakat. Ennek a következménye, hogy a Word2Vec esetében gyakori hogy a szemantikailag legközelebbi szó az egy elütés, nem pedig valid találat. Ennek ellenére a két módszer (amennyiben a Word2Vec algoritmusnál a kisfrekvenciájú tokeneket kiszrjük) az emberi validálás során nagyon hasonló eredményeket hozott Spirling and Rodriguez (n.d.b). A fejezetben a gyakorlati példa során a GloVe algoritmust használjuk majd, mivel véleményünk szerint az implementációt tartalmazó R csomagnak jobb a dokumentációja mint a többi alternatívának. 10.1.1 GloVe használata magyar média korpuszon Az elemzéshez a text2vec csomagot fogjuk használni, ami a GloVe implementációt tartalmazza. A lenti kód a csomag dokumentáción alapul és a Társadalomtudományi Kutatóközpont által a Hungarian Comparative Agendas Project (CAP) adatbázisában tárolt Magyar Nemzet korpuszt használja.17 library(text2vec) library(quanteda) library(readtext) library(readr) library(dplyr) library(tibble) library(stringr) A lenti kód blokk azt mutatja be, hogy hogyan kell a betöltött korpuszt tokenizálni és mátrix formátumba alakítani. A korpusz az a Magyar Nemzet 2004 és 2014 közötti címlapos cikkeit tartalmazza. Az eddigi elkészít lépéseket most is megtesszük: kitöröljük a központozást, számokat, magyar töltelékszavakat, illetve kisbetsítünk és eltávolítjuk a felesleges szóközöket és tördeléseket. mn &lt;- read_csv(&quot;data/mn_large.csv&quot;) mn_clean &lt;- mn %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) Fontos különbség hogy az eddigi munkafolyamatokkal ellentétben a GloVe algoritmus nem egy dokumentum-kifejezés mátrixon dolgozik, hanem egy kifejezések együttes elfordulását tartalmazó mátrixot (feature co-occurence matrix) kell készíteni inputként. Ezt a quanteda fcm() függvényével tudjuk elállítani, ami a tokenekbl készíti el a mátrixot. A tokenek sorrendiségét úgy tudjuk megrízni, hogy egy dfm objektumból csak a kifejezéseket tartjuk meg a featnames() függvény segítségével, majd a teljes token halmazból a tokens_select() függvénnyel kiválasztjuk ket. mn_corpus &lt;- corpus(mn_clean) mn_tokens &lt;- tokens(mn_corpus) %&gt;% tokens_remove(stopwords(language = &quot;hungarian&quot;)) features &lt;- dfm(mn_tokens) %&gt;% dfm_trim(min_termfreq = 5) %&gt;% featnames() mn_tokens &lt;- tokens_select(mn_tokens, features, padding = TRUE) Az fcm megalkotása során a célkifejezéstl való távolság függvényében súlyozzuk a tokeneket. mn_fcm &lt;- fcm(mn_tokens, context = &quot;window&quot;, count = &quot;weighted&quot;, weights = 1 / (1:5), tri = TRUE) A tényleges szóbeágyazás a text2vec csomaggal történik. A GlobalVector egy új környezetet (environment) hoz létre. Itt adhatjuk meg az alapvet paramétereket. A rank a vektor dimenziót adja meg (az irodalomban a 300-500 dimenzió a megszokott). A többi paraméterrel is lehet kísérletezni, hogy mennyire változtatja meg a kapott szóbeágyazásokat. A fit_transform pedig a tényleges becslést végzi. Itt az iterációk számát (a gépi tanulásos irodalomban epoch-nak is hívják a tanulási köröket) és a korai leállás (early stopping) kritériumát a convergence_tol megadásával. Minél több dimenziót szeretnénk és minél több iterációt, annál tovább fog tartani a szóbeágyazás futtatása. Az egyszerség és gyorsaság miatt a lenti kód 10 körös tanulást ad meg, ami a relatíve kicsi Magyar Nemzet korpuszon ~3 perc alatt fut le.18 Természetesen minél nagyobb korpuszon, minél több iterációt futtatunk, annál pontosabb eredményt fogunk kapni. A text2vec csomag képes a számítások párhuzamosítására, így alapbeállításként a rendelkezésre álló összes CPU magot teljesen kihasználja a számításhoz. Ennek ellenére egy százezres, milliós korpusz esetén több óra is lehet a tanítás. glove &lt;- GlobalVectors$new(rank = 300, x_max = 10, learning_rate = 0.1) mn_main &lt;- glove$fit_transform(mn_fcm, n_iter = 10, convergence_tol = 0.01) #&gt; INFO [01:22:47.049] epoch 1, loss 0.2292 #&gt; INFO [01:23:23.897] epoch 2, loss 0.0964 #&gt; INFO [01:24:00.117] epoch 3, loss 0.0704 #&gt; INFO [01:24:36.654] epoch 4, loss 0.0492 #&gt; INFO [01:25:13.590] epoch 5, loss 0.0412 #&gt; INFO [01:25:50.046] epoch 6, loss 0.0361 #&gt; INFO [01:26:27.177] epoch 7, loss 0.0325 #&gt; INFO [01:27:04.233] epoch 8, loss 0.0297 #&gt; INFO [01:27:40.279] epoch 9, loss 0.0274 #&gt; INFO [01:28:17.147] epoch 10, loss 0.0255 A végleges szóvektorokat a becslés során elkészült két mátrix összegeként kapjuk. mn_context &lt;- glove$components mn_word_vectors &lt;- mn_main + t(mn_context) Az egyes szavakhoz legközelebb álló szavakat a koszinusz hasonlóság alapján kapjuk, a sim2() függvénnyel. A lenti példában l2 normalizálást alkalmazunk, majd a kapott hasonlósági vektort csökken sorrendbe rendezzük. Példaként a polgármester szónak a környezetét nézzük meg. Mivel a korpuszunk egy politikai napilap, ezért nem meglep, hogy a legközelebbi szavak a politikához kapcsolódnak. teszt &lt;- mn_word_vectors[&quot;polgármester&quot;, , drop = F] cos_sim_rom &lt;- sim2(x = mn_word_vectors, y = teszt, method = &quot;cosine&quot;, norm = &quot;l2&quot;) head(sort(cos_sim_rom[, 1], decreasing = TRUE), 5) #&gt; polgármester mszps szocialista fideszes györgy #&gt; 1.0000000 0.5334455 0.4882484 0.4840095 0.4761667 A lenti show_vector() függvényt definiálva a kapott eredmény egy data frame lesz, és az n változtatásával a kapcsolódó szavak számát is könnyen változtathatjuk. show_vector &lt;- function(vectors, pattern, n = 5) { term &lt;- mn_word_vectors[pattern, , drop = F] cos_sim &lt;- sim2(x = vectors, y = term, method = &quot;cosine&quot;, norm = &quot;l2&quot;) cos_sim_head &lt;- head(sort(cos_sim[, 1], decreasing = TRUE), n) output &lt;- enframe(cos_sim_head, name = &quot;term&quot;, value = &quot;dist&quot;) return(output) } Példaként a barack nem gyümölcsöket fog adni, hanem az Egyesült Államok elnökét és hozzá kapcsolódó szavakat. show_vector(mn_word_vectors, &quot;barack&quot;, 10) #&gt; # A tibble: 10 x 2 #&gt; term dist #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 barack 1. #&gt; 2 obama 0.727 #&gt; 3 elnök 0.431 #&gt; 4 amerikai 0.375 #&gt; 5 demokrata 0.303 #&gt; 6 bush 0.274 #&gt; 7 elnököt 0.267 #&gt; 8 republikánus 0.267 #&gt; 9 köztársasági 0.259 #&gt; 10 egyesült 0.247 Ugyanez mködik magyar vezetkkel is. show_vector(mn_word_vectors, &quot;orbán&quot;, 10) #&gt; # A tibble: 10 x 2 #&gt; term dist #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 orbán 1. #&gt; 2 viktor 0.935 #&gt; 3 miniszterelnök 0.734 #&gt; 4 mondta 0.680 #&gt; 5 kormányfo 0.672 #&gt; 6 hangsúlyozta 0.661 #&gt; 7 fidesz 0.638 #&gt; 8 fogalmazott 0.627 #&gt; 9 jelentette 0.621 #&gt; 10 beszélt 0.604 Egy kíváló tanulmányban Spirling and Rodriguez (n.d.a) összehasonlítják a Word2Vec és GloVe módszereket, különbözö paraméterekkel, adatbázisokkal. Amennyiben valakit komolyabban érdekelnek a szóbeágyazás gyakorlati alkalmazásának a részletei annak mindenképp ajánljuk elolvasásra. A Magyar CAP Projekt által kezelt adatbázisok itt megtalálhatóak: https://cap.tk.hu/adatbazisok A futtatásra használt PC nem különösebben ers: 4 magos Intel Core i5-4460 (3.2GHz) CPU és 16GB RAM "],["szövegskálázás-felügyelet-nélküli-és-felügyelt-megoldások.html", "11 Szövegskálázás: felügyelet nélküli és felügyelt megoldások 11.1 Wordfish 11.2 Wordscores", " 11 Szövegskálázás: felügyelet nélküli és felügyelt megoldások library(readr) library(dplyr) library(stringr) library(ggplot2) library(quanteda) library(quanteda.textmodels) A skálázási algoritmusokat egy kicsi korpuszon fogjuk bemutatni. A minta dokumentumok a 2014-2018 parlamenti ciklusban frakcióvezet politikusok egy-egy véletlenszeren kiválasztott napirend eltti felszólalása. Összes a ciklusban 11 frakcióvezetje volt a két kormánypárti és öt ellenzéki frakciónak.19 A dokumentumokon a rutin elkészítési lépéseket végezzük csak el (tördelések, számok, központozás kitörlése, kisbetsítés). Természetesen minél alaposabbak vagyunk a szövegek tisztításával, annál pontosabb végeredményt fogunk kapni. parl_beszedek &lt;- read_csv(&quot;data/ps_sample.csv&quot;) beszedek_tiszta &lt;- parl_beszedek %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) A Wordfish és Wordscores algoritmus is ugyanazt a kiinduló corpus és dfm objektumot fogja használni, amit a szokásos módon a quanteda csomag corpus() függvényével hozunk létre. A leíró statisztikai tááblázatban látszik, hogy a beszédek hosszúsága nem egységes, a leghosszabb 10267 szavas, a legrövidebb pedig 1976. Az átlagos dokumentum hossz az 5135.5714286. A korpusz szemléltet célú, az eddig megszokott módon minél több/hosszabb dokumentummal dolgozunk, annál könnyebb dolga van az algoritmusoknak. beszedek_corpus &lt;- corpus(beszedek_tiszta) summary(beszedek_corpus) #&gt; Corpus consisting of 10 documents, showing 10 documents: #&gt; #&gt; Text Types Tokens Sentences id #&gt; text1 442 819 1 20142018_024_0002_0002 #&gt; text2 354 607 1 20142018_055_0002_0002 #&gt; text3 426 736 1 20142018_064_0002_0002 #&gt; text4 314 538 1 20142018_115_0002_0002 #&gt; text5 354 589 1 20142018_158_0002_0002 #&gt; text6 333 538 1 20142018_172_0002_0002 #&gt; text7 344 559 1 20142018_206_0002_0002 #&gt; text8 352 628 1 20142018_212_0002_0002 #&gt; text9 317 492 1 20142018_236_0002_0002 #&gt; text10 343 600 1 20142018_249_0002_0002 #&gt; felszolalo part #&gt; Vona Gábor (Jobbik) Jobbik #&gt; Dr. Schiffer András (LMP) LMP #&gt; Dr. Szél Bernadett (LMP) LMP #&gt; Tóbiás József (MSZP) MSZP #&gt; Schmuck Erzsébet (LMP) LMP #&gt; Dr. Tóth Bertalan (MSZP) MSZP #&gt; Volner János (Jobbik) Jobbik #&gt; Kósa Lajos (Fidesz) Fidesz #&gt; Harrach Péter (KDNP) KDNP #&gt; Dr. Gulyás Gergely (Fidesz) Fidesz Végezetül elkészítjük a dfm mátrixot és a magyar stopszavakat kitöröljük. beszedek_dfm &lt;- beszedek_corpus %&gt;% tokens() %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% dfm() 11.1 Wordfish A wordfish felügyelet nélküli skálázást a quanteda_textmodels csomagban implementált textmodel_wordfish() függvény fogja végezni. A megadott dir = c(1, 2) paraméterrel a két dokumentum relatív \\(\\theta\\) értékét tudjuk rögzíteni, mégpedig úgy hogy \\(\\theta_{dir1} &lt; \\theta_{dir2}\\). Alapbeállításként az els és utolsó dokumentumot teszi ide be az algoritmus. A lenti példánál mi a pártpozíciók alapján a Jobbikos Vona Gábor és az LMP-s Schiffer András egy-egy beszédét használtuk. A summary() használható az illesztett modellel, és a dokumentumonkénti \\(\\theta\\) koefficienst tudjuk így megnézni. beszedek_wf &lt;- textmodel_wordfish(beszedek_dfm, dir = c(2, 1)) summary(beszedek_wf) #&gt; #&gt; Call: #&gt; textmodel_wordfish.dfm(x = beszedek_dfm, dir = c(2, 1)) #&gt; #&gt; Estimated Document Positions: #&gt; theta se #&gt; text1 1.79474 0.04219 #&gt; text2 0.08931 0.04001 #&gt; text3 1.00137 0.03908 #&gt; text4 -0.09988 0.04232 #&gt; text5 0.73596 0.04355 #&gt; text6 0.18572 0.04452 #&gt; text7 -0.72832 0.03590 #&gt; text8 -0.80587 0.03358 #&gt; text9 -0.52028 0.04005 #&gt; text10 -1.65273 0.03794 #&gt; #&gt; Estimated Feature Scores: #&gt; vona gábor jobbik tisztelt elnök úr országgyulés tegnapi #&gt; beta 3.675 2.321 1.9710 0.2391 -0.11149 0.02755 1.2286 4.372 #&gt; psi -4.980 -2.734 -0.7531 0.4566 -0.05693 0.28721 -0.6705 -5.314 #&gt; napon helyen tartottak idoközi önkormányzati választásokat két #&gt; beta 2.991 3.103 3.675 3.675 3.675 3.675 1.1894 #&gt; psi -3.009 -2.630 -4.980 -4.980 -4.980 -4.980 -0.9439 #&gt; érdekelt recsken ózdon október nyertünk örömmel közlöm ország #&gt; beta 3.675 4.372 4.774 3.405 3.675 3.675 3.675 1.7470 #&gt; psi -4.980 -5.314 -5.545 -3.230 -4.980 -4.980 -4.980 -0.3643 #&gt; közvéleményével amúgy is tudnak mindkét jobbikos polgármester #&gt; beta 3.675 3.675 0.9128 1.433 3.675 3.675 3.675 #&gt; psi -4.980 -4.980 1.8345 -1.737 -4.980 -4.980 -4.980 Amennyiben szeretnénk a szavak szintjén is megnézni a \\(\\beta\\) (a szavakhoz társított súly, ami a relatív fontosságát mutatja) és \\(\\psi\\) (a szó fix effekt, ami az eltér szófrekvencia kezeléséért felels) koefficiensekhez, akkor a beszedek_wf objektumban tárolt értékeket egy data frame-be tudjuk bemásolni. A dokumentumok hosszára és a szófrekfenviát figyelembe véve, a negatív \\(\\beta\\) érték szavakat gyakrabban használják a negatív \\(\\theta\\) koefficienssel rendelkez politikusok. szavak_wf &lt;- data.frame( word = beszedek_wf$features, beta = beszedek_wf$beta, psi = beszedek_wf$psi ) szavak_wf %&gt;% arrange(beta) %&gt;% head(n = 15) #&gt; word beta psi #&gt; 1 czeglédy -5.900663 -6.222629 #&gt; 2 csaba -5.769959 -6.151399 #&gt; 3 human -5.438681 -5.975155 #&gt; 4 operator -5.438681 -5.975155 #&gt; 5 zrt -5.216835 -5.860931 #&gt; 6 fizette -4.927204 -5.717002 #&gt; 7 gyanú -4.927204 -5.717002 #&gt; 8 szocialista -4.927204 -5.717002 #&gt; 9 elkövetett -4.509192 -5.521276 #&gt; 10 tárgya -4.509192 -5.521276 #&gt; 11 céghálózat -4.509192 -5.521276 #&gt; 12 diákok -4.509192 -5.521276 #&gt; 13 májusi -4.509192 -5.521276 #&gt; 14 júniusi -4.509192 -5.521276 #&gt; 15 büntetoeljárás -4.509192 -5.521276 Ez a pozitív értékekre is igaz. szavak_wf %&gt;% arrange(desc(beta)) %&gt;% head(n = 15) #&gt; word beta psi #&gt; 1 nemzetközi 5.057078 -5.720709 #&gt; 2 önöknek 4.977502 -4.778607 #&gt; 3 ózdon 4.773523 -5.544626 #&gt; 4 kétharmados 4.773523 -5.544626 #&gt; 5 igenis 4.773523 -5.544626 #&gt; 6 választási 4.773523 -5.544626 #&gt; 7 geopolitikai 4.773523 -5.544626 #&gt; 8 ártatlanság 4.773523 -5.544626 #&gt; 9 vélelme 4.773523 -5.544626 #&gt; 10 tegnapi 4.372320 -5.314088 #&gt; 11 recsken 4.372320 -5.314088 #&gt; 12 lássuk 4.372320 -5.314088 #&gt; 13 tolünk 4.372320 -5.314088 #&gt; 14 janiczak 4.372320 -5.314088 #&gt; 15 szavazattal 4.372320 -5.314088 Az eredményeinket mind a szavak és mind a dokumentumok szintjén tudjuk vizualizálni. Elsként a klasszikus Eiffel-torony ábrát reprodukáljuk, ami a szavak gyakorisága és skálára gyakorolt befolyásának az illusztrálására szolgál. Ehhez a már elkészült szavak_wf data framet és a ggplot2 csomagot fogjuk használni. Mivel a korpuszunk nagyon kicsi ezért csak 2410 kifejezést fogunk ábrázolni. Ennek ellenére a lényeg kirajzolódik a lenti ábrán is.20 Kihasználhatjuk, hogy a ggplot ábra definiálása közben a felhasznált bemeneti data frame-t különböz szempontok alapján lehet szrni. így ábrázolni tudjuk a gyakran használt ám semleges szavakat (magas \\(\\psi\\), alacsony \\(\\beta\\)), illetve a ritkább de meghatározóbb szavakat (magas \\(\\beta\\), alacsony \\(\\psi\\)). ggplot(szavak_wf, aes(x = beta, y = psi)) + geom_point(color = &quot;grey&quot;) + geom_text( data = filter(szavak_wf, beta &gt; 5 | beta &lt; -4.5 | psi &gt; 0), aes(beta, psi, label = word), alpha = 0.7 ) + labs( x = expression(beta), y = expression(psi) ) A dokumentumok szintjén is érdemes megvizsgálni az eredményeket. Ehhez a dokumentum szint paramétereket fogjuk egy data framebe gyjteni: a \\(\\theta\\) ideológiai pozíciót, illetve a beszél nevét. A vizualizáció kedvéért a párttagságot is hozzáadjuk. A data frame összerakása után az alsó és fels határát is kiszámoljuk a konfidencia intervallumnak és azt is ábrázoljuk. dokumentumok_wf &lt;- data.frame( speaker = beszedek_wf$x@docvars$felszolalo, part = beszedek_wf$x@docvars$part, theta = beszedek_wf$theta, theta_se = beszedek_wf$se.theta ) %&gt;% mutate( lower = theta - 1.96 * theta_se, upper = theta + 1.96 * theta_se ) ggplot(dokumentumok_wf, aes(theta, reorder(speaker, theta))) + geom_point() + geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0) + labs( y = NULL, x = expression(theta) ) A párt metaadattal összehasonlíthatjuk az egy párthoz tartozó frakcióvezetk értékeit a facet_wrap() használatával. Figzeljünk arra hogy az y tengelyen szabadon ggplot(dokumentumok_wf, aes(theta, reorder(speaker, theta))) + geom_point() + geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0) + labs( y = NULL, x = expression(theta) ) + facet_wrap(~part, ncol = 1, scales = &quot;free_y&quot;) 11.2 Wordscores A wordscores egy felügyelt gépi tanulásos skálázási algoritmus, ahol a a referencia dokumentumok szövege a tanító halmaz. A modell illesztést a wordfish-ez hasonlóan a quanteda.textmodels csomagban található textmodel_wordscores() függvény végzi. A kiinduló dfm ugyanaz mint amit a fejezet elején elkészítettünk, a beszedek_dfm. A referencia pontokat dokumentumváltozóként hozzáadjuk a dfm-hez a refrencia_pont oszlopot, ami NA értéket kap alapértelmezetten. A kiválasztott referencia dokumentumoknál pedig egyenként hozzáadjuk az értékeket. Erre több megoldás is van, az egyszerbb út, hogy az egyik és másik végletet a -1; 1 intervallummal jelöljük. Ennek a lehetséges alternatívája, hogy egy küls, már validált forrást használunk. Pártok esetén ilyen lehet a Chapel Hill szakérti kérdívének a pontszámai, a Manifesto projekt által kódolt jobb-bal (rile) dimenzó. A lenti példánál mi maradunk az egyszerbb bináris kódolásnál. A wordfish eredményt alapul véve a két referencia pont a Gulyás Gergely és Szél Bernadett beszédei lesznek.21 Ezek a 3. és 10. dokumentumok. docvars(beszedek_dfm, &quot;referencia_pont&quot;) &lt;- NA docvars(beszedek_dfm, &quot;referencia_pont&quot;)[3] &lt;- -1 docvars(beszedek_dfm, &quot;referencia_pont&quot;)[10] &lt;- 1 docvars(beszedek_dfm) #&gt; id felszolalo part referencia_pont #&gt; 1 20142018_024_0002_0002 Vona Gábor (Jobbik) Jobbik NA #&gt; 2 20142018_055_0002_0002 Dr. Schiffer András (LMP) LMP NA #&gt; 3 20142018_064_0002_0002 Dr. Szél Bernadett (LMP) LMP -1 #&gt; 4 20142018_115_0002_0002 Tóbiás József (MSZP) MSZP NA #&gt; 5 20142018_158_0002_0002 Schmuck Erzsébet (LMP) LMP NA #&gt; 6 20142018_172_0002_0002 Dr. Tóth Bertalan (MSZP) MSZP NA #&gt; 7 20142018_206_0002_0002 Volner János (Jobbik) Jobbik NA #&gt; 8 20142018_212_0002_0002 Kósa Lajos (Fidesz) Fidesz NA #&gt; 9 20142018_236_0002_0002 Harrach Péter (KDNP) KDNP NA #&gt; 10 20142018_249_0002_0002 Dr. Gulyás Gergely (Fidesz) Fidesz 1 A lenti wordscore model specifikáció követi a Laver, Benoit, and Garry (2003) - ben leírtakat. beszedek_ws &lt;- textmodel_wordscores( x = beszedek_dfm, y = docvars(beszedek_dfm, &quot;referencia_pont&quot;), scale = &quot;linear&quot;, smooth = 0 ) summary(beszedek_ws, 10) #&gt; #&gt; Call: #&gt; textmodel_wordscores.dfm(x = beszedek_dfm, y = docvars(beszedek_dfm, #&gt; &quot;referencia_pont&quot;), scale = &quot;linear&quot;, smooth = 0) #&gt; #&gt; Reference Document Statistics: #&gt; score total min max mean median #&gt; text1 NA 486 0 18 0.2017 0 #&gt; text2 NA 395 0 12 0.1639 0 #&gt; text3 -1 439 0 12 0.1822 0 #&gt; text4 NA 330 0 7 0.1369 0 #&gt; text5 NA 360 0 8 0.1494 0 #&gt; text6 NA 328 0 5 0.1361 0 #&gt; text7 NA 349 0 5 0.1448 0 #&gt; text8 NA 387 0 10 0.1606 0 #&gt; text9 NA 307 0 13 0.1274 0 #&gt; text10 1 383 0 8 0.1589 0 #&gt; #&gt; Wordscores: #&gt; (showing first 10 elements) #&gt; tisztelt elnök úr országgyulés ország is #&gt; -0.07547 0.39255 0.06813 0.06813 -1.00000 -0.19859 #&gt; sot nemhogy tette fidesz #&gt; -1.00000 -1.00000 -1.00000 1.00000 Az illesztett wordscores modellünkkel ezek után már meg tudjuk becsülni a korpuszban lév többi dokumentum pozícióját. Ehhez az R beépített predict() megoldását használjuk. A kiegészít opciókkal a konfidencia intervallum alsó és fels határát is meg tudjuk becsülni, ami jól jön hogyha szeretnénk ábrázolni az eredményt. beszedek_ws_pred &lt;- predict( beszedek_ws, newdata = beszedek_dfm, interval = &quot;confidence&quot; ) beszedek_ws_pred &lt;- as.data.frame(beszedek_ws_pred$fit) beszedek_ws_pred #&gt; fit lwr upr #&gt; text1 -0.489860579 -0.62138707 -0.35833409 #&gt; text2 -0.234609623 -0.39658117 -0.07263807 #&gt; text3 -0.909048451 -0.93507086 -0.88302605 #&gt; text4 -0.296528588 -0.47539855 -0.11765863 #&gt; text5 -0.259074418 -0.44948427 -0.06866457 #&gt; text6 0.006320468 -0.23056645 0.24320738 #&gt; text7 0.165042014 -0.06144022 0.39152425 #&gt; text8 -0.077739857 -0.27645536 0.12097565 #&gt; text9 -0.123985348 -0.31176579 0.06379509 #&gt; text10 0.909048451 0.87934394 0.93875296 A kapott modellünket a wordfishez hasonlóan tudjuk ábrázolni, miután a beszedek_ws_pred objektumból egy data framet csinálunk és a ggplot2-vel elkészítjük a vizualizációt. A dokumentumok_ws két részbl áll össze. Elször a wordscores modell objektumunkból a frakcióvezetk neveit és pártjaikat emeljük ki (kicsit körülményes a dolog mert egy komplexebb objektumban tárolja ket a quanteda, de az str() függvény tud segíteni ilyen esetekben). A dokumentumok becsült pontszámait pedig a beszedek_ws_pred objektumból készített data frame hozzácsatolásával tesszük meg. Ehhez a dplyr csomag bind_cols függvényét használjuk. Fontos, hogy itt teljesen biztosnak kell lennünk abban, hogy a sorok a két data frame esetében ugyanarra a dokumentumra vonatkoznak. dokumentumok_ws &lt;- data.frame( speaker = beszedek_ws$x@docvars$felszolalo, part = beszedek_ws$x@docvars$part ) dokumentumok_ws &lt;- bind_cols(dokumentumok_ws, beszedek_ws_pred) dokumentumok_ws #&gt; speaker part fit lwr upr #&gt; text1 Vona Gábor (Jobbik) Jobbik -0.489860579 -0.62138707 -0.35833409 #&gt; text2 Dr. Schiffer András (LMP) LMP -0.234609623 -0.39658117 -0.07263807 #&gt; text3 Dr. Szél Bernadett (LMP) LMP -0.909048451 -0.93507086 -0.88302605 #&gt; text4 Tóbiás József (MSZP) MSZP -0.296528588 -0.47539855 -0.11765863 #&gt; text5 Schmuck Erzsébet (LMP) LMP -0.259074418 -0.44948427 -0.06866457 #&gt; text6 Dr. Tóth Bertalan (MSZP) MSZP 0.006320468 -0.23056645 0.24320738 #&gt; text7 Volner János (Jobbik) Jobbik 0.165042014 -0.06144022 0.39152425 #&gt; text8 Kósa Lajos (Fidesz) Fidesz -0.077739857 -0.27645536 0.12097565 #&gt; text9 Harrach Péter (KDNP) KDNP -0.123985348 -0.31176579 0.06379509 #&gt; text10 Dr. Gulyás Gergely (Fidesz) Fidesz 0.909048451 0.87934394 0.93875296 A lenti példánál a párton belüli bontást illusztráljuk, a facet_wrap() segítségével. ggplot(dokumentumok_ws, aes(fit, reorder(speaker, fit))) + geom_point() + geom_errorbarh(aes(xmin = lwr, xmax = upr), height = 0) + labs( y = NULL, x = &quot;Wordscore&quot; ) + facet_wrap(~part, ncol = 1, scales = &quot;free_y&quot;) A mintába nem került be Rogán Antal, akinek csak egy darab napirend eltti felszólalása volt. A quanteda.textplots csomag több megoldást is kínál az ábrák elkészítésére. Mivel ezek a megoldások kifejezetten a quanteda elemzések ábrázolására készültek, ezért rövid egysoros függvényekkel tudunk gyorsan ábrákat készíteni. A hátrányuk, hogy kevésbé tudjuk személyre szabni az ábráinkat, mint a ggplot2 példák esetében. A quanteda.textplots megoldásokat ezen a linken demonstrálják a csomag készíti: https://quanteda.io/articles/pkgdown/examples/plotting.html Azért nem a Vona Gábor beszédét választottuk, mert az gyaníthatóan egy kiugró érték ami nem reprezentálja a sokaságot megfelen. "],["szövegösszehasonlítás.html", "12 Szövegösszehasonlítás", " 12 Szövegösszehasonlítás tizedik fejezet "],["természetes-nyelv-feldolgozás-nlp.html", "13 Természetes-nyelv feldolgozás (NLP)", " 13 Természetes-nyelv feldolgozás (NLP) tizenegyedik fejezet "],["osztályozás-és-felügyelt-tanulás.html", "14 Osztályozás és felügyelt tanulás", " 14 Osztályozás és felügyelt tanulás tizenkeddik fejezet "],["függelék.html", "15 Függelék 15.1 Az R és az RStudio használata 15.2 Vizualizáció", " 15 Függelék 15.1 Az R és az RStudio használata Az R egy programozási nyelv, amely alkalmas statisztikai számítások elvégzésére és ezek eredményeinek grafikus megjelenítésére. Az R ingyenes, nyílt forráskódú szoftver, mely telepíthet mind Windows, mind Linux, mind MacOS operációs rendszerek alatt, az alábbi oldalról: https://cran.r-project.org/ Az RStudio az R integrált fejleszti környezete (integrated development environment, IDE), mely egy olyan felhasználóbarát felületet biztosít, ami egyszerbb és átláthatóbb munkát tesz lehetvé. Az RStudio az alábbi oldalról tölthet le: https://rstudio.com/products/rstudio/download/ A point and click\" szoftverekkel szemben az R használata során kódot kell írni, ami bizonyos programozási jártasságot feltételez, de a késbbiekben lehetvé teszi azt adott kutatási kérdéshez maximálisan illeszked kódok összeállítását, melyek segítségével az elemzések mások számára is megbízhatóan reprodukálhatóak lesznek. Ugyancsak az R használata mellett szól, hogy komoly fejleszti és felhasználói közösséggel rendelkezik, így a használat során felmerül problémákra általában gyorsan megoldást találhatunk. 15.1.1 Az RStudio kezdfelülete Az RStudio kezdfelülete négy panelbl, eszközsorból és menüsorból áll: Figure 15.1: RStudio felhasználói felület Az (1) editor ablak szolgál a kód beírására, futtatására és mentésére. A (2) console ablakban jelenik meg a lefuttatott kód és az eredmények. A jobb fels ablak (3) environment fülén láthatóak a memóriában tárolt adatállományok, változók és felhasználói függvények. A history fül mutatja a korábban lefuttatott utasításokat. A jobb alsó ablak (4) files fülén az aktuális munkakönyvtárban lev mappákat és fájlok találjuk, míg a plot fülön az elemzéseink során elkészített ábrák jelennek meg. A packages fülön frissíthetjük a meglév r csomagokat és telepíthetünk újakat. A help fülön a különböz függvények, parancsok leírását, és használatát találjuk meg. A Tools -&gt; Global Options menüpont végezhetjük el az RStudio testreszabását. Így például beállíthatjuk az ablaktér elrendezését (Pane layout), vagy a színvilágot (Appearance), illetve azt hogy a kódok ne fussanak ki az ablakból (Code -&gt; Editing -&gt; Soft wrap R source files) 15.1.2 Projekt alapú munka Bár nem kötelez, de javasolt, hogy az RStudio-ban projekt alapon dolgozzunk, mivel így az összes  az adott projekttel kapcsolatos fájlt  egy mappában tárolhatjuk. Új projekt beállítását a File-&gt;New Project menüben tehetjük meg, ahol a saját gépünk egy könyvtárát kell kiválasztani, ahová az R scripteket, az adat- és elzményfájlokat menti. Ezenkívül a Tools-&gt;Global Options-&gt;General menüpont alatt le kell tiltani a Restore most recently opened project at startup és a Restore .RData ino workspace at startup beállítást, valamint Save workspace to .RData on exit értékre be kell állítani a Never értéket. Figure 15.2: RStudio projekt beállítások A szükséges beállítások után a File -&gt; New Project menüben hozhatjuk létre a projektet. Itt arra is lehetségünk van, hogy kiválasszuk, hogy a projektünket egy teljesen új könyvtárba, vagy egy meglévbe kívánjuk menteni, esetleg egy meglév projekt új verzióját szeretnénk létrehozni. Ha sikeresen létrehoztuk a projektet, az RStudio jobb fels sarkában látnunk kell annak nevét. 15.1.3 Scriptek szerkesztése, függvények használata Új script a File -&gt; New -&gt; File -&gt; R Script menüpontban hozható létre, mentésére a File-&gt;Save menüpontban egy korábbi script megnyitására File -&gt; Open menüpontban van lehetségünk. Script bármilyen szövegszerkesztvel írható és beilleszthet az editor ablakba. A scripteket érdemes magyarázatokkal (kommentekkel) ellátni, hogy a késbbiekben pontosan követhet legyen, hogy melyik parancs segítségével pontosan milyen lépéseket hajtottunk végre. A magyarázatokat vagy más néven kommenteket kettskereszt (#) karakterrel vezetjük be. A scriptbeli utasítások az azokat tartalmazó sorokra állva vagy több sort kijelölve a Run feliratra kattintva vagy a Ctrl+Enter billentyparanccsal futtathatók le. A lefuttatott parancsok és azok eredményei ezután a bal alsó sarokban lév console ablakban jelennek meg és ugyanitt kapunk hibaüzenetet is, ha valamilyen hibát vétettünk a scriptben. A munkafolyamat során létrehozott állományok (ábrák, fájlok) ebbe az ún. munkakönyvtárba (working directory) mentdnek. Az aktuális munkakönyvtár neve, elérési útja a getwd() utasítással jeleníthet meg. A könyvtárban található állományok listázására a list.files() utasítással van lehetségünk. Ha a korábbiaktól eltér munkakönyvtárat akarunk megadni, azt a setwd() függvénnyel tehetjük meg, ahol a ()-ben az adott mappa elérési útját kell megadnunk. Az elérési útban a meghajtó azonosítóját, majd a mappák, almappák nevét vagy egy normál irányú perjel (/), vagy két fordított perjel (\\\\) választja el, mivel az elérési út karakterlánc, ezért azt idézjelek vagy aposztrófok közé kell tennünk. Az aktuális munkakönyvtárba beléphetünk a jobb alsó ablak file lapján a More -&gt; Go To Working Directory segítségével. Ugyanitt a Set Working Directory-val munkakönyvtárnak állíthatjuk be az a mappát, amelyben épp benne vagyunk. Figure 15.3: Working directory beállítások A munkafolyamat befejezésére a q() vagy quit() függvényel van lehetségünk. A munkafolyamat során különböz objektumokat hozunk létre, melyek az RStudio jobb fels ablakának environment fülén jelennek meg, a mentett objektumokat a fent látható sepr ikonra kattintva törölhetjük a memóriából. Az environment ablakra érdemes úgy gondolni hogy ott jelennek meg a memóriában tárolt értékek. Az R-ben objektumokkal dolgozunk, amik a teljesség igénye nélkül lehetnek egyszer szám vektortok, vagy akár komplex listák, illetve függvények, ábrák. Az RStudio jobb alsó ablakának plots fülén láthatjuk azon parancsok eredményét, melyek kimenete valamilyen ábra. A packages fülnél a már telepített és a letölthet kiegészít csomagokat jeleníthetjük meg. A help fülön a korábban említettek szerint a súgó érhet el. Az RStudio-ban használható billentyparancsok teljes listáját Alt+Shift+K billentykombinációval tekinthetjük meg. Néhány gyakrabban használt, hasznos billentyparancs: Ctrl+Enter: futtassa a kódot az aktuális sorban Ctrl+Alt+B: futtassa a kódot az elejétl az aktuális sorig Ctrl+Alt+E: futtassa a kódot az aktuális sortól a forrásfájl végéig Ctrl+D: törölje az aktuális sort Az R-ben beépített függvények (function) állnak rendelkezésünkre a számítások végrehajtására, emellett több csomag (package) is letölthet, amelyek különböz függvényeket tartalmaznak. A függvények a következképpen épülnek fel: függvénynév(paraméter). Például tartalom képernyre való kiíratását a print() függvénnyel tehetjük, amelynek gömböly zárójelekkel határolt részébe írhatjuk a megjelenítend szöveget. A citation() függvénnyel lekérdezhetjük az egyes beépített csomagokra való hivatkozást is: a citation(quanteda) függvény a quanteda csomag hivatkozását adja meg. Az R súgórendszere a help.start() utasítással indítható el. Egy adott függvényre vonatkozó súgórészlet a függvények neve elé kérdjel írásával, vagy a help() argumentumába a kérdéses függvény nevének beírásával jeleníthet meg (pl.: help(sum)). 15.1.4 R csomagok Az R-ben telepíthetk kiegészít csomagok (packages), amelyek alapértelmezetten el nem érhet algoritmusokat, függvényeket tartalmaznak. A csomagok saját dokumentációval rendelkeznek, amelyeket fel kell tüntetni a használatukkal készült publikációink hivatkozáslistájában. A csomagok telepítésre több lehetségünk is van: használhatjuk a menüsor Tools -&gt; Install Packages menüpontját, vagy a jobb alsó ablak Packages fül Install menüpontját, illetve az editor ablakban az install.packages() parancsot futtatva, ahol a ()-be a telepíteni kívánt csomag nevét kell beírnunk (pl.: install.packages(dplyr)). Figure 15.4: Packages fül 15.1.5 Objektumok tárolása, értékadás Az objektumok lehetnek például vektorok, mátrixok (matrix), tömbök (array), adat táblák (data frame). Értékadás nélkül az R csak megjeleníti a mveletek eredményét, de nem tárolja el azokat. Az eredmények eltárolásához azokat egy objektumba kell elmentenünk. Ehhez meg kell adnunk az objektum nevét majd az &lt;- után adjuk meg annak értékét: a &lt;- 12 + 3.Futtatás után az environments fülön megjelenik az a objektum, melynek értéke 15. Az objektumok elnevezésénél figyelnünk kell arra, hogy az R különbséget tesz a kis és nagybetk között, valamint, hogy az ugyanolyan nev objektumokat kérdés nélkül felülírja és ezt a felülírást nem lehet visszavonni. 15.1.6 Vektorok Az R-ben kétféle típusú vektort különböztetünk meg: egyedüli vektor (atomic vector) lista (list) Az egyedüli vektornak hat típusa van, logikai (logical), egész szám (integer), természetes szám (double), karakter (character), komplex szám (complex) és nyers adat (raw). A leggyakrabban valamilyen numerikus, logikai vagy karakter vektorral használjuk. Az egyedüli vektorok onnan kapták a nevüket hogy csak egy féle adattípust tudnak tárolni. A listák ezzel szemben gyakorlatilag bármit tudnak tárolni, akár több listát is egybeágyazhatunk. A vektorok és listák azok az építelemek amikbl felépülnek az R objektumaink. Több érték vagy azonos típusú objektum összefzését a c() függvénnyel végezhetjük el. A lenti példában három különböz objektumot kreálunk, egy numerikusat, egy karaktert és egy logikait. A karakter vektorban az elemeket idzjellel és vesszvel szeparáljuk. A logikai vektor csak TRUE, illetve FALSE értékeket tartalmazhat. numerikus &lt;- c(1, 2, 3, 4, 5) karakter &lt;- c(&quot;kutya&quot;, &quot;macska&quot;, &quot;ló&quot;) logikai &lt;- c(TRUE, TRUE, FALSE) A létrehozott vektorokkal különböz mveleteket végezhetünk el, például összeadhatjuk numerikus vektorainkat. Ebben az esetben az els vektor els eleme a második vektor els eleméhez adódik. c(1:4) + c(10, 20, 30, 40) #&gt; [1] 11 22 33 44 A karaktervektorokat összefzhetjük egymással. Itt egy új objektumot is létrehoztunk, a jobb fels ablakban, az environment fülön láthatjuk, hogy a létrejött karakter_kombinalt objektum egy négy elem (hosszúságú) karaktervektor (chr [1:4]), melynek elemei a \"kutya\",\"macska\",\"ló\",\"nyúl\". Az objektumként tárolt vektorok tartalmát a lefuttatva írathatjuk ki a console ablakba. Habár van print() függvény az R-ben, azt ilyenkor nem szükséges használni. karakter1 &lt;- c(&quot;kutya&quot;, &quot;macska&quot;, &quot;ló&quot;) karakter2 &lt;- c(&quot;nyúl&quot;) karakter_kombinalt &lt;- c(karakter1, karakter2) karakter_kombinalt #&gt; [1] &quot;kutya&quot; &quot;macska&quot; &quot;ló&quot; &quot;nyúl&quot; Ha egy vektorról szeretnénk megtudni, hogy milyen típusú azt a typeof() vagy a class() paranccsal tehetjük meg, ahol ()-ben az adott objektumként tárolt vektor nevét kell megadnunk: typeof(karakter1). A vektor hosszúságát (benne tárolt elemek száma vektorok esetén) a lenght() függvénnyel tudhatjuk meg. typeof(karakter1) #&gt; [1] &quot;character&quot; length(karakter1) #&gt; [1] 3 15.1.7 Faktorok A faktorok a kategórikus adatok tárolására szolgálnak. Faktor típusú változó a factor() függvénnyel hozható létre. A faktor szintjeit (igen, semleges, nem), a levels() függvénnyel kaphatjuk meg míg az adatok címkéit (tehát a kapott válaszok száma), a labels() paranccsal érhetjük el. survey_response &lt;- factor(c(&quot;igen&quot;, &quot;semleges&quot;, &quot;nem&quot;, &quot;semleges&quot;, &quot;nem&quot;, &quot;nem&quot;, &quot;igen&quot;), ordered = TRUE) levels(survey_response) #&gt; [1] &quot;igen&quot; &quot;nem&quot; &quot;semleges&quot; labels(survey_response) #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; 15.1.8 Data frame Az adat táblák (data frame) a statisztikai és adatelemzési folyamatok egyik leggyakrabban használt adattárolási formája. Amikor lehetséges akkor a hosszú formátumban használjuk (az R közösség a tidy jelzvel illeti), aholtéglalap alakú adatszerkezetek, ahol minden sor egy megfigyelés és minden oszlop egy változó [TIDY CITATION]. Egy data frame többféle típusú adatot tartalmazhat. A data frame-k különféle oszlopokból állhatnak, amelyek különféle típusú adatokat tartalmazhatnak, de egy oszlop csak egy típusú adatból állhat. A lent bemutatott data frame 7 megfigyelést és 4 féle változót tartalmaz (id, country, pop, continent). #&gt; id orszag nepesseg kontinens #&gt; 1 1 Thailand 68.7 Asia #&gt; 2 2 Norway 5.2 Europe #&gt; 3 3 North Korea 24.0 Asia #&gt; 4 4 Canada 47.8 North America #&gt; 5 5 Slovenia 2.0 Europe #&gt; 6 6 France 63.6 Europe #&gt; 7 7 Venezuela 31.6 South America A data frame-be rendezett adatokhoz különböz módon férhetünk hozzá, például a data frame nevének majd []-ben a kívánt sor megadásával, kiírathatjuk a console ablakba annak tetszleges sorát ás oszlopát: orszag_adatok[1, 1]. Az R több különböz módot kínál a data frame sorainak és oszlopainak eléréséhez. A [ általános használata: data_frame[sor, oszlop]. Egy másik megoldás a $ haszálata: data_frame$oszlop. orszag_adatok[1, 4] #&gt; [1] Asia #&gt; Levels: Asia Europe North America South America orszag_adatok$orszag #&gt; [1] &quot;Thailand&quot; &quot;Norway&quot; &quot;North Korea&quot; &quot;Canada&quot; &quot;Slovenia&quot; #&gt; [6] &quot;France&quot; &quot;Venezuela&quot; 15.2 Vizualizáció library(ggplot2) library(gapminder) Az elemzéseinkhez használt data frame adatainak alapján a ggplot2 csomag segítségével lehetségünk van különböz vizualizációk készítésére is. A ggplot2 használata során különböz témákat alkalmazhatunk, melyek részletes leírása megtalálható: https://ggplot2.tidyverse.org/reference/ggtheme.html Abban az esetben, ha nem választunk témát, a ggplot2 a következ ábrán is látható alaptémát használja. Ha például a szürke helyett fehér hátteret szeretnénk, alkalmazhatjuk a theme_minmal()parancsot. Szintén gyakran alkalmazott ábra alap a thema_bw(), ami az elztl az ábra keretezésében különbözik. Ha fehér alapon, de a beosztások vonalait feketén szeretnénk megjeleníteni, alkalmazhatjuk a theme_linedraw() függvényt, a theme_void() segítségével pedig egy fehér alapon, beosztásoktól mentes alapot kapunk, a theme_dark() pedig sötét hátteret eredményez. A theme_classic() segítségével az x és y tengelyt jeleníthetjük meg fehér alapon. Egy ábra készítésének alapja mindig a használni kívánt adatkészlet beolvasása, illetve az ábrázolni kiíván változtót vagy változók megadása. Ezt követi a megfelel alakzat kiválasztása, attól függen például, hogy eloszlást, változást, adatok közötti kapcsolatot, vagy elétéseket akarunk ábrázolni. A geom az a geometriai objektum, a mit a diagram az adatok megjelenítésére használ. Agglpot2 több mint 40 féle alakzat alkalmazására ad lehetséget, ezek közül néhány gyakoribbat mutatunk be az alábbiakban. Az alakzatokról részletes leírása található például az alábbi linken: https://r4ds.had.co.nz/data-visualisation.html A következkben a már korábban is használt gapminder adatok segítségével, személetetjük az adatok vizualizálásának alapjait. Elször egyszer alapbeállítások mellett egy histogram típusú vizualizációt készítünk. ggplot( data = gapminder, mapping = aes(x = gdpPercap) ) + geom_histogram() Lehetségünk van arra, hogy az alakzat színét megváltoztatássuk. A használható színek és színkódok megtalálhatóak a ggplot2 leírásában: https://ggplot2-book.org/scale-colour.html ggplot( data = gapminder, mapping = aes(x = gdpPercap) ) + geom_histogram(fill = &quot;yellow&quot;, colour = &quot;green&quot;) Meghatározhatjuk külön-külön a histogram x és y tengelyén ábrázolni kívánt adatokat és választhatjuk azok pontszer ábrázolását is. ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp ) ) + geom_point() Ahogy az elzekben, itt is megváltoztathatjuk az ábra színét. ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp ) ) + geom_point(colour = &quot;blue&quot;) Az fenti script kibvítésével az egyes kontinensek adatait különböz színnel ábrázolhatjuk, az x és y tengelyt elnevezhetjük, a histogramnak címet és alcímet adhatunk, illetve az adataink forrását is feltüntethetjük az alábbi módon: ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp, color = continent ) ) + geom_point() + labs( x = &quot;GDP per capita (log $)&quot;, y = &quot;Life expectancy&quot;, title = &quot;Connection between GDP and Life expectancy&quot;, subtitle = &quot;Points are country-years&quot;, caption = &quot;Source: Gapminder dataset&quot; ) Az ábrán található feliratok méretének, bettípusának és betszínének megválasztásra is lehetségünk van. ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp, color = continent ) ) + geom_point() + labs( x = &quot;GDP per capita (log $)&quot;, y = &quot;Life expectancy&quot;, title = &quot;Connection between GDP and Life expectancy&quot;, subtitle = &quot;Points are country-years&quot;, caption = &quot;Source: Gapminder dataset&quot; ) + theme(plot.title = element_text( size = 12, colour = &quot;red&quot; )) Készíthetünk oszlopdiagramot is, amit a ggplot2 diamonds adatkészletén személtetünk ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut)) Itt is lehetségünk van arra, hogy a diagram színét megváltoztassuk. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut), fill = &quot;darkgreen&quot;) De arra is lehetségünk van, hogy az egyes oszlopok eltér színek legyenek. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = cut)) Arra is van lehetségünk, hogy egyszerre több változót is ábrázoljunk. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity)) Arra ggplot2 segítségével arra is lehetségünk van, hogy csv-bl beolvasott adatainkat vizualizáljuk. plot_cap_1 &lt;- read.csv(&quot;data/plot_cap_1.csv&quot;, head = TRUE, sep = &quot;;&quot;) ggplot(plot_cap_1, aes(Year, fill = Subtopic)) + scale_x_discrete(limits = c(1957, 1958, 1959, 1960, 1961, 1962, 1963)) + geom_bar(position = &quot;dodge&quot;) + labs( x = NULL, y = NULL, title = &quot;A Magyar Közlönyben kihirdetett agrárpolitikai jogszabályok&quot;, subtitle = &quot;N=445&quot; ) + coord_flip() + # az ábra tipusa theme_minimal() + theme(plot.title = element_text(size = 12)) A csv-bl belolvasott adatainból kördiagramot is készíthetünk pie &lt;- read.csv(&quot;data/pie.csv&quot;, head = TRUE, sep = &quot;;&quot;) ggplot(pie, aes(x = &quot;&quot;, y = value, fill = Type)) + geom_bar(stat = &quot;identity&quot;, width = 1) + coord_polar(&quot;y&quot;, start = 0) + scale_fill_brewer(palette = &quot;GnBu&quot;) + labs( title = &quot;A Magyar Közlönyben megjelent jogszabályok típusai&quot;, subtitle = &quot;N = 445&quot; ) + theme_void() Arun, Rajkumar, Venkatasubramaniyan Suresh, CE Veni Madhavan, and MN Narasimha Murthy. 2010. On Finding the Natural Number of Topics with Latent Dirichlet Allocation: Some Observations. In, 391402. Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research 3 (Jan): 9931022. Burtejin, Zorgit. 2016. Csoportosítás (Klaszterezés). In, edited by Miklós Sebk, 85101. Budapest: LHarmattan. Cao, Juan, Tian Xia, Jintao Li, Yongdong Zhang, and Sheng Tang. 2009. A Density-Based Method for Adaptive LDA Model Selection. Neurocomputing 72 (7-9): 17751781. Deveaud, Romain, Eric SanJuan, and Patrice Bellot. 2014. Accurate and Effective Latent Concept Modeling for Ad Hoc Information Retrieval. Document Numérique 17 (1): 6184. Griffiths, T. L., and M. Steyvers. 2004. Finding Scientific Topics. Proceedings of the National Academy of Sciences 101 (Supplement 1): 522835. https://doi.org/10.1073/pnas.0307752101. Grimmer, Justin, and Brandon M Stewart. 2013a. Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis 21 (3): 267297. . 2013b. Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis 21 (3): 267297. Jacobi, Carina, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative Analysis of Large Amounts of Journalistic Texts Using Topic Modelling. Digital Journalism 4 (1): 89106. Laver, Michael, Kenneth Benoit, and John Garry. 2003. Extracting Policy Positions from Political Texts Using Words as Data. American Political Science Review, 311331. Laver, Michael, and John Garry. 2000. Estimating Policy Positions from Political Texts. American Journal of Political Science, 619634. Loughran, Tim, and Bill McDonald. 2011. When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks. The Journal of Finance 66 (1): 3565. Máté, Ákos, Miklós Sebk, and Tamás Barczikay. 2021. The Effect of Central Bank Communication on Sovereign Bond Yields: The Case of Hungary. Edited by Hiranya K. Nath. PLOS ONE 16 (2): e0245515. https://doi.org/10.1371/journal.pone.0245515. Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. arXiv Preprint arXiv:1301.3781. Mikolov, Tomas, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Advances in Pre-Training Distributed Word Representations. In. Pennington, Jeffrey, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word Representation. In, 15321543. Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and David G Rand. 2014. Structural Topic Models for Open-Ended Survey Responses. American Journal of Political Science 58 (4): 10641082. Silge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy Approach. \" OReilly Media, Inc.\". Spirling, Arthur, and Pedro L Rodriguez. n.d.a. Word Embeddings. https://polmeth.mit.edu/sites/default/files/documents/Pedro_Rodriguez.pdf. . n.d.b. Word Embeddings. https://polmeth.mit.edu/sites/default/files/documents/Pedro_Rodriguez.pdf. Tan, Pang-Ning, Michael Steinbach, and Vipin Kumar. 2011. Bevezetés Az Adatbányászatba. Panem Kft. Tikk, Domonkos. 2007a. Szövegbányászat. Budapest: Typotext. . 2007b. Szövegbányászat. Budapest: Typotext. Wickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" OReilly Media, Inc.\". Young, Lori, and Stuart Soroka. 2012. Affective News: The Automated Coding of Sentiment in Political Texts. Political Communication 29 (2): 205231. "]]
