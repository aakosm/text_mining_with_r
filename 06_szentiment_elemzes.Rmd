# Szótáralapú elemzések, érzelem-elemzés

```{r include=FALSE}

source("_common.R")

```

```{r echo=FALSE}
poltext_szotar <- read_rds("data/poltext_dict.Rds")
```

A szentimentelemzés a számítógépes nyelvészet részterülete, melynek célja az egyes szövegek tartalmából kinyerni azokat az információkat, amelyek értékelés fejeznek ki, azaz szentimentértékkel bírnak.[^szent-1] A szentimentelemzés a szövegeket három szinten osztályozza. A legáltalánosabb a dokumentumszintű osztályozás, amikor egy hosszabb szövegegység egészét vizsgáljuk, míg a mondatszintű osztályozásnál a vizsgálat alapegysége a mondat. A legrészletesebb adatokat akkor nyerjük, amikor az elemzést target-szinten végezzük, azaz meghatározzuk azt is, hogy egy-egy érzelem a szövegen belül mire vonatkozik. Mindhárom szinten azonos a feladat, egyrészt meg kell állapítani, hogy az adott egységben van-e értékelés, vélemény vagy érzelem és ha igen, akkor pedig meg kell határozni, hogy milyen azok érzelmi tartalma. A pozitív-negatív-semleges skálán mozgó szentimentelemzéshez mellett az elmúlt két évtizedben jelentős lépések történetek a szövegek emóciótartalmának automatikus vizsgálatára is. A módszer hasonló a szentimentelemzéshez, tartalmilag azonban más skálán mozog. Az emócióelemzés esetén ugyanis, nem csak azt kell meghatározni, hogy egy kifejezés pozitív bvagy negatív tölettel rendelkezik, hanem azt is, hogy milyen érzelmet (öröm, bánat, undor, stb) hordoz. A szótár alapú szentiment- vagy emócióelemzés alapja az az egy egyszerű ötlet, hogy ha tudjuk, hogy egyes szavak milyen érzelmeket, érzéseket hordoznak, akkor ezeket a szavakat egy szövegben megszámolva képet kaphatunk az adott dokumentum érzelmi tartalmáról.Mivel a szótár alapú elemzés az adott szentiment kategórián belüli kulcsszavak gyakoriságán alapul, ezért van aki nem tekinti statisztikai elemzésnek (lásd például @young2012affective). A tágabb kvantitatív szövegelemzési kontextusban az osztályozáson (classification) belül a felügyelt módszerekhez hasonlóan itt is ismert kategóriákkal dolgozunk, azaz előre meghatározzuk, hogy egy-egy adott szó pozitív vagy negatív szentimentértékű, vagy továbbmenve, milyen érzelmet hordoz csak egyszerűbb módszertannal [@grimmer2013text].A kulcsszavakra építés miatt a módszer a kvalitatív és kvantitatív kutatási vonalak találkozásának is tekinthető, hiszen egy-egy szónak az érzelmi töltete nem mindig ítélhető meg objektíven. Mint minden módszer esetében, itt is kiemelten fontos hogy ellenőrízzük hogy a használt szótár kategóriák és kulcsszavak fedik-e a valóságot. Más szavakkal: *validate, validate, validate*.

[^szent-1]: Bővebben lásd például: [@liuSentimentAnalysisSubjectivity2010]

**A módszer előnyei:**

-   Tökéletesen megbízható: a számításoknak nincsen probabilisztikus (azaz valószínűségre épülő) eleme, mint például a Support Vector alapú osztályozásnak, illetve az emberi szövegkódolásnál előforduló problémákat (például, hogy két kódoló, vagy ugyanazon kodóló két különböző időpontban nem azonosan értékeli ugyanazt a kifejezést) is elkerüljük így.
-   Képesek vagyunk vele mérni a szöveg látens dimenzióit.
-   Széles körben alkalmazható, egyszerűen számolható. A politikatudományon és számítógépes nyelvészeten belül nagyon sok kész szótár elérhető, amelyek különböző módszerekkel készültek és különböző területet fednek le (pl.: populizmus, pártprogramok policy tartalma, érzelmek, gazdasági tartalom.)[^szent-2]
-   Relatíve könnyen adaptálható egyik nyelvi környezetből másikba, bár szótárfordítások esetén külön hangsúlyt kell fektetni a validálásra.

[^szent-2]: A lehetséges, területspecifikus szótáralkotási módszerekről részletesebben ezekben a cikkekben lehet olvasni: @laver2000estimating; @young2012affective; @loughranWhenLiabilityNot2011; @mateEffectCentralBank2021

**A módszer lehetéges hátrányai:**

-   A szótár hatékonysága és validitása azon múlik hogy mennyire egyezik a szótár és a vizsgálni kívánt dokumentum területe. Nem mindegy például, hogy a szótárunkkal a gazdasági bizonytalanságot szeretnék tőzsdei jelentések alapján vizsgálni vagy nézők filmekre adott értékeléseit. Léteznek általános szentimentszótárak, ezek hatékonysága azonban általában alulmúlja a terület-specifikus szótárakét. (Nem mindegy például, hogy valaminek )
-   A terület-specifikus szótár építése egy kvalitatív folyamat (lsd. a lábjegyzetben), éppen ezért idő és emberi erőforrás igényes.
-   A szózsák alapú elemzéseknél a kontextus elvész. Gondoljunk például a tagadásra: a *"nem vagyok boldog"* kifejezés esetén egy általános szentiment szótár a tagadás miatt félreosztályozná a mondat érzelmi töltését, hiszen a boldog szó önmagában a pozitív kategóriába tartozik. Természetesen az automatikus tagadás-kezelésre is vannak lehetőségek, de a kérdés komplexitása miatt ezek bemutatásától most eltekintünk.

A legnagyobb méretű általános szentimentszótár az angol nyelvű SentiWordNet (SWN), ami kb. 150 000 szót tartalmaz, amelyek mindegyike a három szentimentérték - pozitív, negatív, semleges - közül kapott egyet.[^szent-3][@baccianellaSentiwordnetEnhancedLexical2010]

[^szent-3]: A szótár és dokumentációja elérhető az alábbi linken: [https://github.com/aesuli/SentiWordNet](https://github.com/aesuli/SentiWordNet)

Az R-ben végzett szentimentelemzés során, angol nyelvű szövegekhez több beépített általános szentimentszótár is a rendelkezésünkre áll.[^szent-4] A teljesség igénye nélkül említhetjük az AFINN[^szent-5], a bing[^szent-6] és a az nrc[^szent-7] szótárakat. Az elemzés sikere több faktortól is függ. Fontos hogy a korpuszban lévő dokumentumokat körültekintően tisztítsuk meg az elemzés elején (lásd a 4. fejezetet a szövegelőkészítésről). A következő lépésben meg kell bizonyosodnunk arról, hogy a kiválasztott szentiment szótár alkalmazható a korpuszunkra. Amennyiben nem találunk alkalmas szótárt, akkor a saját szótár validálására kell figyelni. A negyedik fejezetben leírtak itt is érvényesek, érdemes a dokumentum-kifejezés mátrixot súlyozni valamilyen módon.

[^szent-4]: A quanteda.dictionaries csomag leírása és a benne található szótárak az alábbi github linken érhetőek el: <https://github.com/kbenoit/quanteda.dictionaries>

[^szent-5]: <http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html>

[^szent-6]: <https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html>

[^szent-7]: <http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm>

## Szótárak az R-ben

A szótár alapú elemzéshez a `quanteda` csomagot fogjuk használni, illetve a 3. fejezetben már megismert `readr`, `stringr`, `dplyr` tidyverse csomagokat.[^szent-8]

[^szent-8]: A szentiment elemzéshez gyakran használt csomag még a `tidytext`. Az online is szabadon elérhető @silge2017text 2. fejezetében részletesen is bemutatják a szerzők a `tidytext` munkafolyamatot ([https://www.tidytextmining.com/sentiment.html](https://www.tidytextmining.com/sentiment.html)).

```{r message=FALSE}
library(readr)
library(stringr)
library(dplyr)
library(ggplot2)
library(quanteda)
```

Mielőtt a két esettanulmányt bemutatnánk, vizsgáljuk meg hogy hogyan néz ki egy szentiment szótár az R-ben. A szótárt kézzel úgy tudjuk létrehozni, hogy egy listán belül létrehozzuk karaktervektorként a kategóriákat és a kulcsszavakat és ezt a listát a `quanteda` `dictionary` függvényével eltároljuk.

```{r}
szentiment_szotar <- dictionary(
  list(
    pozitiv = c("jó", "boldog", "öröm"),
    negativ = c("rossz", "szomorú", "lehangoló")
    )
  )

szentiment_szotar
```

A `quanteda`, `quanteda.corpora` és `tidytext` R csomagok több széles körben használt szentiment szótárat tartalmaznak, így nem kell kézzel replikálni minden egyes szótárat amit használni szeretnénk.

A szentiment elemzési munkafolyamat amit a részfejezetben bemutatunk a következő lépésekből áll:

1.  dokumentumok betöltése
2.  szöveg előkészítése
3.  a korpusz létrehozása
4.  dokumentum-kifejezés mátrix
5.  szótár betöltése
6.  a dokumentum-kifejezés mátrix szűrése a szótárban lévő kulcsszavakkal
7.  az eredmény vizualizálása, további felhasználása

A fejezetben két különböző korpuszt fogunk elemezni: a 2006-os Magyar Nemzet címlapjainak egy 252 cikkből álló mintájának szentimentjét vizsgáljuk egy magyar szentiment szótárral.[^szent-9] A második korpusz a Magyar Nemzeti Bank angol nyelvű sajtóközleményeiből áll, amin bemutatjuk egy széles körben használt gazdasági szótár használatát.[^szent-10]

[^szent-9]: A korpusz a Hungarian Comapartive Agendas Project keretében készült és regisztáció után, kutatási célra elérhető az alábbi linken: [https://cap.tk.hu/a-media-es-a-kozvelemeny-napirendje](https://cap.tk.hu/a-media-es-a-kozvelemeny-napirendje).

[^szent-10]: A korpusz, a szótár és az elemzés teljes dokumentációja elérhető az alábbi github linken: [https://github.com/poltextlab/central_bank_communication](https://github.com/poltextlab/central_bank_communication), a teljes elemzés [@mateEffectCentralBank2021]elérhető: [https://doi.org/10.1371/journal.pone.0245515](https://doi.org/10.1371/journal.pone.0245515)

## Magyar Nemzet cikkek

```{r message=FALSE}
mn_minta <- read_csv("data/magyar_nemzet_small.csv")

summary(mn_minta)
```

A `read_csv()` segítségével beolvassuk a Magyar Nemzet adatbázis egy kisebb részét, ami az esetünkben a 2006-os címlapokon szereplő hírek . A `summary()`, ahogy a neve is mutatja, egy gyors áttekintést nyújt a betöltött adatbázisról. Látjuk, hogy `r nrow(mn_minta)` sorból (megfigyelés) és `r ncol(mn_minta)` oszlopból (változó) áll. Az első ránézésre látszik hogy a text változónk tartalmazza a szövegeket, és hogy tisztításra szorulnak.

Az első szöveget megnézve látjuk, hogy a standard előkészítési lépések mellett a sortörést (`\n`) is ki kell törölnünk.

```{r tidy = TRUE, tidy.opts=list(width.cutoff=80)}
mn_minta$text[1]
```

Habár a `quanteda` is lehetőséget ad néhány előkészítő lépésre, érdemes ezt olyan céleszközzel tenni ami nagyobb rugalmasságot ad a kezünkbe. Mi erre a célra a `stringr` csomagot használjuk. Első lépésben kitöröljük a sortöréseket (`\n`), a központozást, számokat, kisbetűsítünk minden szót. Előfordulhat hogy (számunkra nehezen látható) extra szóközök maradnak a szövegben. Ezeket az `str_squish()`-el tüntetjük el. A szöveg eleji és végi extra szóközöket (ún. leading vagy trailing white space) az `str_trim()` vágja le.

```{r}
mn_tiszta <- mn_minta %>%
  mutate(
    text = str_remove_all(string = text, pattern = "\n"),
    text = str_remove_all(string = text, pattern = "[:punct:]"),
    text = str_remove_all(string = text, pattern = "[:digit:]"),
    text = str_to_lower(text),
    text = str_trim(text),
    text = str_squish(text)
  )
```

A szöveg sokkal jobban néz ki, habár észrevehetjük hogy maradhattak benne problémás részek, főleg a sortörés miatt, ami sajnos hol egyes szavak közepén van (a jobbik eset), vagy pedig pont szóhatáron, ez esetben a két szó sajnos összevonódik. Az egyszerűség kedvéért feltételezzük hogy ez kellően ritkán fordul elő ahhoz hogy ne befolyásolja az elemzésünk eredményét.

```{r tidy = TRUE, tidy.opts=list(width.cutoff=80)}
mn_tiszta$text[1]
```

Miután kész a tiszta(bb) szövegünk, kreálunk egy korpuszt a quanteda `corpus()` fuggvenyevel. A létrehozott corpus objektum a szöveg mellett egyéb dokumentum meta adatokat is tud tárolni (dátum, író, hely, stb.) Ezeket mi is hozzáadhatjuk (erre majd látunk példát nemsokára) illetve amikor létrehozzuk a korpuszt a data frame-ünkből, akkor automatikusan meta adatokként tárolódnak az változóink. Jelen esetben az egyetlen dokumentum változónk az a dátum lesz a szöveg mellett. A korpusz dokumentum változóihoz a `docvars()` segítségével tudunk hozzáférni.

```{r}
mn_corpus <- corpus(mn_tiszta)

head(docvars(mn_corpus), 5)
```

A következő lépés a dokumentum-kifejezés mátrix létrehozása a `dfm()` függvénnyel (ami a *document-feature matrix* rövidítése). Előszőr tokenekre bontjuk a szövegeket a `tokens()`-el, és aztán ezt a tokenizált szózsákot kapja meg a dfm inputnak. A sornak a végén a létrehozott mátrixunkat TF-IDF módszerrel súlyozzuk a `dfm_tfidf()` használatával.

```{r}
mn_dfm <- mn_corpus %>%
  tokens(what = "word") %>%
  dfm() %>%
  dfm_tfidf()
```

A cikkek szentimentjét egy magyar szótárral fogjuk becsülni, amit a Társadalomtudományi Kutatóközpont kutatói a Mesterséges Intelligencia Nemzeti Laboratórium projekt keretében készítettek.[^szent-11] Két dimenziót tarlamaz (pozitív és negatív), `r length(poltext_szotar$positive)` pozitív és `r length(poltext_szotar$negative)` negatív kulcsszóval. Ez nem számít kirívóan nagynak a szótárak között, mivel az adott kategóriák minél teljesebb lefedése a cél.

[^szent-11]: [https://milab.tk.hu/hu](https://milab.tk.hu/hu) A szótár és a hozzátartozó dokumentáció elérhető az alábbi github oldalon: <https://github.com/poltextlab/sentiment_hun>

```{r eval=FALSE}
poltext_szotar <- read_rds("data/poltext_dict.Rds")
```

```{r tidy = TRUE, tidy.opts=list(width.cutoff=80)}
poltext_szotar
```

Az egyes dokumentumok szentimentjét a `dfm_lookup()` becsüli, ahol az előző lépésben létrehozott súlyozott dfm az input és a magyar szentimentszótár a dictionary. Egy gyors pillantás az eredményre és látjuk hogy minden dokumentumhoz készült egy pozitív és egy negatív értéket. A TF-IDF súlyozás miatt nem látunk egész számokat (a súlyozás nélkül a sima szófrekvenciát kapnánk).

```{r}
mn_szentiment <- dfm_lookup(mn_dfm, dictionary = poltext_szotar)

head(mn_szentiment, 5)

```

Ahhoz hogy fel tudjuk használni a kapott eredményt, érdemes dokumentumváltozóként eltárolni a korpuszban. Ezt a fent már használt `docvars()` segítségével tudjuk megtenni, ahol a második argumentumkét az új változó nevét adjuk meg stringként.

```{r}

docvars(mn_corpus, "pos") <- as.numeric(mn_szentiment[, 1])
docvars(mn_corpus, "neg") <- as.numeric(mn_szentiment[, 2])

head(docvars(mn_corpus), 5)

```

Végül a kapott korpuszt a kiszámolt szentiment értékekkel a `quanteda`-ban lévő `convert()` fügvénnyel data frame-é alakítjuk. A convert függvény dokumentációját érdemes elolvasni, mert ennek segítségével tudjuk a `quanteda`-ban elkészült objektumainkat átalakítani úgy, hogy azt más csomagok is tudják használni.

```{r}
mn_df <- convert(mn_corpus, to = "data.frame")


summary(mn_df)
```

Mielőtt vizualizálnánk az eredményt érdemes a napi szintre aggregálni a szentimentet és egy nettó értéket kalkulálni.[^szent-12]

[^szent-12]: A csoportosított adatokkal való munka bővebb bemutatását lsd. a Függelékben

```{r}
mn_df <- mn_df %>%
  group_by(doc_date) %>%
  summarise(
    daily_pos = sum(pos),
    daily_neg = sum(neg),
    net_daily = daily_pos - daily_neg
  )
```

A plot alapján a szentimentekben több kiugrást is talapsztalhatunk. Természetesen messzemenő következtetéseket egy ilyen kis korpusz alapján nem vonhatunk le, de a kiugrásokhoz tartozó cikkek kvalitatív vizsgálatával megállapíthatjuk, hogy az áprilisi kiugrást a választásokhoz kötődő cikkek pozitív hangulata, míg az októberi negatív kilengést az öszödi beszéd nyilvánosságra kerüléséhez köthető cikkek negatív szentimentje okozza.

```{r tidy = TRUE, tidy.opts=list(width.cutoff=80), fig.cap="Magyar Nemzet címlap szentimentje"}
ggplot(mn_df, aes(doc_date, net_daily)) +
  geom_line() +
  labs(
    y = "Szentiment",
    x = NULL,
    caption = "Adatforrás: https://cap.tk.hu/"
  )
```

## MNB sajtóközlemények

A második esettanulmányban a kontextuális szótár elemzést mutatjuk be egy angol nyelvű korpusz és specializált szótár segítségével. A korpusz az MNB kamatdöntéseit kísérő nemzetközi sajtóközleményei és a szótár pedig a @loughranWhenLiabilityNot2011 pénzügyi szentimentszótár.[^szent-13] A szótár a `quanteda.dictionaries` csomag részeként elérhető, illetve a tankönyv honlapján is megtalálható.

[^szent-13]: A témával részletesebben is foglalkoztunk a @mateEffectCentralBank2021 tanulmányban, ahol egy saját monetáris szentiment szótárt mutatunk be. Az implementáció és a hozzá tartozó R forráskód a nyilvános <https://doi.org/10.6084/m9.figshare.13526156.v1> linken.

```{r}
penzugy_szentiment <- readRDS("data/dictionary_LoughranMcDonald.Rds")
```

```{r tidy = TRUE, tidy.opts=list(width.cutoff=80)}
penzugy_szentiment
```

A szentiment szótár `r length(penzugy_szentiment)` kategóriából áll. A legtöbb kulcsszó a negatív dimenzióhoz van (`r length(penzugy_szentiment[["NEGATIVE"]])`).

A munkamenet hasonló a Magyar Nemzetes példához:

1.  adat betöltés
2.  szövegtisztítás
3.  korpusz
4.  tokenek
5.  kulcs kontextuális tokenek szűrése
6.  dfm előállítás és szentiment számítás
7.  az eredmény vizualizálása, további felhasználása

```{r}
mnb_pr <- read_csv("data/mnb_pr_corpus.csv")

summary(mnb_pr)
```

Az adatbázisunk `r nrow(mnb_pr)` megfigyelésből és `r ncol(mnb_pr)` változóból áll. Az egyetlen lényeges dokumentum meta adat itt is a szövegek megjelenési ideje.

A szövegeket ugyanazokkal a standard eszközökkel kezeljük mint a Magyar Nemzet esetében. Érdemes minden esetben ellenőrizni, hogy az R kód amit használunk az tényleg azt csinálja-e mint amit szeretnénk hogy csináljon. Ez hatványozottan igaz abban az esetben, amikor szövegekkel és regular expressionökkel dolgozunk.

```{r}
mnb_tiszta <- mnb_pr %>%
  mutate(
    text = str_remove_all(string = text, pattern = "[:cntrl:]"),
    text = str_remove_all(string = text, pattern = "[:punct:]"),
    text = str_remove_all(string = text, pattern = "[:digit:]"),
    text = str_to_lower(text),
    text = str_trim(text),
    text = str_squish(text)
  )
```

Miután rendelkezésre állnak a tiszta dokumentumaink, egy karaktervektorba gyűjtjük azokat a kulcsszavakat amelyek környékén szeretnénk megfigyelni a szentiment alakulását. A példa kedvéért mi az `unemp*`, `growth`, `gdp`, `inflation*` szótöveket és szavakat választottuk. A `tokens_keep()` megtartja a kulcsszavainkat és egy általunk megadott +/- n tokenes környezetüket (jelen esetben 10). A szentiment elemzést pedig már ezen a jóval kisebb mátrixon fogjuk lefuttatni. A `phrase()` segítségével több szóból álló kifejezéséket is vizsgálhatunk. Ilyen szókapcsolat például az *Európai Unió* is, ahol lényeges hogy egyben kezeljük a két szót.

```{r}
mnb_corpus <- corpus(mnb_tiszta)

gazdasag <- c("unemp*", "growth", "gdp", "inflation*", "inflation expectation*")

mnb_token <- tokens(mnb_corpus) %>%
  tokens_keep(pattern = phrase(gazdasag), window = 10)
```

A szentimentet most is egy súlyozott dfm-ből számoljuk. A kész eredményt hozzáadjuk a korpuszhoz majd data framet hozunk létre belőle. A 9 kategóriából 5-öt adunk választunk csak ki, amelyeknek jegybanki környezetben értelmezhető tartalma van.

```{r}
mnb_szentiment <- tokens_lookup(mnb_token, dictionary = penzugy_szentiment) %>%
  dfm() %>%
  dfm_tfidf()

docvars(mnb_corpus, "negative") <- as.numeric(mnb_szentiment[, "negative"])
docvars(mnb_corpus, "positive") <- as.numeric(mnb_szentiment[, "positive"])
docvars(mnb_corpus, "uncertainty") <- as.numeric(mnb_szentiment[, "uncertainty"])
docvars(mnb_corpus, "constraining") <- as.numeric(mnb_szentiment[, "constraining"])
docvars(mnb_corpus, "superfluous") <- as.numeric(mnb_szentiment[, "superfluous"])

mnb_df <- convert(mnb_corpus, to = "data.frame")
```

A célunk hogy szentiment kategóriánkénti bontásban mutassuk be az elemzésünk eredményét, de előtte egy kicsit alakítani kell a data frame-n, hogy a második fejezetben is tárgyalt *tidy* formára hozzuk. A különböző szentiment értékeket tartalmazó oszlopokat fogjuk átrendezni úgy hogy kreálunk egy "sent_type" változót ahol a kategória nevet fogjuk eltárolni és egy "sent_score" változót, ahol a szentiment értéket. Ehhez a `tidyr`-ben található `pivot_longer()` -t használjuk.

```{r}
mnb_df <- mnb_df %>%
  pivot_longer(
    cols = negative:superfluous,
    names_to = "sent_type",
    values_to = "sent_score"
  )
```

Az átalakítás után már könnyedén tudjuk kategóriákra bontva megjeleníteni az MNB közlemények különböző látens dimenzióit. Fontos emlékezni arra, hogy ez az eredmény a kulcsszavaink +/- 10 tokenes környezetében lévő szavak szentimentjét mérik. Ami érdekes eredmény, hogy a felesleges "töltelék" szövegek (superflous kategória) szinte soha nem fordulnak elő a kulcsszavaink körül. A többi érték is nagyjából megfelel a várakozásainknak, habár a 2008-as gazdasági válság nem tűnik kiugró pontnak. Azonban a 2010 utáni európai válság már láthatóan megjelenik az idősorainkban.

A szótár amit használtunk az alapvetően az Egyesült Államokban a tőzsdén kereskedett cégek publikus beszámolóiból készült így elképzelhető, hogy egyes jegybanki környezetben sokat használt kifejezés nincs benne. A validálásra a kapott eredményeknek ezért is nagyon fontos, illetve érdemes azzal is tisztában lenni hogy a szótáras módszer nem tökéletes (ahogy az emberi vagy más gépi kódolás sem).

```{r fig.cap="Magyar Nemzeti Bank közleményeinek szentimentje"}
ggplot(mnb_df, aes(date, sent_score)) +
  geom_line() +
  labs(
    y = "Szentiment",
    x = NULL
  ) +
  facet_wrap(~sent_type, ncol = 2)
```
