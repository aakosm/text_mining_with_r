# Osztályozás és felügyelt tanulás

## 12.1. Fogalmi alapok

## 12.2. Osztályozás felügyelt tanulással

Az alábbi fejezetben a CAP magyar média gyűjteményéből a napilap címlapokat tartalmazó modult használjuk.[^1] Az induló adatbázis 61 835 cikk szövegét és metaadatait (összesen öt változót: sorszám, fájlnév, a közpolitikai osztály kódja, szöveg, illetve a korpusz forrása -- *Magyar Nemzet* vagy *Népszabadság*) tartalmazza. Az a célunk, hogy az egyes cikkekhez kézzel, jó minőségben (két, egymástól függetlenül dolgozó kódoló által) kiosztott és egyeztetett közpolitikai kódokat -- ez a tanítóhalmaz -- arra használjuk, hogy meghatározzuk egy kiválasztott cikkcsoport hasonló kódjait. Az osztályozási feladathoz a CAP közpolitikai kódrendszerét használjuk, mely 21 közpolitikai kategóriát határoz meg az oktatástól az egészségügyön át a honvédelemig [^2]

[^1]: <https://cap.tk.hu/a-media-es-a-kozvelemeny-napirendje>

[^2]: [Közpolitikai témakörök kódkönyve](Comparative%20Agendas%20Project)[<https://cap.tk.hu/kozpolitikai-cap>]

Annak érdekében, hogy egyértelműen értékelhessük a gépi tanulás hatékonyságát, a kiválasztott cikkcsoport (azaz a teszthalmaz) esetében is ismerjük a kézi kódolás eredményét („éles“ kutatási helyzetben, ismeretlen kódok esetében ugyanakkor ezt gyakran szintén csak egy kisebb mintán tudjuk kézzel validálni). További fontos lépés, hogy az ésszerű futási idő érdekében a gyakorlat során a teljes adatbázisból -- és ezen belül is csak a Népszabadság részhalmazból -- fogunk venni egy 4500 darabos mintát. E mintán fogjuk vizsgálni, hogy milyen hatékonysággal képes a modellünk egy megadott közpolitikai kódba besorolni egy adott cikket, illetve, hogy ezt képes-e a hús-vér kutatókkal azonos színvonalon megtenni. Nézzük mindezek után a kutatás lépéseit!

A munkakönyvtár megfelelő beállítása után (`setwd()`) először behívjuk a szükséges csomagokat, melyek közül a `quanteda` a szokásos szövegbányászati alapcsomagunk, az `e1071` az SVM algoritmus használatát teszi a lehetővé, a `ggplot2` a vizualizációt segíti, a `dplyr` a korpuszon végzett műveletekhez kell, a `SparseM` pedig a mátrixtranszformációkhoz.

```{r include = TRUE}
library(quanteda)
library(e1071)
library(ggplot2)
library(dplyr)
library(SparseM)
library(HunMineR)

set.seed(1234)
```

Ezt követően meg adjuk azt a kódkategóriát, melynek kapcsán szeretnénk elvégezni a bináris osztályozást (beletartozik-e az adott cikk az adott kategóriába vagy nem). A kódban ez az egyes, azaz a makrogazdaság (adózás, költségvetés, monetáris politika stb.) lesz.

```{r include = TRUE}
CAPcode <- 1
```

Ezt követi a szövegelőkészítésen és tisztításon már átesett adatok betöltése. Ahogyan arról korábban már volt szó, `read.csv` függvény használatakor megadhatjuk a beolvasás paramétereit, így például, hogy milyen karakterkódolást szeretnénk használni (esetünkben ez az utf-8 lesz).

```{r include = TRUE}
Data_NOL_MNO <- data_nol_mno_clean
```

A következő lépésben eltávolítjuk a szöveges adatot nem tartalmazó (a text változóra semmilyen értéket nem adó) sorokat az adatbázisból.

```{r include = TRUE}
Data_NOL_MNO_ures <- Data_NOL_MNO[Data_NOL_MNO$text == "",]
Data_NOL_MNO <- Data_NOL_MNO[!(Data_NOL_MNO$filename %in% Data_NOL_MNO_ures$filename),]
```

```{r include = TRUE}
nrow(Data_NOL_MNO_ures)
```

Látható, hogy összesen 13 ilyen elemet találtunk.

Majd szétválasztjuk a két újsághoz tartozó cikkeket is (esetünkben a *Népszabadságot* alapul véve).

```{r include = TRUE}
Data_NOL <- Data_NOL_MNO[Data_NOL_MNO$corpus == "NOL",]
```

Mint az **Environment** ablakban is látható, ez a lépés 37 135 cikkes halmazt adott. Ezen a ponton megkezdhetjük a felügyelt gépi tanulásra épülő lépéseket! Először egy 4500-as mintát veszünk az adatbázisból, hogy rövidítsük a futásidőt.

```{r include = TRUE}
Data_NOL <- Data_NOL[sample(nrow(Data_NOL), 4500), ]
```

Majd kutatási feladatunknak megfelelően felveszünk egy új címkét (*label*), ami már kifejezetten azt mutatja, hogy egy cikk a makrogazdasági (1) vagy bármilyen más (0) osztályba tartozik-e.

```{r include = TRUE}
Data_NOL$label <- ifelse(Data_NOL$majortopic_code == CAPcode, 1, 0)
```

Ezt követően felosztjuk a 4500 cikket 2:1 arányban tanító- és teszthalmazra.

```{r include = TRUE}
training <- Data_NOL[sample(nrow(Data_NOL), 3000), ]
Data_NOL$training <- ifelse(Data_NOL$filename %in% training$filename, 1, 0)
```

Ahhoz, hogy a `quanteda` dtm-et tudjon készíteni a cikk-gyűjteményünkből, először egy korpusz objektumot kell létrehozni belőlük.

```{r include = TRUE}
lemma_corpus_NOL <- corpus(Data_NOL, docid_field = "filename", text_field = "text")
```

Így a dfm függvénnyel már létre tudjuk hozni a dokumentum-kifejezés mátrixunkat!

```{r include = TRUE}
dtm_NOL <- dfm(lemma_corpus_NOL)
```

A dimenzió-csökkentés érdekében eltávolítjuk a ritka (5-nél kevesebbszer szereplő) kifejezéseket a mátrixból. Láthatjuk, hogy ezzel a feature -ök majd 81 százalékát eltávolítottuk!

```{r include = TRUE}
dtm_NOL <- dfm_trim(dtm_NOL, min_docfreq = 5, verbose=TRUE)
```

Ezen a ponton megkezdhetjük a modellünk tanítását! Erre a fent említett SVM algoritmust használjuk. E lépés futási ideje akár 1-2 percig is eltarthat!

```{r eval=FALSE}
SVMmodel <- svm(
  x=dtm_NOL[dtm_NOL@docvars$training == 1,], 
  y	=dtm_NOL[dtm_NOL@docvars$training == 1,]@docvars$label,
  kernel="linear", 
  cost	=0.1, 
  probability=TRUE
  )
```

```{r echo=FALSE}
#saveRDS(SVMmodel, file = "data/temp/svm_model.rds")

SVMmodel <- readRDS("data/temp/svm_model.rds")
```


A létrejövő *predictions* objektum tartalmazza majd az előrejelzési eredményeket.

```{r include = TRUE}
predictions <- predict(SVMmodel, dtm_NOL[dtm_NOL@docvars$training == 0,])
```

```{r include = TRUE}
Data_NOL_predictions <- cbind(Data_NOL[Data_NOL$training == 0,], predictions)
```

Ezt követően meghatározzuk azt a küszöbértéket, ahonnan egy előrejelzést 1-es címkéhez tartozónak (azaz egy cikk szövegét makrogazdaságinak) tekintünk.

```{r include = TRUE}
cutoff_point = 0.5
```

Majd ez alapján átalakítjuk az SVM eredményeinket immár a végleges, a bináris osztályozási feladatnak megfelelő előrejelzésekké. A`cbind` függvénnyel pedig összevonjuk egy táblázatba az előrejelzés eredményét és ennek cimkéit.

```{r include = TRUE}
predictions_label <- ifelse(predictions > 0.5, 1, 0)
```

```{r include = TRUE}
Data_NOL_predictions <- cbind(Data_NOL_predictions, predictions_label)
```

Miután eredményeink elkészültek, kiszámoljuk a felügyelt gépi tanulás szokásos eredményességi mutatóit (ettől a résztől kódunkban Pablo Barbera munkájára támaszkodunk)[^3]. Mindezek alapjául a tévesztési mátrix szolgál, vizsgáljuk először meg ennek adatait!

[^3]: [SVM, Random Forests, and beyond](http://pablobarbera.com/ECPR-SC105/code/12-advanced-sml.html).

```{r include = TRUE}
table(Data_NOL_predictions$predictions_label, Data_NOL_predictions$label)
```

Látható, hogy modellünk alapvetően helyesen értékelte, hogy a teszthalmaz cikkeinek döntő többsége nem makrogazdasági tárgyú (1139). Szintén sikerrel sorolt be 83 cikket a makrogazdasági osztályba. Ugyanakkor 117-et hibásan tartott 1-es kategóriásnak, 161-et pedig nem "talált meg" közülük.

A mutatószámok esetében kezdjük a legáltalánosabbal, a hitelességgel (*accuracy*), ami a mátrix átlójára épít!

```{r include = TRUE}
accuracy <- function(ypred, y){
  tab <- table(ypred, y)
  return(sum(diag(tab))/sum(tab))
}

```

```{r include = TRUE}
accuracy(Data_NOL_predictions$predictions_label, Data_NOL_predictions$label)
```

Ennek adatai azt mutatják, hogy nagy általánosságban jól működött a modell. Ugyanakkor kutatási kérdésünk szempontjából fontosabb, hogy milyen arányban találtuk meg a makrogazdasági cikkeket (felidézés - *recall*), illetve milyen pontossággal osztottuk ki ezeket a kódokat (pontosság - *precision*). A következő lépésben így először definiáljuk, majd kiszámoljuk a pontosságot és a felidézést, valamint ezek harmonikus átlagát, az F1 mutatót.

```{r include = TRUE}
precision <- function(ypred, y){
  tab <- table(ypred, y)
  return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
```

```{r include = TRUE}
recall <- function(ypred, y){
  tab <- table(ypred, y)
  return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
```

```{r include = TRUE}
F1 <- function(ypred, y){
  return(2*precision(ypred, y)*recall(ypred, y)/(precision(ypred, y)+recall(ypred, y)))
}
```

```{r include = TRUE}
precision(Data_NOL_predictions$predictions_label, Data_NOL_predictions$label)
recall(Data_NOL_predictions$predictions_label, Data_NOL_predictions$label)
F1(Data_NOL_predictions$predictions_label, Data_NOL_predictions$label)
```

Összességében azt állapíthatjuk meg, hogy "játékmodellünk" közel hasonló arányú, azaz 60 százalékos pontosságot illetve fedést eredményezett. Ez elmaradt a kettős vak kézi kódolás akár 80-90 százalékos találati arányától, ugyanakkor mintánkat didaktikai okokból szándékosan kisebbre vettük a lehetségesnél. A nagymintás adatok alapján (lásd @sebokMulticlassClassificationNewspaper2021) különösen a pontosság esetében akár 90 százalék feletti érték is elérhető felügyelt gépi tanulással, mely ponton már világossá válik a mesterséges intelligenciára épülő megközelítés versenyképessége a kézi kódoláséval, különösen nagymintás projektek esetében.

De térjünk még vissza eredményeinkhez és vizsgáljuk meg őket közpolitikai kódonként is! Itt is látható, hogy 83 cikkre jeleztük előre helyesen az 1-es kódot.

```{r include = TRUE}
table(Data_NOL_predictions$predictions_label, Data_NOL_predictions$majortopic_code)
```

Végezetül előrejelzéseinket ezek kódonkénti gyakorisága szempontjából ábrázoljuk.

```{r include = TRUE}
ggplot_data <- Data_NOL_predictions[,c("majortopic_code","predictions_label")]
ggplot_data$predictions_label <- factor(ggplot_data$predictions_label)
```

```{r include = TRUE}
df <- ggplot_data %>%
  group_by(majortopic_code, predictions_label) %>%
  summarise(n = n())
```

```{r include = TRUE}
ggplot(df, aes(majortopic_code, n, fill = predictions_label)) + 
  geom_bar(, stat = "identity")
```
